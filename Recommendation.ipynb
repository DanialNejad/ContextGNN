{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relbench.datasets import get_dataset, get_dataset_names, register_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from relbench.base import Database, Dataset, Table\n",
    "\n",
    "class TransactionalDataset(Dataset):\n",
    "    # Set timestamps or other relevant information if needed\n",
    "    val_timestamp = pd.Timestamp(\"2022-02-15\")\n",
    "    test_timestamp = pd.Timestamp(\"2022-02-22\")\n",
    "\n",
    "    def make_db(self) -> Database:\n",
    "        # Path to your CSVs folder\n",
    "        path = os.path.join(\"D:/Dani/relbench/relbench/\", \"hyper_data\")\n",
    "        customers = os.path.join(path, \"Customers.csv\")\n",
    "        articles = os.path.join(path, \"Articles.csv\")\n",
    "        branches = os.path.join(path, \"Branches.csv\")\n",
    "        transactions = os.path.join(path, \"Transactions.csv\")\n",
    "\n",
    "        # Ensure that CSV files exist in the specified path\n",
    "        if not os.path.exists(customers):\n",
    "            raise RuntimeError(f\"Dataset not found at '{path}'. Please make sure the CSV files are in the correct folder.\")\n",
    "\n",
    "        # Read the CSV data into pandas DataFrames\n",
    "        customers_df = pd.read_csv(customers)\n",
    "        articles_df = pd.read_csv(articles)\n",
    "        branches_df = pd.read_csv(branches)\n",
    "        transactions_df = pd.read_csv(transactions)\n",
    "        transactions_df['d_dat'] = pd.to_datetime(transactions_df['d_dat'])        \n",
    "        split_date = pd.to_datetime('2021-08-01')\n",
    "        transactions_df = transactions_df[transactions_df['d_dat'] >= split_date]\n",
    "        transactions_df = transactions_df.reset_index(drop=True)\n",
    "        ################################################################################\n",
    "        # Check for and handle duplicate primary keys in articles, customers, and branches tables\n",
    "        ################################################################################\n",
    "\n",
    "        # Handle duplicates in the articles table\n",
    "        if articles_df.duplicated(subset=['articles_id']).any():\n",
    "            print(\"Duplicates found in the 'articles_id' column. Removing duplicates...\")\n",
    "            articles_df = articles_df.drop_duplicates(subset=['articles_id'], keep='first')\n",
    "\n",
    "        # Handle duplicates in the customers table\n",
    "        if customers_df.duplicated(subset=['customers_id']).any():\n",
    "            print(\"Duplicates found in the 'customers_id' column. Removing duplicates...\")\n",
    "            customers_df = customers_df.drop_duplicates(subset=['customers_id'], keep='first')\n",
    "\n",
    "        # Handle duplicates in the branches table\n",
    "        if branches_df.duplicated(subset=['BranchCode']).any():\n",
    "            print(\"Duplicates found in the 'BranchCode' column. Removing duplicates...\")\n",
    "            branches_df = branches_df.drop_duplicates(subset=['BranchCode'], keep='first')\n",
    "\n",
    "        ################################################################################\n",
    "        # Clean and process the data (drop unnecessary columns, handle missing data)\n",
    "        ################################################################################\n",
    "        # Drop unnecessary columns\n",
    "        transactions_df.drop(columns=[\"Return Amount\"], inplace=True)\n",
    "        articles_df.drop(columns=[\"Item Barcode\", \"External Item Number\"], inplace=True)\n",
    "\n",
    "        # Replace any missing or invalid values\n",
    "        transactions_df[\"salesTime\"] = transactions_df[\"salesTime\"].replace(r\"^\\\\N$\", \"00:00:00\", regex=True)\n",
    "        transactions_df = transactions_df.replace(r\"^\\\\N$\", np.nan, regex=True)\n",
    "\n",
    "        # Combine date and time into a single 'datetime' column\n",
    "        # transactions_df['datetime'] = pd.to_datetime(transactions_df['d_dat'] + ' ' + transactions_df['salesTime'])\n",
    "        # transactions_df.drop(columns=[\"d_dat\"], inplace=True)        \n",
    "        # Convert date column to pd.Timestamp\n",
    "        # transactions_df[\"datetime\"] = pd.to_datetime(transactions_df[\"datetime\"])\n",
    "\n",
    "        transactions_df[\"datetime\"] = pd.to_datetime(\n",
    "        transactions_df[\"d_dat\"], format=\"%Y-%m-%d\"\n",
    "        )\n",
    "        transactions_df.drop(columns=[\"d_dat\"], inplace=True)          \n",
    "        # Convert other fields if necessary\n",
    "        transactions_df['price_purchase'] = pd.to_numeric(transactions_df['price_purchase'], errors='coerce')\n",
    "        transactions_df['Discount_ratio'] = pd.to_numeric(transactions_df['Discount_ratio'], errors='coerce')\n",
    "        transactions_df['Quantity'] = pd.to_numeric(transactions_df['Quantity'], errors='coerce')\n",
    "\n",
    "        ################################################################################\n",
    "        # Now we define the table structure and relationships.\n",
    "        ################################################################################\n",
    "\n",
    "        tables = {}\n",
    "\n",
    "        # Articles table\n",
    "        tables[\"article\"] = Table(\n",
    "            df=pd.DataFrame(articles_df),\n",
    "            fkey_col_to_pkey_table={},\n",
    "            pkey_col=\"articles_id\",\n",
    "            time_col=None,\n",
    "        )\n",
    "\n",
    "        # Customers table\n",
    "        tables[\"customer\"] = Table(\n",
    "            df=pd.DataFrame(customers_df),\n",
    "            fkey_col_to_pkey_table={},\n",
    "            pkey_col=\"customers_id\",\n",
    "            time_col=None,\n",
    "        )\n",
    "\n",
    "        # Branches table (renamed from \"branche\" to \"branches\")\n",
    "        tables[\"branches\"] = Table(\n",
    "            df=pd.DataFrame(branches_df),\n",
    "            fkey_col_to_pkey_table={},\n",
    "            pkey_col=\"BranchCode\",\n",
    "            time_col=None,\n",
    "        )\n",
    "\n",
    "        # Transactions table\n",
    "        tables[\"transactions\"] = Table(\n",
    "            df=pd.DataFrame(transactions_df),\n",
    "            fkey_col_to_pkey_table={\n",
    "                \"articles_id\": \"article\",    # Foreign key to articles\n",
    "                \"customers_id\": \"customer\",  # Foreign key to customers\n",
    "                \"BranchCode\": \"branches\",    # Foreign key to branches\n",
    "            },\n",
    "            pkey_col=None,\n",
    "            time_col=\"datetime\",  # Use the combined datetime column for time-based operations\n",
    "        )\n",
    "\n",
    "        return Database(tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from relbench.base import Database, Dataset, Table\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class TransactionalDataset(Dataset):\n",
    "    # Set timestamps or other relevant information if needed\n",
    "    val_timestamp = pd.Timestamp(\"2022-02-15\")\n",
    "    test_timestamp = pd.Timestamp(\"2022-02-22\")\n",
    "\n",
    "    def make_db(self) -> Database:\n",
    "        # Path to your CSVs folder\n",
    "        path = os.path.join(\"D:/Dani/relbench/relbench/\", \"hyper_data\")\n",
    "        customers = os.path.join(path, \"Customers.csv\")\n",
    "        articles = os.path.join(path, \"Articles.csv\")\n",
    "        branches = os.path.join(path, \"Branches.csv\")\n",
    "        transactions = os.path.join(path, \"Transactions.csv\")\n",
    "\n",
    "        # Ensure that CSV files exist in the specified path\n",
    "        if not os.path.exists(customers):\n",
    "            raise RuntimeError(f\"Dataset not found at '{path}'. Please make sure the CSV files are in the correct folder.\")\n",
    "\n",
    "        # Read the CSV data into pandas DataFrames\n",
    "        customers_df = pd.read_csv(customers)\n",
    "        articles_df = pd.read_csv(articles)\n",
    "        branches_df = pd.read_csv(branches)\n",
    "        transactions_df = pd.read_csv(transactions)\n",
    "        \n",
    "        ################################################################################\n",
    "        # Check for and handle duplicate primary keys in articles, customers, and branches tables\n",
    "        ################################################################################\n",
    "\n",
    "        # Handle duplicates in the articles table\n",
    "        if articles_df.duplicated(subset=['articles_id']).any():\n",
    "            print(\"Duplicates found in the 'articles_id' column. Removing duplicates...\")\n",
    "            articles_df = articles_df.drop_duplicates(subset=['articles_id'], keep='first')\n",
    "\n",
    "        # Handle duplicates in the customers table\n",
    "        if customers_df.duplicated(subset=['customers_id']).any():\n",
    "            print(\"Duplicates found in the 'customers_id' column. Removing duplicates...\")\n",
    "            customers_df = customers_df.drop_duplicates(subset=['customers_id'], keep='first')\n",
    "\n",
    "        # Handle duplicates in the branches table\n",
    "        if branches_df.duplicated(subset=['BranchCode']).any():\n",
    "            print(\"Duplicates found in the 'BranchCode' column. Removing duplicates...\")\n",
    "            branches_df = branches_df.drop_duplicates(subset=['BranchCode'], keep='first')\n",
    "\n",
    "        ################################################################################\n",
    "        # Clean and process the data (drop unnecessary columns, handle missing data)\n",
    "        ################################################################################\n",
    "        # Drop unnecessary columns\n",
    "        transactions_df.drop(columns=[\"Return Amount\"], inplace=True)\n",
    "        articles_df.drop(columns=[\"Item Barcode\", \"External Item Number\"], inplace=True)\n",
    "\n",
    "        # Replace any missing or invalid values\n",
    "        transactions_df[\"salesTime\"] = transactions_df[\"salesTime\"].replace(r\"^\\\\N$\", \"00:00:00\", regex=True)\n",
    "        transactions_df = transactions_df.replace(r\"^\\\\N$\", np.nan, regex=True)\n",
    "\n",
    "        # Combine date and time into a single 'datetime' column\n",
    "        transactions_df[\"datetime\"] = pd.to_datetime(\n",
    "            transactions_df[\"d_dat\"], format=\"%Y-%m-%d\"\n",
    "        )\n",
    "        transactions_df.drop(columns=[\"d_dat\"], inplace=True)          \n",
    "\n",
    "        # Convert other fields if necessary\n",
    "        transactions_df['price_purchase'] = pd.to_numeric(transactions_df['price_purchase'], errors='coerce')\n",
    "        transactions_df['Discount_ratio'] = pd.to_numeric(transactions_df['Discount_ratio'], errors='coerce')\n",
    "        transactions_df['Quantity'] = pd.to_numeric(transactions_df['Quantity'], errors='coerce')\n",
    "\n",
    "        ################################################################################\n",
    "        # Article Removal Logic: Remove articles not in transactions in the last 6 months\n",
    "        ################################################################################\n",
    "        # Calculate the date 6 months ago\n",
    "        last_transaction_date = transactions_df[\"datetime\"].max()\n",
    "\n",
    "        # Calculate the date 6 months ago from the last transaction date\n",
    "        six_months_ago = last_transaction_date - timedelta(days=6*30)  # Approximate 6 months\n",
    "\n",
    "        print(f\"Removing articles not in transactions since {six_months_ago.date()}\")\n",
    "\n",
    "        # Find articles that have been in transactions in the last 6 months\n",
    "        recent_transactions = transactions_df[transactions_df['datetime'] >= six_months_ago]\n",
    "        articles_in_recent_transactions = recent_transactions['articles_id'].unique()\n",
    "\n",
    "        # Filter out articles that are in recent transactions\n",
    "        articles_to_remove = articles_df[~articles_df['articles_id'].isin(articles_in_recent_transactions)]\n",
    "\n",
    "        # Remove the articles from the articles DataFrame\n",
    "        articles_df = articles_df[articles_df['articles_id'].isin(articles_in_recent_transactions)]\n",
    "\n",
    "        # Remove the articles from the transactions DataFrame as well\n",
    "        transactions_df = transactions_df[transactions_df['articles_id'].isin(articles_in_recent_transactions)]\n",
    "\n",
    "        # Optionally, save the removed articles to a separate CSV\n",
    "        articles_to_remove.to_csv(os.path.join(path, 'articles_to_remove.csv'), index=False)\n",
    "\n",
    "        print(f\"Removed {len(articles_to_remove)} articles that haven't been in a transaction in the last 6 months.\")\n",
    "\n",
    "        ################################################################################\n",
    "        # Now we define the table structure and relationships.\n",
    "        ################################################################################\n",
    "\n",
    "        tables = {}\n",
    "\n",
    "        # Articles table\n",
    "        tables[\"article\"] = Table(\n",
    "            df=pd.DataFrame(articles_df),\n",
    "            fkey_col_to_pkey_table={},\n",
    "            pkey_col=\"articles_id\",\n",
    "            time_col=None,\n",
    "        )\n",
    "\n",
    "        # Customers table\n",
    "        tables[\"customer\"] = Table(\n",
    "            df=pd.DataFrame(customers_df),\n",
    "            fkey_col_to_pkey_table={},\n",
    "            pkey_col=\"customers_id\",\n",
    "            time_col=None,\n",
    "        )\n",
    "\n",
    "        # Branches table (renamed from \"branche\" to \"branches\")\n",
    "        tables[\"branches\"] = Table(\n",
    "            df=pd.DataFrame(branches_df),\n",
    "            fkey_col_to_pkey_table={},\n",
    "            pkey_col=\"BranchCode\",\n",
    "            time_col=None,\n",
    "        )\n",
    "\n",
    "        # Transactions table\n",
    "        tables[\"transactions\"] = Table(\n",
    "            df=pd.DataFrame(transactions_df),\n",
    "            fkey_col_to_pkey_table={\n",
    "                \"articles_id\": \"article\",    # Foreign key to articles\n",
    "                \"customers_id\": \"customer\",  # Foreign key to customers\n",
    "                \"BranchCode\": \"branches\",    # Foreign key to branches\n",
    "            },\n",
    "            pkey_col=None,\n",
    "            time_col=\"datetime\",  # Use the combined datetime column for time-based operations\n",
    "        )\n",
    "\n",
    "        return Database(tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from relbench.base import Database, Dataset, Table\n",
    "\n",
    "\n",
    "# Path to your CSVs folder\n",
    "path = os.path.join(\"D:/Dani/relbench/relbench/burger_data/dorsa\", \"new\")   \n",
    "\n",
    "customers = os.path.join(path, \"customer_data.csv\")\n",
    "articles = os.path.join(path, \"article_data.csv\")\n",
    "branches = os.path.join(path, \"branch_data.csv\")\n",
    "transactions = os.path.join(path, \"transaction_data.csv\")\n",
    "calendar = os.path.join(path, \"calendar.xlsx\")\n",
    "# Ensure that CSV files exist in the specified path\n",
    "if not os.path.exists(customers):\n",
    "    raise RuntimeError(f\"Dataset not found at '{path}'. Please make sure the CSV files are in the correct folder.\")\n",
    "\n",
    "# Read the CSV data into pandas DataFrames\n",
    "customers_df = pd.read_csv(customers)\n",
    "articles_df = pd.read_csv(articles)\n",
    "branches_df = pd.read_csv(branches)\n",
    "transactions_df = pd.read_csv(transactions)\n",
    "calendar_df = pd.read_excel(calendar)\n",
    "transactions_df['d_dat'] = pd.to_datetime(transactions_df['Datetime'])\n",
    "transactions_df.drop(columns=[\"Datetime\"], inplace=True)  \n",
    "transactions_df['Date'] = transactions_df['d_dat'].dt.date\n",
    "transactions_df['Date'] = pd.to_datetime(transactions_df['Date'])\n",
    "transactions_df = transactions_df.reset_index(drop=True)\n",
    "transactions_df = pd.merge(transactions_df, calendar_df, how=\"inner\", on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transaction Type</th>\n",
       "      <th>BranchCode</th>\n",
       "      <th>Transaction Serial</th>\n",
       "      <th>Document Barcode</th>\n",
       "      <th>Time</th>\n",
       "      <th>customers_id</th>\n",
       "      <th>articles_id</th>\n",
       "      <th>Style Code</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Base Total Price</th>\n",
       "      <th>...</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Weekend</th>\n",
       "      <th>Week_of_month</th>\n",
       "      <th>Week_of_year</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>Holiday</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>فاکتور فروش</td>\n",
       "      <td>10284.0</td>\n",
       "      <td>27631.0</td>\n",
       "      <td>10284-27631</td>\n",
       "      <td>12:07:41</td>\n",
       "      <td>10121860.0</td>\n",
       "      <td>54821.0</td>\n",
       "      <td>3416.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.174312e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "      <td>140247</td>\n",
       "      <td>0</td>\n",
       "      <td>Fajr</td>\n",
       "      <td>National</td>\n",
       "      <td>Sh Imam Mousa</td>\n",
       "      <td>Religious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>فاکتور فروش</td>\n",
       "      <td>10124.0</td>\n",
       "      <td>75949.0</td>\n",
       "      <td>10124-75949</td>\n",
       "      <td>10:53:54</td>\n",
       "      <td>10447769.0</td>\n",
       "      <td>53612.0</td>\n",
       "      <td>3919.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.874312e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>140247</td>\n",
       "      <td>0</td>\n",
       "      <td>Fajr</td>\n",
       "      <td>National</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>فاکتور فروش</td>\n",
       "      <td>10706.0</td>\n",
       "      <td>21517.0</td>\n",
       "      <td>10706-21517</td>\n",
       "      <td>21:28:09</td>\n",
       "      <td>10469286.0</td>\n",
       "      <td>59459.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.040909e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>140316</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>فاکتور فروش</td>\n",
       "      <td>11008.0</td>\n",
       "      <td>4992.0</td>\n",
       "      <td>11008-4992</td>\n",
       "      <td>21:36:02</td>\n",
       "      <td>10075749.0</td>\n",
       "      <td>54126.0</td>\n",
       "      <td>3117.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.500000e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>140329</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>فاکتور فروش</td>\n",
       "      <td>10002.0</td>\n",
       "      <td>58726.0</td>\n",
       "      <td>10002-58726</td>\n",
       "      <td>18:30:10</td>\n",
       "      <td>10432889.0</td>\n",
       "      <td>44301.0</td>\n",
       "      <td>3421.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.500000e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>140140</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495844</th>\n",
       "      <td>فاکتور فروش</td>\n",
       "      <td>10563.0</td>\n",
       "      <td>5418.0</td>\n",
       "      <td>10563-5418</td>\n",
       "      <td>22:52:44</td>\n",
       "      <td>10478973.0</td>\n",
       "      <td>62211.0</td>\n",
       "      <td>4542.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.936060e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>140337</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495845</th>\n",
       "      <td>فاکتور فروش</td>\n",
       "      <td>10634.0</td>\n",
       "      <td>3188.0</td>\n",
       "      <td>10634-3188</td>\n",
       "      <td>18:13:09</td>\n",
       "      <td>10477956.0</td>\n",
       "      <td>62193.0</td>\n",
       "      <td>4542.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.955145e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>140338</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495846</th>\n",
       "      <td>فاکتور فروش</td>\n",
       "      <td>10657.0</td>\n",
       "      <td>1308.0</td>\n",
       "      <td>10657-1308</td>\n",
       "      <td>10:43:27</td>\n",
       "      <td>10467100.0</td>\n",
       "      <td>54284.0</td>\n",
       "      <td>3305.0</td>\n",
       "      <td>26</td>\n",
       "      <td>2.027523e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>140310</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495847</th>\n",
       "      <td>فاکتور فروش</td>\n",
       "      <td>10396.0</td>\n",
       "      <td>9557.0</td>\n",
       "      <td>10396-9557</td>\n",
       "      <td>10:55:21</td>\n",
       "      <td>10453553.0</td>\n",
       "      <td>62876.0</td>\n",
       "      <td>4648.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.190258e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>140343</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495848</th>\n",
       "      <td>فاکتور فروش</td>\n",
       "      <td>11019.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>11019-500</td>\n",
       "      <td>21:07:05</td>\n",
       "      <td>10462501.0</td>\n",
       "      <td>42829.0</td>\n",
       "      <td>3471.0</td>\n",
       "      <td>25</td>\n",
       "      <td>4.931193e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "      <td>140252</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>495849 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Transaction Type  BranchCode  Transaction Serial Document Barcode  \\\n",
       "0           فاکتور فروش     10284.0             27631.0      10284-27631   \n",
       "1           فاکتور فروش     10124.0             75949.0      10124-75949   \n",
       "2           فاکتور فروش     10706.0             21517.0      10706-21517   \n",
       "3           فاکتور فروش     11008.0              4992.0       11008-4992   \n",
       "4           فاکتور فروش     10002.0             58726.0      10002-58726   \n",
       "...                 ...         ...                 ...              ...   \n",
       "495844      فاکتور فروش     10563.0              5418.0       10563-5418   \n",
       "495845      فاکتور فروش     10634.0              3188.0       10634-3188   \n",
       "495846      فاکتور فروش     10657.0              1308.0       10657-1308   \n",
       "495847      فاکتور فروش     10396.0              9557.0       10396-9557   \n",
       "495848      فاکتور فروش     11019.0               500.0        11019-500   \n",
       "\n",
       "            Time  customers_id  articles_id  Style Code  Quantity  \\\n",
       "0       12:07:41    10121860.0      54821.0      3416.0         1   \n",
       "1       10:53:54    10447769.0      53612.0      3919.0         1   \n",
       "2       21:28:09    10469286.0      59459.0      2400.0         1   \n",
       "3       21:36:02    10075749.0      54126.0      3117.0         1   \n",
       "4       18:30:10    10432889.0      44301.0      3421.0         1   \n",
       "...          ...           ...          ...         ...       ...   \n",
       "495844  22:52:44    10478973.0      62211.0      4542.0         1   \n",
       "495845  18:13:09    10477956.0      62193.0      4542.0         1   \n",
       "495846  10:43:27    10467100.0      54284.0      3305.0        26   \n",
       "495847  10:55:21    10453553.0      62876.0      4648.0         1   \n",
       "495848  21:07:05    10462501.0      42829.0      3471.0        25   \n",
       "\n",
       "        Base Total Price  ...   Weekday  Weekend  Week_of_month  Week_of_year  \\\n",
       "0           1.174312e+08  ...   Tuesday        0              3            47   \n",
       "1           8.874312e+07  ...  Saturday        0              2            47   \n",
       "2           1.040909e+08  ...    Sunday        0              2            16   \n",
       "3           4.500000e+07  ...    Sunday        0              2            29   \n",
       "4           5.500000e+07  ...    Friday        1              1            40   \n",
       "...                  ...  ...       ...      ...            ...           ...   \n",
       "495844      1.936060e+09  ...  Thursday        1              2            37   \n",
       "495845      1.955145e+09  ...  Saturday        0              2            38   \n",
       "495846      2.027523e+09  ...  Saturday        0              5            10   \n",
       "495847      2.190258e+09  ...  Thursday        1              3            43   \n",
       "495848      4.931193e+09  ...  Saturday        0              3            52   \n",
       "\n",
       "        wm_yr_wk Holiday  event_name_1  event_type_1   event_name_2  \\\n",
       "0         140247       0          Fajr      National  Sh Imam Mousa   \n",
       "1         140247       0          Fajr      National            NaN   \n",
       "2         140316       0           NaN           NaN            NaN   \n",
       "3         140329       0           NaN           NaN            NaN   \n",
       "4         140140       1           NaN           NaN            NaN   \n",
       "...          ...     ...           ...           ...            ...   \n",
       "495844    140337       0           NaN           NaN            NaN   \n",
       "495845    140338       0           NaN           NaN            NaN   \n",
       "495846    140310       0           NaN           NaN            NaN   \n",
       "495847    140343       0           NaN           NaN            NaN   \n",
       "495848    140252       0           NaN           NaN            NaN   \n",
       "\n",
       "        event_type_2  \n",
       "0          Religious  \n",
       "1                NaN  \n",
       "2                NaN  \n",
       "3                NaN  \n",
       "4                NaN  \n",
       "...              ...  \n",
       "495844           NaN  \n",
       "495845           NaN  \n",
       "495846           NaN  \n",
       "495847           NaN  \n",
       "495848           NaN  \n",
       "\n",
       "[495849 rows x 47 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "def identify_invaluable_features(df, missing_threshold=0.8, variance_threshold=0.01, cardinality_threshold=0.5, correlation_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Analyzes a DataFrame and identifies features that are likely invaluable for ML modeling.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - missing_threshold: float (features with more missing values than this ratio are considered invaluable)\n",
    "    - variance_threshold: float (features with variance below this threshold are considered invaluable)\n",
    "    - cardinality_threshold: float (categorical features with unique values exceeding this ratio of total rows are invaluable)\n",
    "    - correlation_threshold: float (features with correlation above this with another feature are invaluable)\n",
    "\n",
    "    Returns:\n",
    "    - Pandas DataFrame with features and reasons why they are considered invaluable.\n",
    "    \"\"\"\n",
    "    \n",
    "    invaluable_features = []\n",
    "    \n",
    "    # High missing values\n",
    "    missing_ratios = df.isnull().mean()\n",
    "    high_missing_cols = missing_ratios[missing_ratios > missing_threshold].index\n",
    "    for col in high_missing_cols:\n",
    "        invaluable_features.append((col, \"High missing values\"))\n",
    "\n",
    "    # Low variance features\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    selector = VarianceThreshold(threshold=variance_threshold)\n",
    "    if not numeric_df.empty:\n",
    "        selector.fit(numeric_df.fillna(0))  # Replace NaN with 0 for variance check\n",
    "        low_variance_cols = numeric_df.columns[~selector.get_support()]\n",
    "        for col in low_variance_cols:\n",
    "            invaluable_features.append((col, \"Low variance\"))\n",
    "\n",
    "    # Single unique value\n",
    "    unique_counts = df.nunique()\n",
    "    single_value_cols = unique_counts[unique_counts == 1].index\n",
    "    for col in single_value_cols:\n",
    "        invaluable_features.append((col, \"Single unique value\"))\n",
    "\n",
    "    # High cardinality categorical features\n",
    "    categorical_df = df.select_dtypes(include=['object', 'category'])\n",
    "    for col in categorical_df.columns:\n",
    "        unique_ratio = categorical_df[col].nunique() / len(df)\n",
    "        if unique_ratio > cardinality_threshold:\n",
    "            invaluable_features.append((col, \"High cardinality\"))\n",
    "\n",
    "    # High correlation features\n",
    "    if not numeric_df.empty:\n",
    "        corr_matrix = numeric_df.corr().abs()\n",
    "        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        high_corr_pairs = [(col, \"High correlation\") for col in upper_tri.columns if any(upper_tri[col] > correlation_threshold)]\n",
    "        invaluable_features.extend(high_corr_pairs)\n",
    "\n",
    "    return pd.DataFrame(invaluable_features, columns=[\"Feature\", \"Reason\"])\n",
    "\n",
    "# Example Usage:\n",
    "# df = pd.read_csv(\"your_dataset.csv\")  # Load your dataset\n",
    "invaluable_features_df = identify_invaluable_features(articles_df)\n",
    "# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Invaluable Features\", dataframe=invaluable_features_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articles_id</th>\n",
       "      <th>Article Description</th>\n",
       "      <th>Base Unit Price</th>\n",
       "      <th>Style Code</th>\n",
       "      <th>Style Description</th>\n",
       "      <th>Collection Code</th>\n",
       "      <th>Collection Name</th>\n",
       "      <th>Idea Code</th>\n",
       "      <th>Idea Description</th>\n",
       "      <th>Initial Pattern Code</th>\n",
       "      <th>...</th>\n",
       "      <th>User</th>\n",
       "      <th>Usage Type</th>\n",
       "      <th>Usage Space</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Size</th>\n",
       "      <th>Season</th>\n",
       "      <th>Year</th>\n",
       "      <th>Model</th>\n",
       "      <th>Combined Feature</th>\n",
       "      <th>Dimensions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54821.0</td>\n",
       "      <td>چلسي بوت نقش‌دال</td>\n",
       "      <td>1.174312e+08</td>\n",
       "      <td>1152</td>\n",
       "      <td>چلسی بوت نقش‌دال</td>\n",
       "      <td>0</td>\n",
       "      <td>عمومي</td>\n",
       "      <td>0</td>\n",
       "      <td>عمومي</td>\n",
       "      <td>218</td>\n",
       "      <td>...</td>\n",
       "      <td>MEN</td>\n",
       "      <td>SMART CASUAL</td>\n",
       "      <td>Unknown Usage Space</td>\n",
       "      <td>DORSA</td>\n",
       "      <td>42</td>\n",
       "      <td>FALL-WINTER</td>\n",
       "      <td>1402</td>\n",
       "      <td>AKM 6408-BOOT</td>\n",
       "      <td>پنجه متوسط گرد - فرم رویه صاف - فرم زیره پاشنه...</td>\n",
       "      <td>Unknown Dimensions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53612.0</td>\n",
       "      <td>کيف پوچ زنانه کلکسيون آهنگ</td>\n",
       "      <td>8.874312e+07</td>\n",
       "      <td>1495</td>\n",
       "      <td>پوچ آهنگ</td>\n",
       "      <td>0</td>\n",
       "      <td>عمومي</td>\n",
       "      <td>0</td>\n",
       "      <td>عمومي</td>\n",
       "      <td>438</td>\n",
       "      <td>...</td>\n",
       "      <td>WOMEN</td>\n",
       "      <td>Unknown Usage Type</td>\n",
       "      <td>Unknown Usage Space</td>\n",
       "      <td>DORSA</td>\n",
       "      <td>Unknown Size</td>\n",
       "      <td>SPRING-SUMMER</td>\n",
       "      <td>1402</td>\n",
       "      <td>Unknown Model</td>\n",
       "      <td>چرم گاوی ناپا - تریم زرد - آستر نقش دال مشکی -...</td>\n",
       "      <td>L 38 * H 28 * D 3 CM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59459.0</td>\n",
       "      <td>کيف دستي زنانه کوچک</td>\n",
       "      <td>1.040909e+08</td>\n",
       "      <td>510</td>\n",
       "      <td>کیف زنانه مربعی کوچک با لوگو  65میل درسا</td>\n",
       "      <td>0</td>\n",
       "      <td>عمومي</td>\n",
       "      <td>0</td>\n",
       "      <td>عمومي</td>\n",
       "      <td>262</td>\n",
       "      <td>...</td>\n",
       "      <td>WOMEN</td>\n",
       "      <td>SMART CASUAL</td>\n",
       "      <td>Unknown Usage Space</td>\n",
       "      <td>DORSA</td>\n",
       "      <td>Unknown Size</td>\n",
       "      <td>S2</td>\n",
       "      <td>1403</td>\n",
       "      <td>Unknown Model</td>\n",
       "      <td>لوگو فلزی پنجره ای 9-65میل - رنگ فلز طلایی - چ...</td>\n",
       "      <td>L 24 * H 20.5 * D 9 CM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54126.0</td>\n",
       "      <td>کيف پول عمودي آکو</td>\n",
       "      <td>4.500000e+07</td>\n",
       "      <td>958</td>\n",
       "      <td>کیف پول عمودی اکو</td>\n",
       "      <td>44</td>\n",
       "      <td>AKO</td>\n",
       "      <td>0</td>\n",
       "      <td>AKO</td>\n",
       "      <td>328</td>\n",
       "      <td>...</td>\n",
       "      <td>MEN</td>\n",
       "      <td>SMART CASUAL</td>\n",
       "      <td>Unknown Usage Space</td>\n",
       "      <td>DORSA</td>\n",
       "      <td>Unknown Size</td>\n",
       "      <td>SPRING-SUMMER</td>\n",
       "      <td>1402</td>\n",
       "      <td>Unknown Model</td>\n",
       "      <td>آستر نقش دال مشکی - فینیش مشکی - چرم گاوی ناپا...</td>\n",
       "      <td>L 10 * H 12 * D 1.5 CM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44301.0</td>\n",
       "      <td>اسنيکر بندي زنانه ساقدار با زيره زرد</td>\n",
       "      <td>5.500000e+07</td>\n",
       "      <td>1155</td>\n",
       "      <td>اسنیکر بندی زنانه ساقدار</td>\n",
       "      <td>0</td>\n",
       "      <td>عمومي</td>\n",
       "      <td>0</td>\n",
       "      <td>عمومي</td>\n",
       "      <td>220</td>\n",
       "      <td>...</td>\n",
       "      <td>WOMEN</td>\n",
       "      <td>CASUAL</td>\n",
       "      <td>Unknown Usage Space</td>\n",
       "      <td>DORSA</td>\n",
       "      <td>37</td>\n",
       "      <td>FALL-WINTER</td>\n",
       "      <td>1399</td>\n",
       "      <td>AKM 4939-WOMEN BOOT</td>\n",
       "      <td>پنجه متوسط گرد - فرم رویه بندی - فرم زیره پاشن...</td>\n",
       "      <td>Unknown Dimensions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45534</th>\n",
       "      <td>50733.0</td>\n",
       "      <td>دستبند خرد طلا 29/18 سايز L</td>\n",
       "      <td>2.004857e+09</td>\n",
       "      <td>1906</td>\n",
       "      <td>دستبند خرد طلا سایز L</td>\n",
       "      <td>68</td>\n",
       "      <td>WISDOM</td>\n",
       "      <td>0</td>\n",
       "      <td>WISDOM</td>\n",
       "      <td>409</td>\n",
       "      <td>...</td>\n",
       "      <td>UNISEX</td>\n",
       "      <td>Unknown Usage Type</td>\n",
       "      <td>-</td>\n",
       "      <td>DORSA JEWELRY</td>\n",
       "      <td>29</td>\n",
       "      <td>SPRING-SUMMER</td>\n",
       "      <td>1402</td>\n",
       "      <td>4543-1</td>\n",
       "      <td>محاسبه وزن طلا - 18</td>\n",
       "      <td>Unknown Dimensions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45535</th>\n",
       "      <td>50731.0</td>\n",
       "      <td>دستبند خرد طلا 28/96 سايز L</td>\n",
       "      <td>2.003945e+09</td>\n",
       "      <td>1906</td>\n",
       "      <td>دستبند خرد طلا سایز L</td>\n",
       "      <td>68</td>\n",
       "      <td>WISDOM</td>\n",
       "      <td>0</td>\n",
       "      <td>WISDOM</td>\n",
       "      <td>409</td>\n",
       "      <td>...</td>\n",
       "      <td>UNISEX</td>\n",
       "      <td>Unknown Usage Type</td>\n",
       "      <td>-</td>\n",
       "      <td>DORSA JEWELRY</td>\n",
       "      <td>28</td>\n",
       "      <td>SPRING-SUMMER</td>\n",
       "      <td>1402</td>\n",
       "      <td>4543-1</td>\n",
       "      <td>محاسبه وزن طلا - 96</td>\n",
       "      <td>Unknown Dimensions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45536</th>\n",
       "      <td>62211.0</td>\n",
       "      <td>دستبند خرد طلا 28/76 سايز M</td>\n",
       "      <td>1.936060e+09</td>\n",
       "      <td>1905</td>\n",
       "      <td>دستبند خرد طلا سایز M</td>\n",
       "      <td>68</td>\n",
       "      <td>WISDOM</td>\n",
       "      <td>0</td>\n",
       "      <td>WISDOM</td>\n",
       "      <td>409</td>\n",
       "      <td>...</td>\n",
       "      <td>UNISEX</td>\n",
       "      <td>Unknown Usage Type</td>\n",
       "      <td>-</td>\n",
       "      <td>DORSA JEWELRY</td>\n",
       "      <td>28</td>\n",
       "      <td>SPRING-SUMMER</td>\n",
       "      <td>1402</td>\n",
       "      <td>4542-1</td>\n",
       "      <td>محاسبه وزن طلا - 76</td>\n",
       "      <td>Unknown Dimensions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45537</th>\n",
       "      <td>62193.0</td>\n",
       "      <td>دستبند خرد طلا 28/57 سايز M</td>\n",
       "      <td>1.955145e+09</td>\n",
       "      <td>1905</td>\n",
       "      <td>دستبند خرد طلا سایز M</td>\n",
       "      <td>68</td>\n",
       "      <td>WISDOM</td>\n",
       "      <td>0</td>\n",
       "      <td>WISDOM</td>\n",
       "      <td>409</td>\n",
       "      <td>...</td>\n",
       "      <td>UNISEX</td>\n",
       "      <td>Unknown Usage Type</td>\n",
       "      <td>-</td>\n",
       "      <td>DORSA JEWELRY</td>\n",
       "      <td>28</td>\n",
       "      <td>SPRING-SUMMER</td>\n",
       "      <td>1402</td>\n",
       "      <td>4542-1</td>\n",
       "      <td>محاسبه وزن طلا - 57</td>\n",
       "      <td>Unknown Dimensions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45538</th>\n",
       "      <td>62876.0</td>\n",
       "      <td>دستبند خرد طلا 28.18 سفيدسايزM</td>\n",
       "      <td>2.190258e+09</td>\n",
       "      <td>1972</td>\n",
       "      <td>دستبند خرد طلا سایز M</td>\n",
       "      <td>68</td>\n",
       "      <td>WISDOM</td>\n",
       "      <td>3</td>\n",
       "      <td>WISDOM</td>\n",
       "      <td>409</td>\n",
       "      <td>...</td>\n",
       "      <td>WOMEN</td>\n",
       "      <td>Unknown Usage Type</td>\n",
       "      <td>Unknown Usage Space</td>\n",
       "      <td>DORSA JEWELRY</td>\n",
       "      <td>28</td>\n",
       "      <td>SPRING-SUMMER</td>\n",
       "      <td>1401</td>\n",
       "      <td>4648-1</td>\n",
       "      <td>محاسبه وزن طلا - 18</td>\n",
       "      <td>Unknown Dimensions</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45539 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       articles_id                   Article Description  Base Unit Price  \\\n",
       "0          54821.0                      چلسي بوت نقش‌دال     1.174312e+08   \n",
       "1          53612.0            کيف پوچ زنانه کلکسيون آهنگ     8.874312e+07   \n",
       "2          59459.0                   کيف دستي زنانه کوچک     1.040909e+08   \n",
       "3          54126.0                     کيف پول عمودي آکو     4.500000e+07   \n",
       "4          44301.0  اسنيکر بندي زنانه ساقدار با زيره زرد     5.500000e+07   \n",
       "...            ...                                   ...              ...   \n",
       "45534      50733.0           دستبند خرد طلا 29/18 سايز L     2.004857e+09   \n",
       "45535      50731.0           دستبند خرد طلا 28/96 سايز L     2.003945e+09   \n",
       "45536      62211.0           دستبند خرد طلا 28/76 سايز M     1.936060e+09   \n",
       "45537      62193.0           دستبند خرد طلا 28/57 سايز M     1.955145e+09   \n",
       "45538      62876.0        دستبند خرد طلا 28.18 سفيدسايزM     2.190258e+09   \n",
       "\n",
       "       Style Code                         Style Description  Collection Code  \\\n",
       "0            1152                          چلسی بوت نقش‌دال                0   \n",
       "1            1495                                  پوچ آهنگ                0   \n",
       "2             510  کیف زنانه مربعی کوچک با لوگو  65میل درسا                0   \n",
       "3             958                         کیف پول عمودی اکو               44   \n",
       "4            1155                  اسنیکر بندی زنانه ساقدار                0   \n",
       "...           ...                                       ...              ...   \n",
       "45534        1906                     دستبند خرد طلا سایز L               68   \n",
       "45535        1906                     دستبند خرد طلا سایز L               68   \n",
       "45536        1905                     دستبند خرد طلا سایز M               68   \n",
       "45537        1905                     دستبند خرد طلا سایز M               68   \n",
       "45538        1972                     دستبند خرد طلا سایز M               68   \n",
       "\n",
       "      Collection Name  Idea Code Idea Description  Initial Pattern Code  ...  \\\n",
       "0               عمومي          0            عمومي                   218  ...   \n",
       "1               عمومي          0            عمومي                   438  ...   \n",
       "2               عمومي          0            عمومي                   262  ...   \n",
       "3                 AKO          0              AKO                   328  ...   \n",
       "4               عمومي          0            عمومي                   220  ...   \n",
       "...               ...        ...              ...                   ...  ...   \n",
       "45534          WISDOM          0           WISDOM                   409  ...   \n",
       "45535          WISDOM          0           WISDOM                   409  ...   \n",
       "45536          WISDOM          0           WISDOM                   409  ...   \n",
       "45537          WISDOM          0           WISDOM                   409  ...   \n",
       "45538          WISDOM          3           WISDOM                   409  ...   \n",
       "\n",
       "         User          Usage Type          Usage Space          Brand  \\\n",
       "0         MEN        SMART CASUAL  Unknown Usage Space          DORSA   \n",
       "1       WOMEN  Unknown Usage Type  Unknown Usage Space          DORSA   \n",
       "2       WOMEN        SMART CASUAL  Unknown Usage Space          DORSA   \n",
       "3         MEN        SMART CASUAL  Unknown Usage Space          DORSA   \n",
       "4       WOMEN              CASUAL  Unknown Usage Space          DORSA   \n",
       "...       ...                 ...                  ...            ...   \n",
       "45534  UNISEX  Unknown Usage Type                    -  DORSA JEWELRY   \n",
       "45535  UNISEX  Unknown Usage Type                    -  DORSA JEWELRY   \n",
       "45536  UNISEX  Unknown Usage Type                    -  DORSA JEWELRY   \n",
       "45537  UNISEX  Unknown Usage Type                    -  DORSA JEWELRY   \n",
       "45538   WOMEN  Unknown Usage Type  Unknown Usage Space  DORSA JEWELRY   \n",
       "\n",
       "               Size         Season  Year                Model  \\\n",
       "0                42    FALL-WINTER  1402        AKM 6408-BOOT   \n",
       "1      Unknown Size  SPRING-SUMMER  1402        Unknown Model   \n",
       "2      Unknown Size             S2  1403        Unknown Model   \n",
       "3      Unknown Size  SPRING-SUMMER  1402        Unknown Model   \n",
       "4                37    FALL-WINTER  1399  AKM 4939-WOMEN BOOT   \n",
       "...             ...            ...   ...                  ...   \n",
       "45534            29  SPRING-SUMMER  1402               4543-1   \n",
       "45535            28  SPRING-SUMMER  1402               4543-1   \n",
       "45536            28  SPRING-SUMMER  1402               4542-1   \n",
       "45537            28  SPRING-SUMMER  1402               4542-1   \n",
       "45538            28  SPRING-SUMMER  1401               4648-1   \n",
       "\n",
       "                                        Combined Feature  \\\n",
       "0      پنجه متوسط گرد - فرم رویه صاف - فرم زیره پاشنه...   \n",
       "1      چرم گاوی ناپا - تریم زرد - آستر نقش دال مشکی -...   \n",
       "2      لوگو فلزی پنجره ای 9-65میل - رنگ فلز طلایی - چ...   \n",
       "3      آستر نقش دال مشکی - فینیش مشکی - چرم گاوی ناپا...   \n",
       "4      پنجه متوسط گرد - فرم رویه بندی - فرم زیره پاشن...   \n",
       "...                                                  ...   \n",
       "45534                                محاسبه وزن طلا - 18   \n",
       "45535                                محاسبه وزن طلا - 96   \n",
       "45536                                محاسبه وزن طلا - 76   \n",
       "45537                                محاسبه وزن طلا - 57   \n",
       "45538                                محاسبه وزن طلا - 18   \n",
       "\n",
       "                   Dimensions  \n",
       "0          Unknown Dimensions  \n",
       "1        L 38 * H 28 * D 3 CM  \n",
       "2      L 24 * H 20.5 * D 9 CM  \n",
       "3      L 10 * H 12 * D 1.5 CM  \n",
       "4          Unknown Dimensions  \n",
       "...                       ...  \n",
       "45534      Unknown Dimensions  \n",
       "45535      Unknown Dimensions  \n",
       "45536      Unknown Dimensions  \n",
       "45537      Unknown Dimensions  \n",
       "45538      Unknown Dimensions  \n",
       "\n",
       "[45539 rows x 29 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transaction Type</td>\n",
       "      <td>Single unique value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Document Barcode</td>\n",
       "      <td>High cardinality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sale Total Price</td>\n",
       "      <td>High correlation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Net Sales</td>\n",
       "      <td>High correlation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tax</td>\n",
       "      <td>High correlation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Net Sales Minus Sales Voucher</td>\n",
       "      <td>High correlation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Feature               Reason\n",
       "0               Transaction Type  Single unique value\n",
       "1               Document Barcode     High cardinality\n",
       "2               Sale Total Price     High correlation\n",
       "3                      Net Sales     High correlation\n",
       "4                            Tax     High correlation\n",
       "5  Net Sales Minus Sales Voucher     High correlation"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invaluable_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Transaction Type', 'BranchCode', 'Transaction Serial',\n",
       "       'Document Barcode', 'Time', 'customers_id', 'articles_id', 'Style Code',\n",
       "       'Quantity', 'Base Total Price', 'Sale Total Price', 'Net Sales',\n",
       "       'Year_x', 'Month_x', 'Day', 'Season', 'Allocated Credit',\n",
       "       'Sales Voucher', 'Gift Voucher', 'Cash Card', 'Special Discount',\n",
       "       'Promotion Discount', 'Small Change Discount', 'Tax', 'Total Discounts',\n",
       "       'Net Sales Minus Sales Voucher', 'Discount Percentage', 'd_dat', 'Date',\n",
       "       'Date_S', 'd', 'Year_y', 'Day_of_year', 'Month_y', 'Day_of_month',\n",
       "       'Day_of_week', 'even_odd', 'Weekday', 'Weekend', 'Week_of_month',\n",
       "       'Week_of_year', 'wm_yr_wk', 'Holiday', 'event_name_1', 'event_type_1',\n",
       "       'event_name_2', 'event_type_2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from relbench.base import Database, Dataset, Table\n",
    "\n",
    "class TransactionalDataset(Dataset):\n",
    "    # Set timestamps or other relevant information if needed\n",
    "    # val_timestamp = pd.Timestamp(\"2024-11-17\")\n",
    "    # test_timestamp = pd.Timestamp(\"2024-12-18\")\n",
    "    val_timestamp = pd.Timestamp(\"2025-01-3\")\n",
    "    test_timestamp = pd.Timestamp(\"2025-01-11\")\n",
    "    def make_db(self) -> Database:\n",
    "        # Path to your CSVs folder\n",
    "        path = os.path.join(\"D:/Dani/relbench/relbench/burger_data/dorsa\", \"new\")   \n",
    "\n",
    "        customers = os.path.join(path, \"customer_data.csv\")\n",
    "        articles = os.path.join(path, \"article_data.csv\")\n",
    "        branches = os.path.join(path, \"branch_data.csv\")\n",
    "        transactions = os.path.join(path, \"transaction_data.csv\")\n",
    "        calendar = os.path.join(path, \"calendar.xlsx\")\n",
    "        # Ensure that CSV files exist in the specified path\n",
    "        if not os.path.exists(customers):\n",
    "            raise RuntimeError(f\"Dataset not found at '{path}'. Please make sure the CSV files are in the correct folder.\")\n",
    "\n",
    "        # Read the CSV data into pandas DataFrames\n",
    "        customers_df = pd.read_csv(customers)\n",
    "        articles_df = pd.read_csv(articles)\n",
    "        branches_df = pd.read_csv(branches)\n",
    "        transactions_df = pd.read_csv(transactions)\n",
    "        calendar_df = pd.read_excel(calendar)\n",
    "        transactions_df['d_dat'] = pd.to_datetime(transactions_df['Datetime'])\n",
    "        transactions_df.drop(columns=[\"Datetime\"], inplace=True)  \n",
    "        transactions_df['Date'] = transactions_df['d_dat'].dt.date\n",
    "        transactions_df['Date'] = pd.to_datetime(transactions_df['Date'])\n",
    "        transactions_df = transactions_df.reset_index(drop=True)\n",
    "        transactions_df = pd.merge(transactions_df, calendar_df, how=\"inner\", on='Date')\n",
    "        \n",
    "        ################################################################################\n",
    "        # Check for and handle duplicate primary keys in articles, customers, and branches tables\n",
    "        ################################################################################\n",
    "        # customers_df['customers_id'] = customers_df['Customer ID']\n",
    "        # customers_df.drop(columns=[\"Customer ID\"], inplace=True)\n",
    "        articles_df.drop(columns=['articles_id'], inplace=True)\n",
    "        articles_df['articles_id'] = articles_df[\"Style Code\"]\n",
    "\n",
    "\n",
    "        transactions_df.drop(columns=['articles_id'], inplace=True)\n",
    "        transactions_df['articles_id'] = transactions_df[\"Style Code\"]\n",
    "\n",
    "\n",
    "        # transactions_df['customers_id'] = transactions_df['Customer ID']\n",
    "        # transactions_df.drop(columns=[\"Customer ID\"], inplace=True)\n",
    "\n",
    "        # transactions_df['BranchCode'] = transactions_df['Branch ID']\n",
    "        # transactions_df.drop(columns=[\"Branch ID\"], inplace=True)\n",
    "\n",
    "        # branches_df['BranchCode'] = branches_df['Branch ID']\n",
    "        # branches_df.drop(columns=[\"Branch ID\"], inplace=True)        \n",
    "\n",
    "        # Handle duplicates in the articles table\n",
    "        articles_df = articles_df.drop_duplicates(subset=['articles_id'], keep='first')\n",
    "        \n",
    "        # Handle duplicates in the customers table\n",
    "        customers_df = customers_df.drop_duplicates(subset=['customers_id'], keep='first')\n",
    "        \n",
    "        # Handle duplicates in the branches table\n",
    "        branches_df = branches_df.drop_duplicates(subset=['BranchCode'], keep='first')\n",
    "        \n",
    "        ################################################################################\n",
    "        # Filter customers with 3 or more transactions\n",
    "        ################################################################################\n",
    "        unique_transactions = transactions_df.drop_duplicates(subset=['customers_id', 'Transaction Serial'])\n",
    "\n",
    "        # Count the unique customers\n",
    "        transaction_count = unique_transactions['customers_id'].value_counts()    \n",
    "        valid_customers = transaction_count[transaction_count >= 3].index\n",
    "        transactions_df = transactions_df[transactions_df['customers_id'].isin(valid_customers)]\n",
    "        customers_df = customers_df[customers_df['customers_id'].isin(valid_customers)]\n",
    "        \n",
    "        ################################################################################\n",
    "        # Clean and process the data (drop unnecessary columns, handle missing data)\n",
    "        ################################################################################\n",
    "        transactions_df.drop(columns=[\"event_name_2\", \"event_type_2\", \"Transaction Type\", \"Document Barcode\", \"Sale Total Price\", \"Net Sales\", \"Tax\", \n",
    "\"Net Sales Minus Sales Voucher\", \"d\", \"Year_y\", \"Day_of_year\", \"Month_y\", \"Day_of_month\", \"Week_of_month\", \n",
    "\"Week_of_year\", \"wm_yr_wk\", \"Date_S\", \"Date\"], inplace=True)\n",
    "        transactions_df = transactions_df.replace(r\"^\\\\N$\", np.nan, regex=True)\n",
    "\n",
    "        # Convert date column to pd.Timestamp\n",
    "        transactions_df[\"datetime\"] = pd.to_datetime(transactions_df[\"d_dat\"], format=\"%Y-%m-%d\")\n",
    "        transactions_df.drop(columns=[\"d_dat\"], inplace=True)          \n",
    "        \n",
    "        ################################################################################\n",
    "        # Now we define the table structure and relationships.\n",
    "        ################################################################################\n",
    "        tables = {}\n",
    "\n",
    "        # Articles table\n",
    "        tables[\"article\"] = Table(\n",
    "            df=pd.DataFrame(articles_df),\n",
    "            fkey_col_to_pkey_table={},\n",
    "            pkey_col=\"articles_id\",\n",
    "            time_col=None,\n",
    "        )\n",
    "\n",
    "        # Customers table\n",
    "        tables[\"customer\"] = Table(\n",
    "            df=pd.DataFrame(customers_df),\n",
    "            fkey_col_to_pkey_table={},\n",
    "            pkey_col=\"customers_id\",\n",
    "            time_col=None,\n",
    "        )\n",
    "\n",
    "        # Branches table\n",
    "        tables[\"branches\"] = Table(\n",
    "            df=pd.DataFrame(branches_df),\n",
    "            fkey_col_to_pkey_table={},\n",
    "            pkey_col=\"BranchCode\",\n",
    "            time_col=None,\n",
    "        )\n",
    "\n",
    "        # Transactions table\n",
    "        tables[\"transactions\"] = Table(\n",
    "            df=pd.DataFrame(transactions_df),\n",
    "            fkey_col_to_pkey_table={\n",
    "                \"articles_id\": \"article\",    # Foreign key to articles\n",
    "                \"customers_id\": \"customer\",  # Foreign key to customers\n",
    "                \"BranchCode\": \"branches\",    # Foreign key to branches\n",
    "            },\n",
    "            pkey_col=None,\n",
    "            time_col=\"datetime\",  # Use the combined datetime column for time-based operations\n",
    "        )\n",
    "\n",
    "        return Database(tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactional_dataset = TransactionalDataset()\n",
    "db = transactional_dataset.make_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = db.table_dict[\"transactions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Table(df=\n",
       "        BranchCode  Transaction Serial      Time  customers_id  Style Code  \\\n",
       "0          10284.0             27631.0  12:07:41    10121860.0      3416.0   \n",
       "1          10124.0             75949.0  10:53:54    10447769.0      3919.0   \n",
       "2          10706.0             21517.0  21:28:09    10469286.0      2400.0   \n",
       "3          11008.0              4992.0  21:36:02    10075749.0      3117.0   \n",
       "4          10002.0             58726.0  18:30:10    10432889.0      3421.0   \n",
       "...            ...                 ...       ...           ...         ...   \n",
       "495838     10486.0              4804.0  18:42:41    10443557.0      4541.0   \n",
       "495841     10317.0             11390.0  19:02:00    10183163.0      4543.0   \n",
       "495843     10563.0              5333.0  17:28:55    10026437.0      4543.0   \n",
       "495844     10563.0              5418.0  22:52:44    10478973.0      4542.0   \n",
       "495848     11019.0               500.0  21:07:05    10462501.0      3471.0   \n",
       "\n",
       "        Quantity  Base Total Price  Year_x  Month_x   Day  ...  \\\n",
       "0              1      1.174312e+08  1402.0     11.0  17.0  ...   \n",
       "1              1      8.874312e+07  1402.0     11.0  14.0  ...   \n",
       "2              1      1.040909e+08  1403.0      4.0  10.0  ...   \n",
       "3              1      4.500000e+07  1403.0      7.0   8.0  ...   \n",
       "4              1      5.500000e+07  1401.0     10.0   2.0  ...   \n",
       "...          ...               ...     ...      ...   ...  ...   \n",
       "495838         1      1.438817e+09  1403.0      8.0  11.0  ...   \n",
       "495841         1      1.560967e+09  1403.0      1.0  21.0  ...   \n",
       "495843         1      2.003945e+09  1403.0      8.0  12.0  ...   \n",
       "495844         1      1.936060e+09  1403.0      9.0   8.0  ...   \n",
       "495848        25      4.931193e+09  1402.0     12.0  19.0  ...   \n",
       "\n",
       "       Discount Percentage  Day_of_week  even_odd   Weekday  Weekend  Holiday  \\\n",
       "0                      0.0            3         1   Tuesday        0        0   \n",
       "1                      0.0            0         0  Saturday        0        0   \n",
       "2                      0.0            1         1    Sunday        0        0   \n",
       "3                      0.0            1         1    Sunday        0        0   \n",
       "4                      0.0            6         0    Friday        1        1   \n",
       "...                    ...          ...       ...       ...      ...      ...   \n",
       "495838                 0.0            6         0    Friday        1        1   \n",
       "495841                 0.0            3         1   Tuesday        0        0   \n",
       "495843                 0.0            0         0  Saturday        0        0   \n",
       "495844                 0.0            5         1  Thursday        1        0   \n",
       "495848                 0.0            0         0  Saturday        0        0   \n",
       "\n",
       "        event_name_1  event_type_1  articles_id            datetime  \n",
       "0               Fajr      National       3416.0 2024-02-06 12:07:41  \n",
       "1               Fajr      National       3919.0 2024-02-03 10:53:54  \n",
       "2                NaN           NaN       2400.0 2024-06-30 21:28:09  \n",
       "3                NaN           NaN       3117.0 2024-09-29 21:36:02  \n",
       "4                NaN           NaN       3421.0 2022-12-23 18:30:10  \n",
       "...              ...           ...          ...                 ...  \n",
       "495838           NaN           NaN       4541.0 2024-11-01 18:42:41  \n",
       "495841           NaN           NaN       4543.0 2024-04-09 19:02:00  \n",
       "495843           NaN           NaN       4543.0 2024-11-02 17:28:55  \n",
       "495844           NaN           NaN       4542.0 2024-11-28 22:52:44  \n",
       "495848           NaN           NaN       3471.0 2024-03-09 21:07:05  \n",
       "\n",
       "[319438 rows x 29 columns],\n",
       "  fkey_col_to_pkey_table={'articles_id': 'article', 'customers_id': 'customer', 'BranchCode': 'branches'},\n",
       "  pkey_col=None,\n",
       "  time_col=datetime)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.table_dict[\"transactions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Table(df=\n",
       "    BranchCode                  Branch Name   City\n",
       "0      10284.0              درسا اطلس تبريز  تبريز\n",
       "1      10124.0                     درسا ارگ  تهران\n",
       "2      10706.0            فروشگاه درسا قلهک  تهران\n",
       "3      11008.0                درسا سام سنتر  تهران\n",
       "4      10002.0                   درسا گاندي  تهران\n",
       "..         ...                          ...    ...\n",
       "70     10435.0     درسا احمد آباد - جواهرات   مشهد\n",
       "71     10311.0           درسا ارگ - جواهرات  تهران\n",
       "72     10431.0         درسا کرمان - جواهرات  کرمان\n",
       "73     10743.0  درسا فرودگاه تبريز -جواهرات  تبريز\n",
       "74     13827.0      درسا حيات سبز - جواهرات  تهران\n",
       "\n",
       "[75 rows x 3 columns],\n",
       "  fkey_col_to_pkey_table={},\n",
       "  pkey_col=BranchCode,\n",
       "  time_col=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.table_dict[\"branches\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = db.table_dict[\"article\"].df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article Description</th>\n",
       "      <th>Base Unit Price</th>\n",
       "      <th>Style Code</th>\n",
       "      <th>Style Description</th>\n",
       "      <th>Collection Code</th>\n",
       "      <th>Collection Name</th>\n",
       "      <th>Idea Code</th>\n",
       "      <th>Idea Description</th>\n",
       "      <th>Initial Pattern Code</th>\n",
       "      <th>Initial Pattern Description</th>\n",
       "      <th>...</th>\n",
       "      <th>Usage Type</th>\n",
       "      <th>Usage Space</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Size</th>\n",
       "      <th>Season</th>\n",
       "      <th>Year</th>\n",
       "      <th>Model</th>\n",
       "      <th>Combined Feature</th>\n",
       "      <th>Dimensions</th>\n",
       "      <th>articles_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>چلسي بوت نقش‌دال</td>\n",
       "      <td>1.174312e+08</td>\n",
       "      <td>1152</td>\n",
       "      <td>چلسی بوت نقش‌دال</td>\n",
       "      <td>0</td>\n",
       "      <td>عمومي</td>\n",
       "      <td>0</td>\n",
       "      <td>عمومي</td>\n",
       "      <td>218</td>\n",
       "      <td>قالب 9717 مردانه</td>\n",
       "      <td>...</td>\n",
       "      <td>SMART CASUAL</td>\n",
       "      <td>Unknown Usage Space</td>\n",
       "      <td>DORSA</td>\n",
       "      <td>42</td>\n",
       "      <td>FALL-WINTER</td>\n",
       "      <td>1402</td>\n",
       "      <td>AKM 6408-BOOT</td>\n",
       "      <td>پنجه متوسط گرد - فرم رویه صاف - فرم زیره پاشنه...</td>\n",
       "      <td>Unknown Dimensions</td>\n",
       "      <td>1152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>کيف پوچ زنانه کلکسيون آهنگ</td>\n",
       "      <td>8.874312e+07</td>\n",
       "      <td>1495</td>\n",
       "      <td>پوچ آهنگ</td>\n",
       "      <td>0</td>\n",
       "      <td>عمومي</td>\n",
       "      <td>0</td>\n",
       "      <td>عمومي</td>\n",
       "      <td>438</td>\n",
       "      <td>پوچ آهنگ_3206</td>\n",
       "      <td>...</td>\n",
       "      <td>Unknown Usage Type</td>\n",
       "      <td>Unknown Usage Space</td>\n",
       "      <td>DORSA</td>\n",
       "      <td>Unknown Size</td>\n",
       "      <td>SPRING-SUMMER</td>\n",
       "      <td>1402</td>\n",
       "      <td>Unknown Model</td>\n",
       "      <td>چرم گاوی ناپا - تریم زرد - آستر نقش دال مشکی -...</td>\n",
       "      <td>L 38 * H 28 * D 3 CM</td>\n",
       "      <td>1495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>کيف دستي زنانه کوچک</td>\n",
       "      <td>1.040909e+08</td>\n",
       "      <td>510</td>\n",
       "      <td>کیف زنانه مربعی کوچک با لوگو  65میل درسا</td>\n",
       "      <td>0</td>\n",
       "      <td>عمومي</td>\n",
       "      <td>0</td>\n",
       "      <td>عمومي</td>\n",
       "      <td>262</td>\n",
       "      <td>کیف دسته دار-2400</td>\n",
       "      <td>...</td>\n",
       "      <td>SMART CASUAL</td>\n",
       "      <td>Unknown Usage Space</td>\n",
       "      <td>DORSA</td>\n",
       "      <td>Unknown Size</td>\n",
       "      <td>S2</td>\n",
       "      <td>1403</td>\n",
       "      <td>Unknown Model</td>\n",
       "      <td>لوگو فلزی پنجره ای 9-65میل - رنگ فلز طلایی - چ...</td>\n",
       "      <td>L 24 * H 20.5 * D 9 CM</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>کيف پول عمودي آکو</td>\n",
       "      <td>4.500000e+07</td>\n",
       "      <td>958</td>\n",
       "      <td>کیف پول عمودی اکو</td>\n",
       "      <td>44</td>\n",
       "      <td>AKO</td>\n",
       "      <td>0</td>\n",
       "      <td>AKO</td>\n",
       "      <td>328</td>\n",
       "      <td>کیف پول کوچک بای فولد-2286</td>\n",
       "      <td>...</td>\n",
       "      <td>SMART CASUAL</td>\n",
       "      <td>Unknown Usage Space</td>\n",
       "      <td>DORSA</td>\n",
       "      <td>Unknown Size</td>\n",
       "      <td>SPRING-SUMMER</td>\n",
       "      <td>1402</td>\n",
       "      <td>Unknown Model</td>\n",
       "      <td>آستر نقش دال مشکی - فینیش مشکی - چرم گاوی ناپا...</td>\n",
       "      <td>L 10 * H 12 * D 1.5 CM</td>\n",
       "      <td>958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>اسنيکر بندي زنانه ساقدار با زيره زرد</td>\n",
       "      <td>5.500000e+07</td>\n",
       "      <td>1155</td>\n",
       "      <td>اسنیکر بندی زنانه ساقدار</td>\n",
       "      <td>0</td>\n",
       "      <td>عمومي</td>\n",
       "      <td>0</td>\n",
       "      <td>عمومي</td>\n",
       "      <td>220</td>\n",
       "      <td>قالب 3-049 اسنیکر زنانه</td>\n",
       "      <td>...</td>\n",
       "      <td>CASUAL</td>\n",
       "      <td>Unknown Usage Space</td>\n",
       "      <td>DORSA</td>\n",
       "      <td>37</td>\n",
       "      <td>FALL-WINTER</td>\n",
       "      <td>1399</td>\n",
       "      <td>AKM 4939-WOMEN BOOT</td>\n",
       "      <td>پنجه متوسط گرد - فرم رویه بندی - فرم زیره پاشن...</td>\n",
       "      <td>Unknown Dimensions</td>\n",
       "      <td>1155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45487</th>\n",
       "      <td>دستبندنقش دال پرکريستال سواروسکي طلا 14/66سايز M</td>\n",
       "      <td>8.541356e+08</td>\n",
       "      <td>1275</td>\n",
       "      <td>دستبند طلا یک ردیف توری نقش دال  پر کریستال سا...</td>\n",
       "      <td>63</td>\n",
       "      <td>DALL</td>\n",
       "      <td>3</td>\n",
       "      <td>DALL</td>\n",
       "      <td>234</td>\n",
       "      <td>النگوی یک ردیف توری نقش دال</td>\n",
       "      <td>...</td>\n",
       "      <td>Unknown Usage Type</td>\n",
       "      <td>Unknown Usage Space</td>\n",
       "      <td>DORSA JEWELRY</td>\n",
       "      <td>14</td>\n",
       "      <td>S1</td>\n",
       "      <td>1403</td>\n",
       "      <td>3586-1</td>\n",
       "      <td>محاسبه وزن طلا - 66 - کریستال 6862</td>\n",
       "      <td>Unknown Dimensions</td>\n",
       "      <td>1275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45522</th>\n",
       "      <td>انگشتر شأن طلاباکريستال سواروسکي دودي 18.26 سا...</td>\n",
       "      <td>1.240587e+09</td>\n",
       "      <td>1979</td>\n",
       "      <td>انگشتر شأن طلا سایز S</td>\n",
       "      <td>93</td>\n",
       "      <td>شأن</td>\n",
       "      <td>3</td>\n",
       "      <td>شأن</td>\n",
       "      <td>494</td>\n",
       "      <td>انگشتر سه وجهی دو دور</td>\n",
       "      <td>...</td>\n",
       "      <td>Unknown Usage Type</td>\n",
       "      <td>Unknown Usage Space</td>\n",
       "      <td>DORSA JEWELRY</td>\n",
       "      <td>18</td>\n",
       "      <td>S1</td>\n",
       "      <td>1403</td>\n",
       "      <td>4664-1</td>\n",
       "      <td>محاسبه وزن طلا - 26 - کریستال 6719 - کریستال 6...</td>\n",
       "      <td>Unknown Dimensions</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45523</th>\n",
       "      <td>دستبند خيال بغل دال با کريستال طلا 24/21 سايز S</td>\n",
       "      <td>1.261884e+09</td>\n",
       "      <td>1306</td>\n",
       "      <td>دستبند خیال پر کریستال طلا S</td>\n",
       "      <td>83</td>\n",
       "      <td>KHIYAL</td>\n",
       "      <td>0</td>\n",
       "      <td>KHIYAL</td>\n",
       "      <td>372</td>\n",
       "      <td>النگوی بغل دال پر کریستال</td>\n",
       "      <td>...</td>\n",
       "      <td>Unknown Usage Type</td>\n",
       "      <td>Unknown Usage Space</td>\n",
       "      <td>DORSA JEWELRY</td>\n",
       "      <td>24</td>\n",
       "      <td>SPRING-SUMMER</td>\n",
       "      <td>1402</td>\n",
       "      <td>3632-1</td>\n",
       "      <td>محاسبه وزن طلا - 21 - کریستال 6862 - کریستال 7223</td>\n",
       "      <td>Unknown Dimensions</td>\n",
       "      <td>1306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45524</th>\n",
       "      <td>انگشتر دژ با راکس و کريستال طلا 16.36 سايز L</td>\n",
       "      <td>1.276972e+09</td>\n",
       "      <td>1783</td>\n",
       "      <td>انگشتر دژ با راکس و کریستال طلا سایز L</td>\n",
       "      <td>72</td>\n",
       "      <td>AUDACIOUS</td>\n",
       "      <td>3</td>\n",
       "      <td>AUDACIOUS</td>\n",
       "      <td>498</td>\n",
       "      <td>Unknown Pattern</td>\n",
       "      <td>...</td>\n",
       "      <td>Unknown Usage Type</td>\n",
       "      <td>Unknown Usage Space</td>\n",
       "      <td>DORSA JEWELRY</td>\n",
       "      <td>16</td>\n",
       "      <td>SPRING-SUMMER</td>\n",
       "      <td>1400</td>\n",
       "      <td>4359-1</td>\n",
       "      <td>محاسبه وزن طلا - 36 - کریستال 6719 - کریستال 6669</td>\n",
       "      <td>Unknown Dimensions</td>\n",
       "      <td>1783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45538</th>\n",
       "      <td>دستبند خرد طلا 28.18 سفيدسايزM</td>\n",
       "      <td>2.190258e+09</td>\n",
       "      <td>1972</td>\n",
       "      <td>دستبند خرد طلا سایز M</td>\n",
       "      <td>68</td>\n",
       "      <td>WISDOM</td>\n",
       "      <td>3</td>\n",
       "      <td>WISDOM</td>\n",
       "      <td>409</td>\n",
       "      <td>دستبند کاف خرد</td>\n",
       "      <td>...</td>\n",
       "      <td>Unknown Usage Type</td>\n",
       "      <td>Unknown Usage Space</td>\n",
       "      <td>DORSA JEWELRY</td>\n",
       "      <td>28</td>\n",
       "      <td>SPRING-SUMMER</td>\n",
       "      <td>1401</td>\n",
       "      <td>4648-1</td>\n",
       "      <td>محاسبه وزن طلا - 18</td>\n",
       "      <td>Unknown Dimensions</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2168 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Article Description  Base Unit Price  \\\n",
       "0                                       چلسي بوت نقش‌دال     1.174312e+08   \n",
       "1                             کيف پوچ زنانه کلکسيون آهنگ     8.874312e+07   \n",
       "2                                    کيف دستي زنانه کوچک     1.040909e+08   \n",
       "3                                      کيف پول عمودي آکو     4.500000e+07   \n",
       "4                   اسنيکر بندي زنانه ساقدار با زيره زرد     5.500000e+07   \n",
       "...                                                  ...              ...   \n",
       "45487   دستبندنقش دال پرکريستال سواروسکي طلا 14/66سايز M     8.541356e+08   \n",
       "45522  انگشتر شأن طلاباکريستال سواروسکي دودي 18.26 سا...     1.240587e+09   \n",
       "45523    دستبند خيال بغل دال با کريستال طلا 24/21 سايز S     1.261884e+09   \n",
       "45524       انگشتر دژ با راکس و کريستال طلا 16.36 سايز L     1.276972e+09   \n",
       "45538                     دستبند خرد طلا 28.18 سفيدسايزM     2.190258e+09   \n",
       "\n",
       "       Style Code                                  Style Description  \\\n",
       "0            1152                                   چلسی بوت نقش‌دال   \n",
       "1            1495                                           پوچ آهنگ   \n",
       "2             510           کیف زنانه مربعی کوچک با لوگو  65میل درسا   \n",
       "3             958                                  کیف پول عمودی اکو   \n",
       "4            1155                           اسنیکر بندی زنانه ساقدار   \n",
       "...           ...                                                ...   \n",
       "45487        1275  دستبند طلا یک ردیف توری نقش دال  پر کریستال سا...   \n",
       "45522        1979                              انگشتر شأن طلا سایز S   \n",
       "45523        1306                       دستبند خیال پر کریستال طلا S   \n",
       "45524        1783             انگشتر دژ با راکس و کریستال طلا سایز L   \n",
       "45538        1972                              دستبند خرد طلا سایز M   \n",
       "\n",
       "       Collection Code Collection Name  Idea Code Idea Description  \\\n",
       "0                    0           عمومي          0            عمومي   \n",
       "1                    0           عمومي          0            عمومي   \n",
       "2                    0           عمومي          0            عمومي   \n",
       "3                   44             AKO          0              AKO   \n",
       "4                    0           عمومي          0            عمومي   \n",
       "...                ...             ...        ...              ...   \n",
       "45487               63            DALL          3             DALL   \n",
       "45522               93             شأن          3              شأن   \n",
       "45523               83          KHIYAL          0           KHIYAL   \n",
       "45524               72       AUDACIOUS          3        AUDACIOUS   \n",
       "45538               68          WISDOM          3           WISDOM   \n",
       "\n",
       "       Initial Pattern Code  Initial Pattern Description  ...  \\\n",
       "0                       218             قالب 9717 مردانه  ...   \n",
       "1                       438                پوچ آهنگ_3206  ...   \n",
       "2                       262            کیف دسته دار-2400  ...   \n",
       "3                       328   کیف پول کوچک بای فولد-2286  ...   \n",
       "4                       220      قالب 3-049 اسنیکر زنانه  ...   \n",
       "...                     ...                          ...  ...   \n",
       "45487                   234  النگوی یک ردیف توری نقش دال  ...   \n",
       "45522                   494        انگشتر سه وجهی دو دور  ...   \n",
       "45523                   372    النگوی بغل دال پر کریستال  ...   \n",
       "45524                   498              Unknown Pattern  ...   \n",
       "45538                   409               دستبند کاف خرد  ...   \n",
       "\n",
       "               Usage Type          Usage Space          Brand          Size  \\\n",
       "0            SMART CASUAL  Unknown Usage Space          DORSA            42   \n",
       "1      Unknown Usage Type  Unknown Usage Space          DORSA  Unknown Size   \n",
       "2            SMART CASUAL  Unknown Usage Space          DORSA  Unknown Size   \n",
       "3            SMART CASUAL  Unknown Usage Space          DORSA  Unknown Size   \n",
       "4                  CASUAL  Unknown Usage Space          DORSA            37   \n",
       "...                   ...                  ...            ...           ...   \n",
       "45487  Unknown Usage Type  Unknown Usage Space  DORSA JEWELRY            14   \n",
       "45522  Unknown Usage Type  Unknown Usage Space  DORSA JEWELRY            18   \n",
       "45523  Unknown Usage Type  Unknown Usage Space  DORSA JEWELRY            24   \n",
       "45524  Unknown Usage Type  Unknown Usage Space  DORSA JEWELRY            16   \n",
       "45538  Unknown Usage Type  Unknown Usage Space  DORSA JEWELRY            28   \n",
       "\n",
       "              Season  Year                Model  \\\n",
       "0        FALL-WINTER  1402        AKM 6408-BOOT   \n",
       "1      SPRING-SUMMER  1402        Unknown Model   \n",
       "2                 S2  1403        Unknown Model   \n",
       "3      SPRING-SUMMER  1402        Unknown Model   \n",
       "4        FALL-WINTER  1399  AKM 4939-WOMEN BOOT   \n",
       "...              ...   ...                  ...   \n",
       "45487             S1  1403               3586-1   \n",
       "45522             S1  1403               4664-1   \n",
       "45523  SPRING-SUMMER  1402               3632-1   \n",
       "45524  SPRING-SUMMER  1400               4359-1   \n",
       "45538  SPRING-SUMMER  1401               4648-1   \n",
       "\n",
       "                                        Combined Feature  \\\n",
       "0      پنجه متوسط گرد - فرم رویه صاف - فرم زیره پاشنه...   \n",
       "1      چرم گاوی ناپا - تریم زرد - آستر نقش دال مشکی -...   \n",
       "2      لوگو فلزی پنجره ای 9-65میل - رنگ فلز طلایی - چ...   \n",
       "3      آستر نقش دال مشکی - فینیش مشکی - چرم گاوی ناپا...   \n",
       "4      پنجه متوسط گرد - فرم رویه بندی - فرم زیره پاشن...   \n",
       "...                                                  ...   \n",
       "45487                 محاسبه وزن طلا - 66 - کریستال 6862   \n",
       "45522  محاسبه وزن طلا - 26 - کریستال 6719 - کریستال 6...   \n",
       "45523  محاسبه وزن طلا - 21 - کریستال 6862 - کریستال 7223   \n",
       "45524  محاسبه وزن طلا - 36 - کریستال 6719 - کریستال 6669   \n",
       "45538                                محاسبه وزن طلا - 18   \n",
       "\n",
       "                   Dimensions articles_id  \n",
       "0          Unknown Dimensions        1152  \n",
       "1        L 38 * H 28 * D 3 CM        1495  \n",
       "2      L 24 * H 20.5 * D 9 CM         510  \n",
       "3      L 10 * H 12 * D 1.5 CM         958  \n",
       "4          Unknown Dimensions        1155  \n",
       "...                       ...         ...  \n",
       "45487      Unknown Dimensions        1275  \n",
       "45522      Unknown Dimensions        1979  \n",
       "45523      Unknown Dimensions        1306  \n",
       "45524      Unknown Dimensions        1783  \n",
       "45538      Unknown Dimensions        1972  \n",
       "\n",
       "[2168 rows x 29 columns]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Table(df=\n",
       "       Customer Category  customers_id Customer Process    City\n",
       "0                  عمومي    10121860.0              طلا   تبريز\n",
       "1                  عمومي    10447769.0             برنز   تهران\n",
       "2                  عمومي    10469286.0             برنز   تهران\n",
       "3                  عمومي    10075749.0        پلاتینیوم   تهران\n",
       "4                  عمومي    10432889.0             برنز   تهران\n",
       "...                  ...           ...              ...     ...\n",
       "176963             عمومي    10467100.0             نقره   اهواز\n",
       "176964             عمومي    10465852.0             نقره   تهران\n",
       "176966             عمومي    10204307.0             نقره  اصفهان\n",
       "176967             عمومي    10481987.0             نقره   تهران\n",
       "176968             عمومي    10477956.0             نقره   شيراز\n",
       "\n",
       "[156796 rows x 4 columns],\n",
       "  fkey_col_to_pkey_table={},\n",
       "  pkey_col=customers_id,\n",
       "  time_col=None)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.table_dict[\"customer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatetime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midxmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\pandas\\core\\indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[1;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\pandas\\core\\indexing.py:1752\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[1;32m-> 1752\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\pandas\\core\\indexing.py:1685\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1683\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[1;32m-> 1685\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "table.df.iloc[table.df[\"datetime\"].idxmax()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BranchCode                           10672.0\n",
       "Transaction Serial                    9800.0\n",
       "Time                                19:34:50\n",
       "customers_id                      10359133.0\n",
       "Style Code                            5034.0\n",
       "Quantity                                   1\n",
       "Base Total Price                 110000000.0\n",
       "Year_x                                1403.0\n",
       "Month_x                                  9.0\n",
       "Day                                     27.0\n",
       "Season                                 پاییز\n",
       "Allocated Credit                  17600000.0\n",
       "Sales Voucher                            0.0\n",
       "Gift Voucher                             0.0\n",
       "Cash Card                                0.0\n",
       "Special Discount                         0.0\n",
       "Promotion Discount                       0.0\n",
       "Small Change Discount                    0.0\n",
       "Total Discounts                   54727273.0\n",
       "Discount Percentage                     20.0\n",
       "Day_of_week                                3\n",
       "even_odd                                   1\n",
       "Weekday                              Tuesday\n",
       "Weekend                                    0\n",
       "Holiday                                    0\n",
       "event_name_1                             NaN\n",
       "event_type_1                             NaN\n",
       "articles_id                           5034.0\n",
       "datetime                 2024-12-17 19:34:50\n",
       "Name: 355215, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.df.iloc[table.df[\"datetime\"].idxmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rel-amazon',\n",
       " 'rel-avito',\n",
       " 'rel-event',\n",
       " 'rel-f1',\n",
       " 'rel-hm',\n",
       " 'rel-stack',\n",
       " 'rel-trial',\n",
       " 'Dorsa-aras669']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "register_dataset(\"Dorsa-aras669\", TransactionalDataset)\n",
    "get_dataset_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransactionalDataset()"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_dataset = get_dataset(\"Dorsa-aras669\")\n",
    "hyper_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2025-01-03 00:00:00'), Timestamp('2025-01-11 00:00:00'))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_dataset.val_timestamp, hyper_dataset.test_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import relbench\n",
    "\n",
    "relbench.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "from relbench.tasks import get_task, get_task_names, register_task\n",
    "from relbench.base import Database, EntityTask, RecommendationTask, Table, TaskType\n",
    "from relbench.metrics import (\n",
    "    accuracy,\n",
    "    average_precision,\n",
    "    f1,\n",
    "    link_prediction_map,\n",
    "    link_prediction_precision,\n",
    "    link_prediction_recall,\n",
    "    mae,\n",
    "    r2,\n",
    "    rmse,\n",
    "    roc_auc,\n",
    ")\n",
    "from metrics import link_prediction_top\n",
    "class UserItemPurchaseTask(RecommendationTask):\n",
    "    r\"\"\"Predict the list of articles each customer will purchase in the next seven\n",
    "    days.\"\"\"\n",
    "\n",
    "    task_type = TaskType.LINK_PREDICTION\n",
    "    src_entity_col = \"customer_id\"\n",
    "    src_entity_table = \"customer\"\n",
    "    dst_entity_col = \"article_id\"\n",
    "    dst_entity_table = \"article\"\n",
    "    time_col = \"timestamp\"\n",
    "    timedelta = pd.Timedelta(days=7)\n",
    "    metrics = [link_prediction_precision, link_prediction_recall, link_prediction_map, link_prediction_top]\n",
    "    eval_k = 12\n",
    "\n",
    "    def make_table(self, db: Database, timestamps: \"pd.Series[pd.Timestamp]\") -> Table:\n",
    "        customer = db.table_dict[\"customer\"].df\n",
    "        transactions = db.table_dict[\"transactions\"].df\n",
    "        timestamp_df = pd.DataFrame({\"timestamp\": timestamps})\n",
    "\n",
    "        df = duckdb.sql(\n",
    "            f\"\"\"\n",
    "            SELECT\n",
    "                t.timestamp,\n",
    "                transactions.customer_id,\n",
    "                LIST(DISTINCT transactions.article_id) AS article_id\n",
    "            FROM\n",
    "                timestamp_df t\n",
    "            LEFT JOIN\n",
    "                transactions\n",
    "            ON\n",
    "                transactions.t_dat > t.timestamp AND\n",
    "                transactions.t_dat <= t.timestamp + INTERVAL '{self.timedelta} days'\n",
    "            GROUP BY\n",
    "                t.timestamp,\n",
    "                transactions.customer_id\n",
    "            \"\"\"\n",
    "        ).df()\n",
    "\n",
    "        return Table(\n",
    "            df=df,\n",
    "            fkey_col_to_pkey_table={\n",
    "                self.src_entity_col: self.src_entity_table,\n",
    "                self.dst_entity_col: self.dst_entity_table,\n",
    "            },\n",
    "            pkey_col=None,\n",
    "            time_col=self.time_col,\n",
    "        )\n",
    "\n",
    "# Task 1: Predict articles each customer will purchase in the next 7 days\n",
    "class CustomerArticlePurchaseTask(RecommendationTask):\n",
    "    r\"\"\"Predict the list of articles each customer will purchase in the next seven days.\"\"\"\n",
    "    \n",
    "    task_type = TaskType.LINK_PREDICTION\n",
    "    src_entity_col = \"customers_id\"\n",
    "    src_entity_table = \"customer\"\n",
    "    dst_entity_col = \"articles_id\"\n",
    "    dst_entity_table = \"article\"\n",
    "    time_col = \"timestamp\"\n",
    "    timedelta = pd.Timedelta(days=7)\n",
    "    metrics = [link_prediction_precision, link_prediction_recall, link_prediction_map, link_prediction_top]\n",
    "    eval_k = 4\n",
    "\n",
    "    def make_table(self, db: Database, timestamps: \"pd.Series[pd.Timestamp]\") -> Table:\n",
    "        transactions = db.table_dict[\"transactions\"].df\n",
    "        timestamp_df = pd.DataFrame({\"timestamp\": timestamps})\n",
    "\n",
    "        df = duckdb.sql(\n",
    "            f\"\"\"\n",
    "            SELECT\n",
    "                t.timestamp,\n",
    "                transactions.customers_id,\n",
    "                LIST(DISTINCT transactions.articles_id) AS articles_id\n",
    "            FROM\n",
    "                timestamp_df t\n",
    "            LEFT JOIN\n",
    "                transactions\n",
    "            ON\n",
    "                transactions.datetime > t.timestamp AND\n",
    "                transactions.datetime <= t.timestamp + INTERVAL '{self.timedelta.days} days'\n",
    "            GROUP BY\n",
    "                t.timestamp,\n",
    "                transactions.customers_id\n",
    "            \"\"\"\n",
    "        ).df()\n",
    "\n",
    "        return Table(\n",
    "            df=df,\n",
    "            fkey_col_to_pkey_table={\n",
    "                self.src_entity_col: self.src_entity_table,\n",
    "                self.dst_entity_col: self.dst_entity_table,\n",
    "            },\n",
    "            pkey_col=None,\n",
    "            time_col=self.time_col,\n",
    "        )\n",
    "\n",
    "\n",
    "# Task 2: Predict customer churn (no purchases in the next week)\n",
    "class CustomerChurnTask(EntityTask):\n",
    "    r\"\"\"Predict the churn for a customer (no transactions) in the next 6 days.\"\"\"\n",
    "\n",
    "    task_type = TaskType.BINARY_CLASSIFICATION\n",
    "    entity_col = \"customers_id\"\n",
    "    entity_table = \"customer\"\n",
    "    time_col = \"timestamp\"\n",
    "    target_col = \"churn\"\n",
    "    timedelta = pd.Timedelta(days=7)\n",
    "    metrics = [average_precision, accuracy, f1, roc_auc]\n",
    "\n",
    "    def make_table(self, db: Database, timestamps: \"pd.Series[pd.Timestamp]\") -> Table:\n",
    "        customer = db.table_dict[\"customer\"].df\n",
    "        transactions = db.table_dict[\"transactions\"].df\n",
    "        timestamp_df = pd.DataFrame({\"timestamp\": timestamps})\n",
    "\n",
    "        df = duckdb.sql(\n",
    "            f\"\"\"\n",
    "            SELECT\n",
    "                timestamp,\n",
    "                customers_id,\n",
    "                CAST(\n",
    "                    NOT EXISTS (\n",
    "                        SELECT 1\n",
    "                        FROM transactions\n",
    "                        WHERE\n",
    "                            transactions.customers_id = customer.customers_id AND\n",
    "                            transactions.datetime > timestamp AND\n",
    "                            transactions.datetime <= timestamp + INTERVAL '{self.timedelta}'\n",
    "                    ) AS INTEGER\n",
    "                ) AS churn\n",
    "            FROM\n",
    "                timestamp_df,\n",
    "                customer\n",
    "            WHERE\n",
    "                EXISTS (\n",
    "                    SELECT 1\n",
    "                    FROM transactions\n",
    "                    WHERE\n",
    "                        transactions.customers_id = customer.customers_id AND\n",
    "                        transactions.datetime > timestamp - INTERVAL '{self.timedelta}' AND\n",
    "                        transactions.datetime <= timestamp\n",
    "                )\n",
    "            \"\"\"\n",
    "        ).df()\n",
    "\n",
    "        return Table(\n",
    "            df=df,\n",
    "            fkey_col_to_pkey_table={self.entity_col: self.entity_table},\n",
    "            pkey_col=None,\n",
    "            time_col=self.time_col,\n",
    "        ) \n",
    "    # def make_table(self, db: Database, timestamps: \"pd.Series[pd.Timestamp]\") -> Table:\n",
    "    #     transactions = db.table_dict[\"transactions\"].df\n",
    "    #     customer = db.table_dict[\"customer\"].df\n",
    "    #     timestamp_df = pd.DataFrame({\"timestamp\": timestamps})\n",
    "\n",
    "    #     df = duckdb.sql(\n",
    "    #         f\"\"\"\n",
    "    #         SELECT\n",
    "    #             t.timestamp,\n",
    "    #             c.customers_id,\n",
    "    #             CAST(\n",
    "    #                 NOT EXISTS (\n",
    "    #                     SELECT 1\n",
    "    #                     FROM transactions\n",
    "    #                     WHERE\n",
    "    #                         transactions.customers_id = c.customers_id AND\n",
    "    #                         transactions.datetime > t.timestamp AND\n",
    "    #                         transactions.datetime <= t.timestamp + INTERVAL '{self.timedelta.days} days'\n",
    "    #                 ) AS INTEGER\n",
    "    #             ) AS churn\n",
    "    #         FROM\n",
    "    #             timestamp_df t,\n",
    "    #             customer c\n",
    "    #         WHERE\n",
    "    #             EXISTS (\n",
    "    #                 SELECT 1\n",
    "    #                 FROM transactions\n",
    "    #                 WHERE\n",
    "    #                     transactions.customers_id = c.customers_id AND\n",
    "    #                     transactions.datetime > t.timestamp - INTERVAL '{self.timedelta.days} days' AND\n",
    "    #                     transactions.datetime <= t.timestamp\n",
    "    #             )\n",
    "    #         \"\"\"\n",
    "    #     ).df()\n",
    "\n",
    "    #     return Table(\n",
    "    #         df=df,\n",
    "    #         fkey_col_to_pkey_table={self.entity_col: self.entity_table},\n",
    "    #         pkey_col=None,\n",
    "    #         time_col=self.time_col,\n",
    "    #     )\n",
    "\n",
    "\n",
    "# Task 3: Predict article sales in the next 7 days\n",
    "class ArticleSalesTask(EntityTask):\n",
    "    r\"\"\"Predict the total sales for an article (sum of `price_purchase`) in the next 7 days.\"\"\"\n",
    "    \n",
    "    task_type = TaskType.REGRESSION\n",
    "    entity_col = \"articles_id\"\n",
    "    entity_table = \"article\"\n",
    "    time_col = \"datetime\"\n",
    "    target_col = \"sales\"\n",
    "    timedelta = pd.Timedelta(days=7)\n",
    "    metrics = [r2, mae, rmse]\n",
    "\n",
    "    def make_table(self, db: Database, timestamps: \"pd.Series[pd.Timestamp]\") -> Table:\n",
    "        transactions = db.table_dict[\"transactions\"].df\n",
    "        articles = db.table_dict[\"article\"].df\n",
    "        timestamp_df = pd.DataFrame({\"timestamp\": timestamps})\n",
    "\n",
    "        df = duckdb.sql(\n",
    "            f\"\"\"\n",
    "            SELECT\n",
    "                t.timestamp,\n",
    "                a.articles_id,\n",
    "                COALESCE(SUM(transactions.price_purchase), 0) AS sales\n",
    "            FROM\n",
    "                timestamp_df t,\n",
    "                article a\n",
    "            LEFT JOIN\n",
    "                transactions\n",
    "            ON\n",
    "                transactions.articles_id = a.articles_id AND\n",
    "                transactions.datetime > t.timestamp AND\n",
    "                transactions.datetime <= t.timestamp + INTERVAL '{self.timedelta.days} days'\n",
    "            GROUP BY\n",
    "                t.timestamp,\n",
    "                a.articles_id\n",
    "            \"\"\"\n",
    "        ).df()\n",
    "\n",
    "        return Table(\n",
    "            df=df,\n",
    "            fkey_col_to_pkey_table={self.entity_col: self.entity_table},\n",
    "            pkey_col=None,\n",
    "            time_col=self.time_col,\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomerArticlePurchaseTask(dataset=TransactionalDataset())"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aras_recom_task = CustomerArticlePurchaseTask(hyper_dataset, cache_dir=\"D:/Dani/relbench/relbench/cache/hyper_ar9564a656678ks4484386789207487\")\n",
    "aras_recom_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aras_recom_task6669']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "register_task(\"Dorsa-aras669\", \"aras_recom_task6669\", CustomerArticlePurchaseTask)\n",
    "get_task_names(\"Dorsa-aras669\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making task table for train split from scratch...\n",
      "(You can also use `get_task(..., download=True)` for tasks prepared by the RelBench team.)\n",
      "Making Database object from scratch...\n",
      "(You can also use `get_dataset(..., download=True)` for datasets prepared by the RelBench team.)\n",
      "Done in 3.93 seconds.\n",
      "Caching Database object to C:\\Users\\KN2C\\AppData\\Local\\relbench\\relbench\\Cache/Dorsa-aras669/db...\n",
      "Done in 0.52 seconds.\n",
      "Loading Database object from C:\\Users\\KN2C\\AppData\\Local\\relbench\\relbench\\Cache/Dorsa-aras669/db...\n",
      "Done in 0.36 seconds.\n",
      "Done in 9.75 seconds.\n",
      "Making task table for val split from scratch...\n",
      "(You can also use `get_task(..., download=True)` for tasks prepared by the RelBench team.)\n",
      "Done in 0.03 seconds.\n",
      "Making task table for test split from scratch...\n",
      "(You can also use `get_task(..., download=True)` for tasks prepared by the RelBench team.)\n",
      "Loading Database object from C:\\Users\\KN2C\\AppData\\Local\\relbench\\relbench\\Cache/Dorsa-aras669/db...\n",
      "Done in 0.12 seconds.\n",
      "Done in 0.17 seconds.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torch.nn import BCEWithLogitsLoss, L1Loss\n",
    "from relbench.datasets import get_dataset\n",
    "from relbench.tasks import get_task\n",
    "\n",
    "dataset = get_dataset(\"Dorsa-aras669\")\n",
    "task = get_task(\"Dorsa-aras669\", \"aras_recom_task6669\")\n",
    "\n",
    "\n",
    "train_table = task.get_table(\"train\")\n",
    "val_table = task.get_table(\"val\")\n",
    "test_table = task.get_table(\"test\")\n",
    "\n",
    "out_channels = 1\n",
    "loss_fn = BCEWithLogitsLoss()\n",
    "tune_metric = \"link_prediction_map\"\n",
    "higher_is_better = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Table(df=\n",
       "       timestamp  customers_id articles_id\n",
       "0     2022-01-14         27706      [1933]\n",
       "1     2021-12-24          4488        [80]\n",
       "2     2021-12-17         12132       [556]\n",
       "3     2021-10-22         20384       [556]\n",
       "4     2021-07-23          2170      [1104]\n",
       "...          ...           ...         ...\n",
       "25086 2021-09-17         23967      [1612]\n",
       "25087 2021-09-03          7373       [679]\n",
       "25088 2023-08-18         42718      [1180]\n",
       "25089 2024-01-26         43030      [1915]\n",
       "25090 2022-09-02          1469       [556]\n",
       "\n",
       "[25091 rows x 3 columns],\n",
       "  fkey_col_to_pkey_table={'customers_id': 'customer', 'articles_id': 'article'},\n",
       "  pkey_col=None,\n",
       "  time_col=timestamp)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Table(df=\n",
       "    timestamp  customers_id        articles_id\n",
       "0  2025-01-03         32179             [2148]\n",
       "1  2025-01-03         21862             [2127]\n",
       "2  2025-01-03         19916              [361]\n",
       "3  2025-01-03         28237             [1559]\n",
       "4  2025-01-03         11875        [679, 1253]\n",
       "5  2025-01-03         15920             [1481]\n",
       "6  2025-01-03         27469       [1195, 1267]\n",
       "7  2025-01-03         42781             [2127]\n",
       "8  2025-01-03         18498              [361]\n",
       "9  2025-01-03          7845             [2148]\n",
       "10 2025-01-03         25184  [1517, 1616, 111]\n",
       "11 2025-01-03         42928       [2127, 1481]\n",
       "12 2025-01-03         20404              [176]\n",
       "13 2025-01-03          1803       [2055, 1628]\n",
       "14 2025-01-03         32628              [361]\n",
       "15 2025-01-03         37515             [1788]\n",
       "16 2025-01-03         43112              [366]\n",
       "17 2025-01-03         18705  [111, 1517, 1650]\n",
       "18 2025-01-03         36303              [177]\n",
       "19 2025-01-03         10433       [1504, 2148]\n",
       "20 2025-01-03         43044   [1267, 554, 653],\n",
       "  fkey_col_to_pkey_table={'customers_id': 'customer', 'articles_id': 'article'},\n",
       "  pkey_col=None,\n",
       "  time_col=timestamp)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Table(df=\n",
       "    timestamp  customers_id   articles_id\n",
       "0  2025-01-11         30891  [1616, 1650]\n",
       "1  2025-01-11         33440        [2127]\n",
       "2  2025-01-11         26934         [554]\n",
       "3  2025-01-11          3241        [2127]\n",
       "4  2025-01-11         24667         [111]\n",
       "5  2025-01-11         36365        [2127]\n",
       "6  2025-01-11         43114        [1315]\n",
       "7  2025-01-11         42928  [1383, 1808]\n",
       "8  2025-01-11         39877        [2127]\n",
       "9  2025-01-11         30074        [2148]\n",
       "10 2025-01-11         20843        [1631]\n",
       "11 2025-01-11           407        [2127]\n",
       "12 2025-01-11          2356         [361]\n",
       "13 2025-01-11         38833   [2127, 199]\n",
       "14 2025-01-11         16398        [1517]\n",
       "15 2025-01-11         21401         [554]\n",
       "16 2025-01-11           378         [361]\n",
       "17 2025-01-11         30227        [2148]\n",
       "18 2025-01-11         10445         [199]\n",
       "19 2025-01-11         41792  [1520, 1074]\n",
       "20 2025-01-11         42129        [2127],\n",
       "  fkey_col_to_pkey_table={'customers_id': 'customer', 'articles_id': 'article'},\n",
       "  pkey_col=None,\n",
       "  time_col=timestamp)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "import torch_frame\n",
    "\n",
    "# Some book keeping\n",
    "from torch_geometric.seed import seed_everything\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)  # check that it's cuda if you want it to run in reasonable time!\n",
    "root_dir = \"D:/Dani/relbench/relbench/data_ARAS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article': {'Article Description': <stype.text_embedded: 'text_embedded'>,\n",
       "  'Base Unit Price': <stype.numerical: 'numerical'>,\n",
       "  'Style Code': <stype.numerical: 'numerical'>,\n",
       "  'Style Description': <stype.text_embedded: 'text_embedded'>,\n",
       "  'Collection Code': <stype.numerical: 'numerical'>,\n",
       "  'Collection Name': <stype.text_embedded: 'text_embedded'>,\n",
       "  'Idea Code': <stype.categorical: 'categorical'>,\n",
       "  'Idea Description': <stype.text_embedded: 'text_embedded'>,\n",
       "  'Initial Pattern Code': <stype.numerical: 'numerical'>,\n",
       "  'Initial Pattern Description': <stype.text_embedded: 'text_embedded'>,\n",
       "  'Product Category': <stype.text_embedded: 'text_embedded'>,\n",
       "  'Product Group': <stype.text_embedded: 'text_embedded'>,\n",
       "  'Product Subgroup': <stype.text_embedded: 'text_embedded'>,\n",
       "  'Product Type': <stype.text_embedded: 'text_embedded'>,\n",
       "  'Material Category': <stype.text_embedded: 'text_embedded'>,\n",
       "  'Material Type': <stype.text_embedded: 'text_embedded'>,\n",
       "  'Processing': <stype.text_embedded: 'text_embedded'>,\n",
       "  'Color': <stype.text_embedded: 'text_embedded'>,\n",
       "  'User': <stype.text_embedded: 'text_embedded'>,\n",
       "  'Usage Type': <stype.categorical: 'categorical'>,\n",
       "  'Usage Space': <stype.text_embedded: 'text_embedded'>,\n",
       "  'Brand': <stype.text_embedded: 'text_embedded'>,\n",
       "  'Size': <stype.text_embedded: 'text_embedded'>,\n",
       "  'Season': <stype.categorical: 'categorical'>,\n",
       "  'Year': <stype.categorical: 'categorical'>,\n",
       "  'Model': <stype.text_embedded: 'text_embedded'>,\n",
       "  'Combined Feature': <stype.text_embedded: 'text_embedded'>,\n",
       "  'Dimensions': <stype.text_embedded: 'text_embedded'>,\n",
       "  'articles_id': <stype.numerical: 'numerical'>},\n",
       " 'branches': {'BranchCode': <stype.numerical: 'numerical'>,\n",
       "  'Branch Name': <stype.text_embedded: 'text_embedded'>,\n",
       "  'City': <stype.text_embedded: 'text_embedded'>},\n",
       " 'customer': {'Customer Category': <stype.categorical: 'categorical'>,\n",
       "  'customers_id': <stype.numerical: 'numerical'>,\n",
       "  'Customer Process': <stype.text_embedded: 'text_embedded'>,\n",
       "  'City': <stype.categorical: 'categorical'>},\n",
       " 'transactions': {'BranchCode': <stype.numerical: 'numerical'>,\n",
       "  'Transaction Serial': <stype.numerical: 'numerical'>,\n",
       "  'Time': <stype.timestamp: 'timestamp'>,\n",
       "  'customers_id': <stype.numerical: 'numerical'>,\n",
       "  'Style Code': <stype.numerical: 'numerical'>,\n",
       "  'Quantity': <stype.numerical: 'numerical'>,\n",
       "  'Base Total Price': <stype.numerical: 'numerical'>,\n",
       "  'Year_x': <stype.numerical: 'numerical'>,\n",
       "  'Month_x': <stype.numerical: 'numerical'>,\n",
       "  'Day': <stype.numerical: 'numerical'>,\n",
       "  'Season': <stype.categorical: 'categorical'>,\n",
       "  'Allocated Credit': <stype.numerical: 'numerical'>,\n",
       "  'Sales Voucher': <stype.numerical: 'numerical'>,\n",
       "  'Gift Voucher': <stype.numerical: 'numerical'>,\n",
       "  'Cash Card': <stype.numerical: 'numerical'>,\n",
       "  'Special Discount': <stype.numerical: 'numerical'>,\n",
       "  'Promotion Discount': <stype.numerical: 'numerical'>,\n",
       "  'Small Change Discount': <stype.numerical: 'numerical'>,\n",
       "  'Total Discounts': <stype.numerical: 'numerical'>,\n",
       "  'Discount Percentage': <stype.numerical: 'numerical'>,\n",
       "  'Day_of_week': <stype.categorical: 'categorical'>,\n",
       "  'even_odd': <stype.categorical: 'categorical'>,\n",
       "  'Weekday': <stype.categorical: 'categorical'>,\n",
       "  'Weekend': <stype.categorical: 'categorical'>,\n",
       "  'Holiday': <stype.categorical: 'categorical'>,\n",
       "  'event_name_1': <stype.text_embedded: 'text_embedded'>,\n",
       "  'event_type_1': <stype.categorical: 'categorical'>,\n",
       "  'articles_id': <stype.numerical: 'numerical'>,\n",
       "  'datetime': <stype.timestamp: 'timestamp'>}}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from relbench.modeling.utils import get_stype_proposal\n",
    "\n",
    "db = dataset.get_db()\n",
    "col_to_stype_dict = get_stype_proposal(db)\n",
    "col_to_stype_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article': {'Article Description': <stype.text_embedded: 'text_embedded'>, 'Base Unit Price': <stype.numerical: 'numerical'>, 'Style Code': <stype.categorical: 'categorical'>, 'Style Description': <stype.text_embedded: 'text_embedded'>, 'Collection Code': <stype.categorical: 'categorical'>, 'Collection Name': <stype.text_embedded: 'text_embedded'>, 'Idea Code': <stype.categorical: 'categorical'>, 'Idea Description': <stype.text_embedded: 'text_embedded'>, 'Initial Pattern Code': <stype.categorical: 'categorical'>, 'Initial Pattern Description': <stype.text_embedded: 'text_embedded'>, 'Product Category': <stype.text_embedded: 'text_embedded'>, 'Product Group': <stype.text_embedded: 'text_embedded'>, 'Product Subgroup': <stype.text_embedded: 'text_embedded'>, 'Product Type': <stype.text_embedded: 'text_embedded'>, 'Material Category': <stype.text_embedded: 'text_embedded'>, 'Material Type': <stype.text_embedded: 'text_embedded'>, 'Processing': <stype.text_embedded: 'text_embedded'>, 'Color': <stype.text_embedded: 'text_embedded'>, 'User': <stype.text_embedded: 'text_embedded'>, 'Usage Type': <stype.categorical: 'categorical'>, 'Usage Space': <stype.text_embedded: 'text_embedded'>, 'Brand': <stype.text_embedded: 'text_embedded'>, 'Size': <stype.text_embedded: 'text_embedded'>, 'Season': <stype.categorical: 'categorical'>, 'Year': <stype.categorical: 'categorical'>, 'Model': <stype.text_embedded: 'text_embedded'>, 'Combined Feature': <stype.text_embedded: 'text_embedded'>, 'Dimensions': <stype.text_embedded: 'text_embedded'>, 'articles_id': <stype.numerical: 'numerical'>}, 'branches': {'BranchCode': <stype.numerical: 'numerical'>, 'Branch Name': <stype.text_embedded: 'text_embedded'>, 'City': <stype.text_embedded: 'text_embedded'>}, 'customer': {'Customer Category': <stype.categorical: 'categorical'>, 'customers_id': <stype.numerical: 'numerical'>, 'Customer Process': <stype.text_embedded: 'text_embedded'>, 'City': <stype.categorical: 'categorical'>}, 'transactions': {'BranchCode': <stype.numerical: 'numerical'>, 'Transaction Serial': <stype.numerical: 'numerical'>, 'Time': <stype.timestamp: 'timestamp'>, 'customers_id': <stype.numerical: 'numerical'>, 'Style Code': <stype.numerical: 'numerical'>, 'Quantity': <stype.numerical: 'numerical'>, 'Base Total Price': <stype.numerical: 'numerical'>, 'Year_x': <stype.categorical: 'categorical'>, 'Month_x': <stype.categorical: 'categorical'>, 'Day': <stype.numerical: 'numerical'>, 'Season': <stype.categorical: 'categorical'>, 'Allocated Credit': <stype.numerical: 'numerical'>, 'Sales Voucher': <stype.numerical: 'numerical'>, 'Gift Voucher': <stype.numerical: 'numerical'>, 'Cash Card': <stype.numerical: 'numerical'>, 'Special Discount': <stype.numerical: 'numerical'>, 'Promotion Discount': <stype.numerical: 'numerical'>, 'Small Change Discount': <stype.numerical: 'numerical'>, 'Total Discounts': <stype.numerical: 'numerical'>, 'Discount Percentage': <stype.numerical: 'numerical'>, 'Day_of_week': <stype.categorical: 'categorical'>, 'even_odd': <stype.categorical: 'categorical'>, 'Weekday': <stype.categorical: 'categorical'>, 'Weekend': <stype.categorical: 'categorical'>, 'Holiday': <stype.categorical: 'categorical'>, 'event_name_1': <stype.text_embedded: 'text_embedded'>, 'event_type_1': <stype.categorical: 'categorical'>, 'articles_id': <stype.numerical: 'numerical'>, 'datetime': <stype.timestamp: 'timestamp'>}}\n"
     ]
    }
   ],
   "source": [
    "# from relbench.modeling.schema import Stype\n",
    "from torch_frame import stype\n",
    "# Modify specific column stype manually\n",
    "col_to_stype_dict['article']['Style Code'] = stype.categorical  \n",
    "col_to_stype_dict['article']['Collection Code'] = stype.categorical  \n",
    "col_to_stype_dict['article']['Idea Code'] = stype.categorical  \n",
    "col_to_stype_dict['article']['Initial Pattern Code'] = stype.categorical  \n",
    " \n",
    "col_to_stype_dict['transactions']['Year_x'] = stype.categorical \n",
    "col_to_stype_dict['transactions']['Month_x'] = stype.categorical \n",
    "# col_to_stype_dict['transactions']['Day'] = stype.sequence_numerical\n",
    "# Now you can use col_to_stype_dict in your modeling pipeline\n",
    "print(col_to_stype_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch import Tensor\n",
    "import torch\n",
    "\n",
    "class BertPersianTextEmbedding:\n",
    "    def __init__(self, device: Optional[torch.device] = None):\n",
    "        # Replace the model with a Persian BERT model\n",
    "        self.model = SentenceTransformer(\"HooshvareLab/bert-fa-zwnj-base\",  # Example Persian BERT model\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    def __call__(self, sentences: List[str]) -> Tensor:\n",
    "        # Encode the sentences using the Persian BERT model and return as a tensor\n",
    "        return torch.from_numpy(self.model.encode(sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name HooshvareLab/bert-fa-zwnj-base. Creating a new one with mean pooling.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at HooshvareLab/bert-fa-zwnj-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch_frame\\data\\stats.py:177: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  ser = pd.to_datetime(ser, format=time_format)\n",
      "c:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch_frame\\data\\mapper.py:290: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  ser = pd.to_datetime(ser, format=self.format, errors='coerce')\n",
      "Embedding raw data in mini-batch: 100%|██████████| 4946/4946 [03:36<00:00, 22.85it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch_frame.config.text_embedder import TextEmbedderConfig\n",
    "from relbench.modeling.graph import make_pkey_fkey_graph\n",
    "\n",
    "text_embedder_cfg = TextEmbedderConfig(\n",
    "    text_embedder=BertPersianTextEmbedding(device=device), batch_size=64\n",
    ")\n",
    "\n",
    "data, col_stats_dict = make_pkey_fkey_graph(\n",
    "    db,\n",
    "    col_to_stype_dict=col_to_stype_dict,  # speficied column types\n",
    "    text_embedder_cfg=text_embedder_cfg,  # our chosen text encoder\n",
    "    cache_dir=os.path.join(\n",
    "        root_dir, f\"rel-aras_recom_materialized_cache7822322232\"\n",
    "    ),  # store materialized graph for convenience\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  article={ tf=TensorFrame([2168, 28]) },\n",
       "  branches={ tf=TensorFrame([75, 2]) },\n",
       "  customer={ tf=TensorFrame([43453, 3]) },\n",
       "  transactions={\n",
       "    tf=TensorFrame([316487, 26]),\n",
       "    time=[316487],\n",
       "  },\n",
       "  (transactions, f2p_articles_id, article)={ edge_index=[2, 36410] },\n",
       "  (article, rev_f2p_articles_id, transactions)={ edge_index=[2, 36410] },\n",
       "  (transactions, f2p_customers_id, customer)={ edge_index=[2, 316487] },\n",
       "  (customer, rev_f2p_customers_id, transactions)={ edge_index=[2, 316487] },\n",
       "  (transactions, f2p_BranchCode, branches)={ edge_index=[2, 316487] },\n",
       "  (branches, rev_f2p_BranchCode, transactions)={ edge_index=[2, 316487] }\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RelBench Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Embedding, ModuleDict\n",
    "from torch_frame.data.stats import StatType\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import MLP\n",
    "from torch_geometric.typing import NodeType\n",
    "\n",
    "from relbench.modeling.nn import HeteroEncoder, HeteroGraphSAGE, HeteroTemporalEncoder\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: HeteroData,\n",
    "        col_stats_dict: Dict[str, Dict[str, Dict[StatType, Any]]],\n",
    "        num_layers: int,\n",
    "        channels: int,\n",
    "        out_channels: int,\n",
    "        aggr: str,\n",
    "        norm: str,\n",
    "        # List of node types to add shallow embeddings to input\n",
    "        shallow_list: List[NodeType] = [],\n",
    "        # ID awareness\n",
    "        id_awareness: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = HeteroEncoder(\n",
    "            channels=channels,\n",
    "            node_to_col_names_dict={\n",
    "                node_type: data[node_type].tf.col_names_dict\n",
    "                for node_type in data.node_types\n",
    "            },\n",
    "            node_to_col_stats=col_stats_dict,\n",
    "        )\n",
    "        self.temporal_encoder = HeteroTemporalEncoder(\n",
    "            node_types=[\n",
    "                node_type for node_type in data.node_types if \"time\" in data[node_type]\n",
    "            ],\n",
    "            channels=channels,\n",
    "        )\n",
    "        self.gnn = HeteroGraphSAGE(\n",
    "            node_types=data.node_types,\n",
    "            edge_types=data.edge_types,\n",
    "            channels=channels,\n",
    "            aggr=aggr,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.head = MLP(\n",
    "            channels,\n",
    "            out_channels=out_channels,\n",
    "            norm=norm,\n",
    "            num_layers=1,\n",
    "        )\n",
    "        self.embedding_dict = ModuleDict(\n",
    "            {\n",
    "                node: Embedding(data.num_nodes_dict[node], channels)\n",
    "                for node in shallow_list\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.id_awareness_emb = None\n",
    "        if id_awareness:\n",
    "            self.id_awareness_emb = torch.nn.Embedding(1, channels)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.encoder.reset_parameters()\n",
    "        self.temporal_encoder.reset_parameters()\n",
    "        self.gnn.reset_parameters()\n",
    "        self.head.reset_parameters()\n",
    "        for embedding in self.embedding_dict.values():\n",
    "            torch.nn.init.normal_(embedding.weight, std=0.1)\n",
    "        if self.id_awareness_emb is not None:\n",
    "            self.id_awareness_emb.reset_parameters()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        batch: HeteroData,\n",
    "        entity_table: NodeType,\n",
    "    ) -> Tensor:\n",
    "        seed_time = batch[entity_table].seed_time\n",
    "        x_dict = self.encoder(batch.tf_dict)\n",
    "\n",
    "        rel_time_dict = self.temporal_encoder(\n",
    "            seed_time, batch.time_dict, batch.batch_dict\n",
    "        )\n",
    "\n",
    "        for node_type, rel_time in rel_time_dict.items():\n",
    "            x_dict[node_type] = x_dict[node_type] + rel_time\n",
    "\n",
    "        for node_type, embedding in self.embedding_dict.items():\n",
    "            x_dict[node_type] = x_dict[node_type] + embedding(batch[node_type].n_id)\n",
    "\n",
    "        x_dict = self.gnn(\n",
    "            x_dict,\n",
    "            batch.edge_index_dict,\n",
    "            batch.num_sampled_nodes_dict,\n",
    "            batch.num_sampled_edges_dict,\n",
    "        )\n",
    "\n",
    "        return self.head(x_dict[entity_table][: seed_time.size(0)])\n",
    "\n",
    "    def forward_dst_readout(\n",
    "        self,\n",
    "        batch: HeteroData,\n",
    "        entity_table: NodeType,\n",
    "        dst_table: NodeType,\n",
    "    ) -> Tensor:\n",
    "        if self.id_awareness_emb is None:\n",
    "            raise RuntimeError(\n",
    "                \"id_awareness must be set True to use forward_dst_readout\"\n",
    "            )\n",
    "        seed_time = batch[entity_table].seed_time\n",
    "        x_dict = self.encoder(batch.tf_dict)\n",
    "        # Add ID-awareness to the root node\n",
    "        x_dict[entity_table][: seed_time.size(0)] += self.id_awareness_emb.weight\n",
    "\n",
    "        rel_time_dict = self.temporal_encoder(\n",
    "            seed_time, batch.time_dict, batch.batch_dict\n",
    "        )\n",
    "\n",
    "        for node_type, rel_time in rel_time_dict.items():\n",
    "            x_dict[node_type] = x_dict[node_type] + rel_time\n",
    "\n",
    "        for node_type, embedding in self.embedding_dict.items():\n",
    "            x_dict[node_type] = x_dict[node_type] + embedding(batch[node_type].n_id)\n",
    "\n",
    "        x_dict = self.gnn(\n",
    "            x_dict,\n",
    "            batch.edge_index_dict,\n",
    "        )\n",
    "\n",
    "        return self.head(x_dict[dst_table])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# from model import Model\n",
    "# from text_embedder import GloveTextEmbedding\n",
    "from torch import Tensor\n",
    "from torch_frame import stype\n",
    "from torch_frame.config.text_embedder import TextEmbedderConfig\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.seed import seed_everything\n",
    "from tqdm import tqdm\n",
    "\n",
    "from relbench.base import Dataset, RecommendationTask, TaskType\n",
    "from relbench.datasets import get_dataset\n",
    "from relbench.modeling.graph import get_link_train_table_input, make_pkey_fkey_graph\n",
    "from relbench.modeling.loader import SparseTensor\n",
    "from relbench.modeling.utils import get_stype_proposal\n",
    "from relbench.tasks import get_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KN2C\\Desktop\\Dani\\relbench\\relbench\\modeling\\graph.py:217: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\SparseCsrTensorImpl.cpp:55.)\n",
      "  dst_node_indices = sparse_coo.to_sparse_csr()\n"
     ]
    }
   ],
   "source": [
    "# Initialize the loader dictionary\n",
    "loader_dict: Dict[str, NeighborLoader] = {}\n",
    "dst_nodes_dict: Dict[str, Tuple[NodeType, Tensor]] = {}\n",
    "\n",
    "# Loop over the train, val, and test splits\n",
    "for split, table in [\n",
    "    (\"train\", train_table),\n",
    "    (\"val\", val_table),\n",
    "    (\"test\", test_table),\n",
    "]:\n",
    "    # Get link train table input for link prediction task\n",
    "    table_input = get_link_train_table_input(\n",
    "        table=table,\n",
    "        task=task,\n",
    "    )\n",
    "    \n",
    "    # Save destination nodes for later use\n",
    "    dst_nodes_dict[split] = table_input.dst_nodes\n",
    "\n",
    "    # Create NeighborLoader for link prediction\n",
    "    loader_dict[split] = NeighborLoader(\n",
    "        data,\n",
    "        num_neighbors=[128 for _ in range(2)],  # Sample subgraphs of depth 2, 128 neighbors per node\n",
    "        time_attr=\"time\",  # Use time attribute if available\n",
    "        input_nodes=table_input.src_nodes,  # Source nodes for link prediction\n",
    "        input_time=table_input.src_time,  # Use src_time if time data is available\n",
    "        subgraph_type=\"bidirectional\",\n",
    "        batch_size=512,\n",
    "        temporal_strategy=\"last\",  # Uniform sampling strategy for time\n",
    "        shuffle=split == \"train\",  # Shuffle only during training\n",
    "        num_workers=0,\n",
    "        persistent_workers=False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model for link prediction task\n",
    "model = Model(\n",
    "    data=data,  # Heterogeneous data object\n",
    "    col_stats_dict=col_stats_dict,  # Column statistics dictionary\n",
    "    num_layers=2,  # Adjust this to match your desired architecture (depth of GNN)\n",
    "    channels=128,  # Number of hidden channels in GNN layers\n",
    "    out_channels=1,  # Output size (for link prediction, usually a scalar per edge)\n",
    "    aggr=\"sum\",  # Aggregation method (can be \"sum\", \"mean\", etc.)\n",
    "    norm=\"layer_norm\",  # Normalization method\n",
    "    id_awareness=True,  # Whether the model is aware of node IDs\n",
    ").to(device)  # Move model to the appropriate device (e.g., GPU)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Use the desired learning rate\n",
    "\n",
    "# Handling sparse destination nodes for training\n",
    "# dst_nodes_dict stores the destination nodes for the \"train\" split (in sparse format)\n",
    "train_sparse_tensor = SparseTensor(dst_nodes_dict[\"train\"][1], device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train() -> float:\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    loss_accum = count_accum = 0\n",
    "    steps = 0\n",
    "    total_steps = min(len(loader_dict[\"train\"]), 2000)  # Change the max_steps_per_epoch to 2000 or your preferred value\n",
    "\n",
    "    for batch in tqdm(loader_dict[\"train\"], total=total_steps):\n",
    "        batch = batch.to(device)  # Move batch data to device (GPU or CPU)\n",
    "\n",
    "        # Forward pass through the model for link prediction (source and destination tables)\n",
    "        out = model.forward_dst_readout(\n",
    "            batch, task.src_entity_table, task.dst_entity_table\n",
    "        ).flatten()  # Flatten the output\n",
    "\n",
    "        batch_size = batch[task.src_entity_table].batch_size  # Get batch size for the source entity table\n",
    "\n",
    "        # Get ground-truth labels\n",
    "        input_id = batch[task.src_entity_table].input_id  # Input IDs for the batch\n",
    "        src_batch, dst_index = train_sparse_tensor[input_id]  # Get the source and destination indices\n",
    "\n",
    "        # Get the target labels by checking if source-destination pairs exist\n",
    "        target = torch.isin(\n",
    "            batch[task.dst_entity_table].batch\n",
    "            + batch_size * batch[task.dst_entity_table].n_id,\n",
    "            src_batch + batch_size * dst_index,\n",
    "        ).float()  # Convert the result to float for loss computation\n",
    "\n",
    "        # Optimization\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        loss = F.binary_cross_entropy_with_logits(out, target)  # Compute binary cross-entropy loss\n",
    "        loss.backward()  # Backpropagation to compute gradients\n",
    "\n",
    "        optimizer.step()  # Update model parameters\n",
    "\n",
    "        # Accumulate the total loss and count for averaging later\n",
    "        loss_accum += float(loss) * out.numel()\n",
    "        count_accum += out.numel()\n",
    "\n",
    "        steps += 1\n",
    "        if steps >= total_steps:\n",
    "            break  # Break the loop if max steps per epoch is reached\n",
    "\n",
    "    # Handle the case where no data was sampled\n",
    "    if count_accum == 0:\n",
    "        warnings.warn(\n",
    "            f\"Did not sample a single '{task.dst_entity_table}' node in any mini-batch. \"\n",
    "            \"Try increasing the number of layers/hops or reducing the batch size.\"\n",
    "        )\n",
    "\n",
    "    # Return average loss for the epoch\n",
    "    return loss_accum / count_accum if count_accum > 0 else float(\"nan\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # No gradient computation for evaluation\n",
    "def test(loader: NeighborLoader) -> np.ndarray:\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    pred_list: list[Tensor] = []  # Store predictions\n",
    "    for batch in tqdm(loader):  # Iterate over batches in the test loader\n",
    "        batch = batch.to(device)  # Move the batch data to the device (GPU or CPU)\n",
    "\n",
    "        # Forward pass through the model for link prediction\n",
    "        out = (\n",
    "            model.forward_dst_readout(\n",
    "                batch, task.src_entity_table, task.dst_entity_table\n",
    "            )\n",
    "            .detach()\n",
    "            .flatten()  # Detach the output from the computational graph\n",
    "        )\n",
    "\n",
    "        batch_size = batch[task.src_entity_table].batch_size  # Get the batch size for source nodes\n",
    "\n",
    "        # Prepare a tensor to hold the scores for the source-destination pairs\n",
    "        scores = torch.zeros(batch_size, task.num_dst_nodes, device=out.device)\n",
    "\n",
    "        # Fill the scores with sigmoid activations for the destination nodes in the current batch\n",
    "        scores[\n",
    "            batch[task.dst_entity_table].batch, batch[task.dst_entity_table].n_id\n",
    "        ] = torch.sigmoid(out)  # Apply sigmoid activation to get probabilities\n",
    "\n",
    "        # Use top-k (e.g., top recommended items) based on the scores\n",
    "        _, pred_mini = torch.topk(scores, k=task.eval_k, dim=1)  # Get top-k predictions\n",
    "        pred_list.append(pred_mini)  # Append predictions to the list\n",
    "\n",
    "    # Concatenate all predictions and move to CPU for further processing\n",
    "    pred = torch.cat(pred_list, dim=0).cpu().numpy()\n",
    "\n",
    "    return pred  # Return the final predictions as a NumPy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [00:04<00:00, 12.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train loss: 0.3260339167547758, Val metrics: {'link_prediction_precision': np.float64(0.09523809523809523), 'link_prediction_recall': np.float64(0.30952380952380953), 'link_prediction_map': np.float64(0.2420634920634921), 'link_prediction_top': np.float64(0.38095238095238093)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [00:02<00:00, 18.15it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 52.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02, Train loss: 0.29487549611980124, Val metrics: {'link_prediction_precision': np.float64(0.10714285714285714), 'link_prediction_recall': np.float64(0.3333333333333333), 'link_prediction_map': np.float64(0.28174603174603174), 'link_prediction_top': np.float64(0.42857142857142855)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [00:02<00:00, 18.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 41.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03, Train loss: 0.2902663164754045, Val metrics: {'link_prediction_precision': np.float64(0.10714285714285714), 'link_prediction_recall': np.float64(0.3333333333333333), 'link_prediction_map': np.float64(0.28174603174603174), 'link_prediction_top': np.float64(0.42857142857142855)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [00:02<00:00, 18.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 43.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04, Train loss: 0.3096420116579359, Val metrics: {'link_prediction_precision': np.float64(0.10714285714285714), 'link_prediction_recall': np.float64(0.3333333333333333), 'link_prediction_map': np.float64(0.23809523809523808), 'link_prediction_top': np.float64(0.42857142857142855)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [00:02<00:00, 18.41it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 52.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Train loss: 0.2868384764465908, Val metrics: {'link_prediction_precision': np.float64(0.10714285714285714), 'link_prediction_recall': np.float64(0.3333333333333333), 'link_prediction_map': np.float64(0.246031746031746), 'link_prediction_top': np.float64(0.42857142857142855)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [00:02<00:00, 18.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 47.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06, Train loss: 0.2846905599187409, Val metrics: {'link_prediction_precision': np.float64(0.10714285714285714), 'link_prediction_recall': np.float64(0.3333333333333333), 'link_prediction_map': np.float64(0.246031746031746), 'link_prediction_top': np.float64(0.42857142857142855)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [00:02<00:00, 18.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 45.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07, Train loss: nan, Val metrics: {'link_prediction_precision': np.float64(0.10714285714285714), 'link_prediction_recall': np.float64(0.3333333333333333), 'link_prediction_map': np.float64(0.2341269841269841), 'link_prediction_top': np.float64(0.42857142857142855)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [00:02<00:00, 18.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 43.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08, Train loss: 0.2795940018904442, Val metrics: {'link_prediction_precision': np.float64(0.09523809523809523), 'link_prediction_recall': np.float64(0.30952380952380953), 'link_prediction_map': np.float64(0.21825396825396823), 'link_prediction_top': np.float64(0.38095238095238093)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [00:02<00:00, 18.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 53.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09, Train loss: 0.2782855828034849, Val metrics: {'link_prediction_precision': np.float64(0.10714285714285714), 'link_prediction_recall': np.float64(0.3333333333333333), 'link_prediction_map': np.float64(0.2341269841269841), 'link_prediction_top': np.float64(0.42857142857142855)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [00:02<00:00, 18.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 52.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train loss: 0.2808184058676759, Val metrics: {'link_prediction_precision': np.float64(0.09523809523809523), 'link_prediction_recall': np.float64(0.30952380952380953), 'link_prediction_map': np.float64(0.21825396825396823), 'link_prediction_top': np.float64(0.38095238095238093)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 51.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Val metrics: {'link_prediction_precision': np.float64(0.10714285714285714), 'link_prediction_recall': np.float64(0.3333333333333333), 'link_prediction_map': np.float64(0.28174603174603174), 'link_prediction_top': np.float64(0.42857142857142855)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 52.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best test metrics: {'link_prediction_precision': np.float64(0.023809523809523808), 'link_prediction_recall': np.float64(0.09523809523809523), 'link_prediction_map': np.float64(0.09523809523809523), 'link_prediction_top': np.float64(0.09523809523809523)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# Initialize variables for tracking the best model and best validation metrics\n",
    "state_dict = None  # This will hold the best model state\n",
    "best_val_metric = 0  # This will store the best validation metric\n",
    "epochs = 10  # Set the number of epochs (you can adjust as needed)\n",
    "eval_epochs_interval = 1  # Evaluate every 'n' epochs (change this based on your needs)\n",
    "# tune_metric = \"link_prediction_map\"  # Define the metric you are tuning\n",
    "\n",
    "# Training and evaluation loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # Run the training function\n",
    "    train_loss = train()\n",
    "    \n",
    "    # Perform evaluation every 'eval_epochs_interval' epochs\n",
    "    if epoch % eval_epochs_interval == 0:\n",
    "        # Run the validation on the validation dataset\n",
    "        val_pred = test(loader_dict[\"val\"])  # Get the predictions from the model\n",
    "        val_metrics = task.evaluate(val_pred, task.get_table(\"val\"))  # Evaluate predictions\n",
    "        \n",
    "        # Print the training loss and validation metrics\n",
    "        print(\n",
    "            f\"Epoch: {epoch:02d}, Train loss: {train_loss}, \"\n",
    "            f\"Val metrics: {val_metrics}\"\n",
    "        )\n",
    "\n",
    "        # Check if the current validation metric is the best\n",
    "        if val_metrics[tune_metric] > best_val_metric:\n",
    "            best_val_metric = val_metrics[tune_metric]  # Update best metric\n",
    "            state_dict = copy.deepcopy(model.state_dict())  # Save the best model state\n",
    "\n",
    "# After training, load the best model weights\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Evaluate the model on the validation set with the best weights\n",
    "val_pred = test(loader_dict[\"val\"])\n",
    "val_metrics = task.evaluate(val_pred, task.get_table(\"val\"))\n",
    "print(f\"Best Val metrics: {val_metrics}\")\n",
    "\n",
    "# Evaluate the model on the test set with the best weights\n",
    "test_pred = test(loader_dict[\"test\"])\n",
    "test_metrics = task.evaluate(test_pred)\n",
    "print(f\"Best test metrics: {test_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2163eeb32f0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from relbench.base import Dataset, RecommendationTask, TaskType\n",
    "from relbench.datasets import get_dataset\n",
    "from relbench.modeling.graph import (\n",
    "    get_link_train_table_input,\n",
    "    make_pkey_fkey_graph,\n",
    ")\n",
    "from relbench.modeling.loader import SparseTensor\n",
    "from relbench.modeling.utils import get_stype_proposal\n",
    "from relbench.tasks import get_task\n",
    "from torch import Tensor\n",
    "from torch_frame import stype\n",
    "from torch_frame.config.text_embedder import TextEmbedderConfig\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.seed import seed_everything\n",
    "from torch_geometric.typing import NodeType\n",
    "from torch_geometric.utils.cross_entropy import sparse_cross_entropy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from contextgnn.nn.models import IDGNN, ContextGNN, ShallowRHSGNN\n",
    "from contextgnn.utils import GloveTextEmbedding, RHSEmbeddingMode\n",
    "\n",
    "# Static configuration parameters\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "eval_epochs_interval = 1\n",
    "batch_size = 256\n",
    "channels = 128\n",
    "aggregation_method = \"sum\"\n",
    "num_layers = 3\n",
    "num_neighbors = 256\n",
    "temporal_strategy = \"last\"\n",
    "share_same_time = True\n",
    "max_steps_per_epoch = 2000\n",
    "num_workers = 0\n",
    "seed = 42\n",
    "model_name = \"idgnn\"  # For example, can be 'idgnn', 'contextgnn', or 'shallowrhsgnn'\n",
    "tune_metric = \"link_prediction_map\"  # Metric used to tune the model\n",
    "cache_dir = os.path.expanduser(\"~/.cache/relbench_examples\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define static num_neighbors for NeighborLoader\n",
    "num_neighbors = [num_neighbors // 2**i for i in range(num_layers)]\n",
    "\n",
    "# Loader dictionary for train, validation, and test sets\n",
    "loader_dict: Dict[str, NeighborLoader] = {}\n",
    "dst_nodes_dict: Dict[str, Tuple[NodeType, Tensor]] = {}\n",
    "num_dst_nodes_dict: Dict[str, int] = {}\n",
    "\n",
    "# Assuming `task` is already defined and provides the dataset information\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    table = task.get_table(split)\n",
    "    table_input = get_link_train_table_input(table, task)\n",
    "    dst_nodes_dict[split] = table_input.dst_nodes\n",
    "    num_dst_nodes_dict[split] = table_input.num_dst_nodes\n",
    "    loader_dict[split] = NeighborLoader(\n",
    "        data,\n",
    "        num_neighbors=num_neighbors,\n",
    "        time_attr=\"time\",\n",
    "        input_nodes=table_input.src_nodes,\n",
    "        input_time=table_input.src_time,\n",
    "        subgraph_type=\"bidirectional\",\n",
    "        batch_size=batch_size,\n",
    "        temporal_strategy=temporal_strategy,\n",
    "        shuffle=split == \"train\",\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=num_workers > 0,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"idgnn\":\n",
    "    model = IDGNN(\n",
    "        data=data,\n",
    "        col_stats_dict=col_stats_dict,\n",
    "        num_layers=num_layers,\n",
    "        channels=channels,\n",
    "        out_channels=1,\n",
    "        aggr=aggregation_method,\n",
    "        norm=\"layer_norm\",\n",
    "        torch_frame_model_kwargs={\n",
    "            \"channels\": 128,\n",
    "            \"num_layers\": 4,\n",
    "        },\n",
    "    ).to(device)\n",
    "elif model_name == \"contextgnn\":\n",
    "    model = ContextGNN(\n",
    "        data=data,\n",
    "        col_stats_dict=col_stats_dict,\n",
    "        rhs_emb_mode=RHSEmbeddingMode.FUSION,\n",
    "        dst_entity_table=task.dst_entity_table,\n",
    "        num_nodes=num_dst_nodes_dict[\"train\"],\n",
    "        num_layers=num_layers,\n",
    "        channels=channels,\n",
    "        aggr=\"sum\",\n",
    "        norm=\"layer_norm\",\n",
    "        embedding_dim=128,\n",
    "        torch_frame_model_kwargs={\n",
    "            \"channels\": 128,\n",
    "            \"num_layers\": 4,\n",
    "        },\n",
    "    ).to(device)\n",
    "elif model_name == 'shallowrhsgnn':\n",
    "    model = ShallowRHSGNN(\n",
    "        data=data,\n",
    "        col_stats_dict=col_stats_dict,\n",
    "        rhs_emb_mode=RHSEmbeddingMode.FUSION,\n",
    "        dst_entity_table=task.dst_entity_table,\n",
    "        num_nodes=num_dst_nodes_dict[\"train\"],\n",
    "        num_layers=num_layers,\n",
    "        channels=channels,\n",
    "        aggr=\"sum\",\n",
    "        norm=\"layer_norm\",\n",
    "        embedding_dim=64,\n",
    "        torch_frame_model_kwargs={\n",
    "            \"channels\": 128,\n",
    "            \"num_layers\": 4,\n",
    "        },\n",
    "    ).to(device)\n",
    "\n",
    "elif model_name == \"contexttransgnn\":\n",
    "    model = ContextTransGNN(\n",
    "        data=data,\n",
    "        col_stats_dict=col_stats_dict,\n",
    "        rhs_emb_mode=RHSEmbeddingMode.FUSION,\n",
    "        dst_entity_table=task.dst_entity_table,\n",
    "        num_nodes=num_dst_nodes_dict[\"train\"],\n",
    "        num_layers=num_layers,\n",
    "        channels=channels,\n",
    "        embedding_dim=64,\n",
    "        transformer_heads=4,\n",
    "        aggr=\"sum\",\n",
    "        norm=\"layer_norm\",\n",
    "    ).to(device)\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported model type {model_name}.\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 155/155 [02:30<00:00,  1.03it/s]\n",
      "Val: 100%|██████████| 1/1 [00:00<00:00, 15.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train loss: 8.351733575017283, Val metrics: {'link_prediction_precision': np.float64(0.07758620689655173), 'link_prediction_recall': np.float64(0.25862068965517243), 'link_prediction_map': np.float64(0.1810344827586207), 'link_prediction_top': np.float64(0.3103448275862069)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 155/155 [02:27<00:00,  1.05it/s]\n",
      "Val: 100%|██████████| 1/1 [00:00<00:00, 17.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02, Train loss: 5.607086999546708, Val metrics: {'link_prediction_precision': np.float64(0.05172413793103448), 'link_prediction_recall': np.float64(0.14942528735632182), 'link_prediction_map': np.float64(0.1221264367816092), 'link_prediction_top': np.float64(0.20689655172413793)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 155/155 [02:30<00:00,  1.03it/s]\n",
      "Val: 100%|██████████| 1/1 [00:00<00:00, 18.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03, Train loss: 5.481380947872887, Val metrics: {'link_prediction_precision': np.float64(0.06896551724137931), 'link_prediction_recall': np.float64(0.17816091954022986), 'link_prediction_map': np.float64(0.09482758620689655), 'link_prediction_top': np.float64(0.27586206896551724)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 155/155 [02:35<00:00,  1.01s/it]\n",
      "Val: 100%|██████████| 1/1 [00:00<00:00, 19.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04, Train loss: 5.388827334616773, Val metrics: {'link_prediction_precision': np.float64(0.02586206896551724), 'link_prediction_recall': np.float64(0.08045977011494251), 'link_prediction_map': np.float64(0.023946360153256706), 'link_prediction_top': np.float64(0.10344827586206896)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 155/155 [02:30<00:00,  1.03it/s]\n",
      "Val: 100%|██████████| 1/1 [00:00<00:00, 19.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Train loss: 5.292511561220247, Val metrics: {'link_prediction_precision': np.float64(0.0), 'link_prediction_recall': np.float64(0.0), 'link_prediction_map': np.float64(0.0), 'link_prediction_top': np.float64(0.0)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 155/155 [02:30<00:00,  1.03it/s]\n",
      "Val: 100%|██████████| 1/1 [00:00<00:00, 19.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06, Train loss: 5.234641221247929, Val metrics: {'link_prediction_precision': np.float64(0.0), 'link_prediction_recall': np.float64(0.0), 'link_prediction_map': np.float64(0.0), 'link_prediction_top': np.float64(0.0)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 155/155 [02:28<00:00,  1.04it/s]\n",
      "Val: 100%|██████████| 1/1 [00:00<00:00, 18.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07, Train loss: 5.1853479167041865, Val metrics: {'link_prediction_precision': np.float64(0.017241379310344827), 'link_prediction_recall': np.float64(0.04597701149425287), 'link_prediction_map': np.float64(0.020114942528735632), 'link_prediction_top': np.float64(0.06896551724137931)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 155/155 [02:32<00:00,  1.01it/s]\n",
      "Val: 100%|██████████| 1/1 [00:00<00:00, 18.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08, Train loss: 5.142359530692381, Val metrics: {'link_prediction_precision': np.float64(0.017241379310344827), 'link_prediction_recall': np.float64(0.04597701149425287), 'link_prediction_map': np.float64(0.0210727969348659), 'link_prediction_top': np.float64(0.06896551724137931)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 155/155 [02:32<00:00,  1.02it/s]\n",
      "Val: 100%|██████████| 1/1 [00:00<00:00, 20.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09, Train loss: 5.13735747882375, Val metrics: {'link_prediction_precision': np.float64(0.017241379310344827), 'link_prediction_recall': np.float64(0.022988505747126436), 'link_prediction_map': np.float64(0.006704980842911877), 'link_prediction_top': np.float64(0.06896551724137931)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 155/155 [02:29<00:00,  1.04it/s]\n",
      "Val: 100%|██████████| 1/1 [00:00<00:00, 19.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train loss: 5.071819476780299, Val metrics: {'link_prediction_precision': np.float64(0.008620689655172414), 'link_prediction_recall': np.float64(0.011494252873563218), 'link_prediction_map': np.float64(0.0038314176245210726), 'link_prediction_top': np.float64(0.034482758620689655)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best val: 100%|██████████| 1/1 [00:00<00:00, 22.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val metrics: {'link_prediction_precision': np.float64(0.07758620689655173), 'link_prediction_recall': np.float64(0.25862068965517243), 'link_prediction_map': np.float64(0.1810344827586207), 'link_prediction_top': np.float64(0.3103448275862069)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 1/1 [00:00<00:00, 24.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best test metrics: {'link_prediction_precision': np.float64(0.05303030303030303), 'link_prediction_recall': np.float64(0.19696969696969696), 'link_prediction_map': np.float64(0.11742424242424243), 'link_prediction_top': np.float64(0.21212121212121213)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train() -> float:\n",
    "    model.train()\n",
    "\n",
    "    loss_accum = count_accum = 0\n",
    "    steps = 0\n",
    "    total_steps = min(len(loader_dict[\"train\"]), max_steps_per_epoch)\n",
    "    sparse_tensor = SparseTensor(dst_nodes_dict[\"train\"][1], device=device)\n",
    "    \n",
    "    for batch in tqdm(loader_dict[\"train\"], total=total_steps, desc=\"Train\"):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Get ground-truth\n",
    "        input_id = batch[task.src_entity_table].input_id\n",
    "        src_batch, dst_index = sparse_tensor[input_id]\n",
    "\n",
    "        # Optimization\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(batch, task.src_entity_table, task.dst_entity_table)\n",
    "        edge_label_index = torch.stack([src_batch, dst_index], dim=0)\n",
    "        loss = sparse_cross_entropy(logits, edge_label_index)\n",
    "        numel = len(batch[task.dst_entity_table].batch)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_accum += float(loss) * numel\n",
    "        count_accum += numel\n",
    "\n",
    "        steps += 1\n",
    "        if steps > max_steps_per_epoch:\n",
    "            break\n",
    "\n",
    "    if count_accum == 0:\n",
    "        warnings.warn(f\"Did not sample a single '{task.dst_entity_table}' node in any mini-batch.\")\n",
    "\n",
    "    return loss_accum / count_accum if count_accum > 0 else float(\"nan\")\n",
    "\n",
    "# Test function\n",
    "@torch.no_grad()\n",
    "def test(loader: NeighborLoader, desc: str) -> np.ndarray:\n",
    "    model.eval()\n",
    "\n",
    "    pred_list: List[Tensor] = []\n",
    "    for batch in tqdm(loader, desc=desc):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        out = model(batch, task.src_entity_table, task.dst_entity_table).detach()\n",
    "        scores = torch.sigmoid(out)\n",
    "\n",
    "        _, pred_mini = torch.topk(scores, k=task.eval_k, dim=1)\n",
    "        pred_list.append(pred_mini)\n",
    "    \n",
    "    pred = torch.cat(pred_list, dim=0).cpu().numpy()\n",
    "    return pred\n",
    "\n",
    "# Training and evaluation loop\n",
    "state_dict = None\n",
    "best_val_metric = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train()\n",
    "    \n",
    "    if epoch % eval_epochs_interval == 0:\n",
    "        val_pred = test(loader_dict[\"val\"], desc=\"Val\")\n",
    "        val_metrics = task.evaluate(val_pred, task.get_table(\"val\"))\n",
    "        print(f\"Epoch: {epoch:02d}, Train loss: {train_loss}, Val metrics: {val_metrics}\")\n",
    "\n",
    "        if val_metrics[tune_metric] > best_val_metric:\n",
    "            best_val_metric = val_metrics[tune_metric]\n",
    "            state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "# Load the best model and evaluate on validation and test sets\n",
    "assert state_dict is not None\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "val_pred = test(loader_dict[\"val\"], desc=\"Best val\")\n",
    "val_metrics = task.evaluate(val_pred, task.get_table(\"val\"))\n",
    "print(f\"Best val metrics: {val_metrics}\")\n",
    "\n",
    "test_pred = test(loader_dict[\"test\"], desc=\"Test\")\n",
    "test_metrics = task.evaluate(test_pred)\n",
    "print(f\"Best test metrics: {test_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train() -> float:\n",
    "    model.train()\n",
    "\n",
    "    loss_accum = count_accum = 0\n",
    "    steps = 0\n",
    "    total_steps = min(len(loader_dict[\"train\"]), max_steps_per_epoch)\n",
    "    sparse_tensor = SparseTensor(dst_nodes_dict[\"train\"][1], device=device)\n",
    "    \n",
    "    for batch in tqdm(loader_dict[\"train\"], total=total_steps, desc=\"Train\"):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Get ground-truth\n",
    "        input_id = batch[task.src_entity_table].input_id\n",
    "        src_batch, dst_index = sparse_tensor[input_id]\n",
    "\n",
    "        # Optimization\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if model_name == 'idgnn':\n",
    "            out = model(batch, task.src_entity_table, task.dst_entity_table).flatten()\n",
    "            batch_size = batch[task.src_entity_table].batch_size\n",
    "\n",
    "            # Get target label\n",
    "            target = torch.isin(\n",
    "                batch[task.dst_entity_table].batch +\n",
    "                batch_size * batch[task.dst_entity_table].n_id,\n",
    "                src_batch + batch_size * dst_index,\n",
    "            ).float()\n",
    "\n",
    "            loss = F.binary_cross_entropy_with_logits(out, target)\n",
    "            numel = out.numel()\n",
    "        elif model_name in ['contextgnn', 'shallowrhsgnn']:\n",
    "            logits = model(batch, task.src_entity_table, task.dst_entity_table)\n",
    "            edge_label_index = torch.stack([src_batch, dst_index], dim=0)\n",
    "            loss = sparse_cross_entropy(logits, edge_label_index)\n",
    "            numel = len(batch[task.dst_entity_table].batch)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_accum += float(loss) * numel\n",
    "        count_accum += numel\n",
    "\n",
    "        steps += 1\n",
    "        if steps > max_steps_per_epoch:\n",
    "            break\n",
    "\n",
    "    if count_accum == 0:\n",
    "        warnings.warn(f\"Did not sample a single '{task.dst_entity_table}' node in any mini-batch.\")\n",
    "\n",
    "    return loss_accum / count_accum if count_accum > 0 else float(\"nan\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "@torch.no_grad()\n",
    "def test(loader: NeighborLoader, desc: str) -> np.ndarray:\n",
    "    model.eval()\n",
    "\n",
    "    pred_list: List[Tensor] = []\n",
    "    for batch in tqdm(loader, desc=desc):\n",
    "        batch = batch.to(device)\n",
    "        batch_size = batch[task.src_entity_table].batch_size\n",
    "\n",
    "        if model_name == \"idgnn\":\n",
    "            out = (model.forward(batch, task.src_entity_table, task.dst_entity_table).detach().flatten())\n",
    "            scores = torch.zeros(batch_size, task.num_dst_nodes, device=out.device)\n",
    "            scores[batch[task.dst_entity_table].batch, batch[task.dst_entity_table].n_id] = torch.sigmoid(out)\n",
    "        elif model_name in ['contextgnn', 'shallowrhsgnn']:\n",
    "            out = model(batch, task.src_entity_table, task.dst_entity_table).detach()\n",
    "            scores = torch.sigmoid(out)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {model_name}.\")\n",
    "\n",
    "        _, pred_mini = torch.topk(scores, k=task.eval_k, dim=1)\n",
    "        pred_list.append(pred_mini)\n",
    "    \n",
    "    pred = torch.cat(pred_list, dim=0).cpu().numpy()\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 99/99 [01:18<00:00,  1.26it/s]\n",
      "Val: 100%|██████████| 1/1 [00:00<00:00, 30.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train loss: 0.3077996562725104, Val metrics: {'link_prediction_precision': np.float64(0.08333333333333333), 'link_prediction_recall': np.float64(0.2619047619047619), 'link_prediction_map': np.float64(0.21428571428571427), 'link_prediction_top': np.float64(0.3333333333333333)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 99/99 [01:18<00:00,  1.26it/s]\n",
      "Val: 100%|██████████| 1/1 [00:00<00:00, 32.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02, Train loss: nan, Val metrics: {'link_prediction_precision': np.float64(0.08333333333333333), 'link_prediction_recall': np.float64(0.2619047619047619), 'link_prediction_map': np.float64(0.20833333333333334), 'link_prediction_top': np.float64(0.3333333333333333)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 99/99 [01:23<00:00,  1.19it/s]\n",
      "Val: 100%|██████████| 1/1 [00:00<00:00, 23.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03, Train loss: 0.28528702723000937, Val metrics: {'link_prediction_precision': np.float64(0.09523809523809523), 'link_prediction_recall': np.float64(0.2857142857142857), 'link_prediction_map': np.float64(0.21626984126984125), 'link_prediction_top': np.float64(0.38095238095238093)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 99/99 [01:18<00:00,  1.26it/s]\n",
      "Val: 100%|██████████| 1/1 [00:00<00:00, 32.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04, Train loss: nan, Val metrics: {'link_prediction_precision': np.float64(0.08333333333333333), 'link_prediction_recall': np.float64(0.2619047619047619), 'link_prediction_map': np.float64(0.2103174603174603), 'link_prediction_top': np.float64(0.3333333333333333)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 99/99 [01:17<00:00,  1.28it/s]\n",
      "Val: 100%|██████████| 1/1 [00:00<00:00, 31.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Train loss: 0.2793998677638418, Val metrics: {'link_prediction_precision': np.float64(0.08333333333333333), 'link_prediction_recall': np.float64(0.2619047619047619), 'link_prediction_map': np.float64(0.20833333333333334), 'link_prediction_top': np.float64(0.3333333333333333)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 99/99 [01:25<00:00,  1.16it/s]\n",
      "Val: 100%|██████████| 1/1 [00:00<00:00, 32.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06, Train loss: 0.2812479808445714, Val metrics: {'link_prediction_precision': np.float64(0.08333333333333333), 'link_prediction_recall': np.float64(0.2619047619047619), 'link_prediction_map': np.float64(0.2103174603174603), 'link_prediction_top': np.float64(0.3333333333333333)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 99/99 [01:19<00:00,  1.25it/s]\n",
      "Val: 100%|██████████| 1/1 [00:00<00:00, 32.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07, Train loss: 0.2762963330451993, Val metrics: {'link_prediction_precision': np.float64(0.08333333333333333), 'link_prediction_recall': np.float64(0.2619047619047619), 'link_prediction_map': np.float64(0.20833333333333334), 'link_prediction_top': np.float64(0.3333333333333333)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 99/99 [01:15<00:00,  1.30it/s]\n",
      "Val: 100%|██████████| 1/1 [00:00<00:00, 31.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08, Train loss: nan, Val metrics: {'link_prediction_precision': np.float64(0.08333333333333333), 'link_prediction_recall': np.float64(0.2619047619047619), 'link_prediction_map': np.float64(0.2103174603174603), 'link_prediction_top': np.float64(0.3333333333333333)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 99/99 [01:19<00:00,  1.25it/s]\n",
      "Val: 100%|██████████| 1/1 [00:00<00:00, 27.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09, Train loss: nan, Val metrics: {'link_prediction_precision': np.float64(0.07142857142857142), 'link_prediction_recall': np.float64(0.23809523809523808), 'link_prediction_map': np.float64(0.20634920634920634), 'link_prediction_top': np.float64(0.2857142857142857)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 99/99 [01:19<00:00,  1.25it/s]\n",
      "Val: 100%|██████████| 1/1 [00:00<00:00, 32.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train loss: 0.26645567435515677, Val metrics: {'link_prediction_precision': np.float64(0.10714285714285714), 'link_prediction_recall': np.float64(0.3333333333333333), 'link_prediction_map': np.float64(0.25793650793650796), 'link_prediction_top': np.float64(0.42857142857142855)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best val: 100%|██████████| 1/1 [00:00<00:00, 31.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val metrics: {'link_prediction_precision': np.float64(0.10714285714285714), 'link_prediction_recall': np.float64(0.3333333333333333), 'link_prediction_map': np.float64(0.25793650793650796), 'link_prediction_top': np.float64(0.42857142857142855)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 1/1 [00:00<00:00, 31.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best test metrics: {'link_prediction_precision': np.float64(0.03571428571428571), 'link_prediction_recall': np.float64(0.14285714285714285), 'link_prediction_map': np.float64(0.10714285714285714), 'link_prediction_top': np.float64(0.14285714285714285)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training and evaluation loop\n",
    "state_dict = None\n",
    "best_val_metric = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train()\n",
    "    \n",
    "    if epoch % eval_epochs_interval == 0:\n",
    "        val_pred = test(loader_dict[\"val\"], desc=\"Val\")\n",
    "        val_metrics = task.evaluate(val_pred, task.get_table(\"val\"))\n",
    "        print(f\"Epoch: {epoch:02d}, Train loss: {train_loss}, Val metrics: {val_metrics}\")\n",
    "\n",
    "        if val_metrics[tune_metric] > best_val_metric:\n",
    "            best_val_metric = val_metrics[tune_metric]\n",
    "            state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "# Load the best model and evaluate on validation and test sets\n",
    "assert state_dict is not None\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "val_pred = test(loader_dict[\"val\"], desc=\"Best val\")\n",
    "val_metrics = task.evaluate(val_pred, task.get_table(\"val\"))\n",
    "print(f\"Best val metrics: {val_metrics}\")\n",
    "\n",
    "test_pred = test(loader_dict[\"test\"], desc=\"Test\")\n",
    "test_metrics = task.evaluate(test_pred)\n",
    "print(f\"Best test metrics: {test_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = loader_dict[\"test\"].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article': TensorFrame(\n",
       "   num_cols=28,\n",
       "   num_rows=2168,\n",
       "   numerical (1): ['Base Unit Price'],\n",
       "   categorical (7): ['Collection Code', 'Idea Code', 'Initial Pattern Code', 'Season', 'Style Code', 'Usage Type', 'Year'],\n",
       "   embedding (20): ['Article Description', 'Brand', 'Collection Name', 'Color', 'Combined Feature', 'Dimensions', 'Idea Description', 'Initial Pattern Description', 'Material Category', 'Material Type', 'Model', 'Processing', 'Product Category', 'Product Group', 'Product Subgroup', 'Product Type', 'Size', 'Style Description', 'Usage Space', 'User'],\n",
       "   has_target=False,\n",
       "   device='cpu',\n",
       " ),\n",
       " 'branches': TensorFrame(\n",
       "   num_cols=2,\n",
       "   num_rows=75,\n",
       "   embedding (2): ['Branch Name', 'City'],\n",
       "   has_target=False,\n",
       "   device='cpu',\n",
       " ),\n",
       " 'customer': TensorFrame(\n",
       "   num_cols=3,\n",
       "   num_rows=43453,\n",
       "   categorical (3): ['City', 'Customer Category', 'Customer Process'],\n",
       "   has_target=False,\n",
       "   device='cpu',\n",
       " ),\n",
       " 'transactions': TensorFrame(\n",
       "   num_cols=26,\n",
       "   num_rows=316487,\n",
       "   numerical (14): ['Allocated Credit', 'Base Total Price', 'Cash Card', 'Day', 'Discount Percentage', 'Gift Voucher', 'Promotion Discount', 'Quantity', 'Sales Voucher', 'Small Change Discount', 'Special Discount', 'Style Code', 'Total Discounts', 'Transaction Serial'],\n",
       "   timestamp (2): ['Time', 'datetime'],\n",
       "   categorical (9): ['Day_of_week', 'Holiday', 'Month_x', 'Season', 'Weekday', 'Weekend', 'Year_x', 'even_odd', 'event_type_1'],\n",
       "   embedding (1): ['event_name_1'],\n",
       "   has_target=False,\n",
       "   device='cpu',\n",
       " )}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.tf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('transactions', 'f2p_articles_id', 'article'): tensor([[    16,     24,     31,  ..., 311381, 311451, 312867],\n",
      "        [   580,      8,    561,  ...,    587,   2133,    556]]), ('article', 'rev_f2p_articles_id', 'transactions'): tensor([[     1,      8,      8,  ...,   2162,   2162,   2162],\n",
      "        [277007,     24,     91,  ..., 279713, 279714, 279715]]), ('transactions', 'f2p_customers_id', 'customer'): tensor([[     0,      1,      2,  ..., 316484, 316485, 316486],\n",
      "        [     0,      1,      2,  ...,  23449,  39691,  35513]]), ('customer', 'rev_f2p_customers_id', 'transactions'): tensor([[     0,      0,      0,  ...,  43452,  43452,  43452],\n",
      "        [     0,    179,    603,  ..., 315994, 316101, 316108]]), ('transactions', 'f2p_BranchCode', 'branches'): tensor([[     0,      1,      2,  ..., 316484, 316485, 316486],\n",
      "        [     0,      1,      2,  ...,     54,     54,     65]]), ('branches', 'rev_f2p_BranchCode', 'transactions'): tensor([[     0,      0,      0,  ...,     74,     74,     74],\n",
      "        [     0,    159,    209,  ..., 316344, 316345, 316346]])}\n"
     ]
    }
   ],
   "source": [
    "if model_name == \"idgnn\":\n",
    "    model = IDGNN(\n",
    "        data=data,\n",
    "        col_stats_dict=col_stats_dict,\n",
    "        num_layers=num_layers,\n",
    "        channels=channels,\n",
    "        out_channels=1,\n",
    "        aggr=aggregation_method,\n",
    "        norm=\"layer_norm\",\n",
    "        torch_frame_model_kwargs={\n",
    "            \"channels\": 128,\n",
    "            \"num_layers\": 4,\n",
    "        },\n",
    "    ).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure batch is correctly extracted\n",
    "batch = loader_dict[\"test\"].data  # Extract HeteroData batch\n",
    "\n",
    "# Extract the correct edge index\n",
    "edge_index = batch.edge_index_dict[(\"transactions\", \"f2p_customers_id\", \"customer\")]\n",
    "\n",
    "# Ensure correct input format\n",
    "if edge_index is None:\n",
    "    raise ValueError(\"Edge index not found for the given edge type.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "IDGNN.forward() got multiple values for argument 'entity_table'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[121], line 19\u001b[0m\n\u001b[0;32m      5\u001b[0m explainer \u001b[38;5;241m=\u001b[39m Explainer(\n\u001b[0;32m      6\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      7\u001b[0m     algorithm\u001b[38;5;241m=\u001b[39mCaptumExplainer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIntegratedGradients\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     edge_mask_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Running the explanation process\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m explanation \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass the entire batch (HeteroData object)\u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentity_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransactions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Correct entity node type\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdst_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcustomer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Correct destination node type\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass edge relationships\u001b[39;49;00m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Select a sample edge\u001b[39;49;00m\n\u001b[0;32m     25\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(explanation)\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch_geometric\\explain\\explainer.py:196\u001b[0m, in \u001b[0;36mExplainer.__call__\u001b[1;34m(self, x, edge_index, target, index, **kwargs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    193\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    194\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m should not be provided for the explanation \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplanation_type\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 196\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_target(prediction)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index, \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch_geometric\\explain\\explainer.py:115\u001b[0m, in \u001b[0;36mExplainer.get_prediction\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 115\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain(training)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: IDGNN.forward() got multiple values for argument 'entity_table'"
     ]
    }
   ],
   "source": [
    "from torch_geometric.explain import Explainer, CaptumExplainer\n",
    "\n",
    "\n",
    "# Define the Explainer\n",
    "explainer = Explainer(\n",
    "    model=model,\n",
    "    algorithm=CaptumExplainer('IntegratedGradients'),\n",
    "    explanation_type='model',\n",
    "    model_config=dict(\n",
    "        mode='regression',  # or 'classification'\n",
    "        task_level='edge',\n",
    "        return_type='raw',\n",
    "    ),\n",
    "    node_mask_type='attributes',  # Ensure attributes are explainable\n",
    "    edge_mask_type='object',\n",
    ")\n",
    "\n",
    "# Running the explanation process\n",
    "explanation = explainer(\n",
    "    x=batch,  # Pass the entire batch (HeteroData object)\n",
    "    entity_table=\"transactions\",  # Correct entity node type\n",
    "    dst_table=\"customer\",  # Correct destination node type\n",
    "    edge_index=edge_index,  # Pass edge relationships\n",
    "    index=0  # Select a sample edge\n",
    ").cpu().detach()\n",
    "\n",
    "print(explanation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1130/1130 [56:08<00:00,  2.98s/it] \n",
      "Val: 100%|██████████| 5/5 [00:07<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train loss: 15.693553883077128, Val metrics: {'link_prediction_precision': np.float64(0.14345637583892618), 'link_prediction_recall': np.float64(0.21569590923617768), 'link_prediction_map': np.float64(0.15491004847129008), 'link_prediction_top': np.float64(0.47315436241610737)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1130/1130 [57:53<00:00,  3.07s/it] \n",
      "Val: 100%|██████████| 5/5 [00:07<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02, Train loss: 14.202766350519356, Val metrics: {'link_prediction_precision': np.float64(0.10339765100671142), 'link_prediction_recall': np.float64(0.15851809683604984), 'link_prediction_map': np.float64(0.10915944258016405), 'link_prediction_top': np.float64(0.36325503355704697)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1130/1130 [57:31<00:00,  3.05s/it] \n",
      "Val: 100%|██████████| 5/5 [00:07<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03, Train loss: 14.02156443823075, Val metrics: {'link_prediction_precision': np.float64(0.06040268456375839), 'link_prediction_recall': np.float64(0.09708872643016939), 'link_prediction_map': np.float64(0.06484200223713646), 'link_prediction_top': np.float64(0.21140939597315436)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1130/1130 [56:41<00:00,  3.01s/it] \n",
      "Val: 100%|██████████| 5/5 [00:07<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04, Train loss: 13.868679549075418, Val metrics: {'link_prediction_precision': np.float64(0.061870805369127514), 'link_prediction_recall': np.float64(0.09752816395014381), 'link_prediction_map': np.float64(0.06422445935868755), 'link_prediction_top': np.float64(0.21728187919463088)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1130/1130 [55:50<00:00,  2.97s/it] \n",
      "Val: 100%|██████████| 5/5 [00:07<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Train loss: 13.76965204429071, Val metrics: {'link_prediction_precision': np.float64(0.05641778523489933), 'link_prediction_recall': np.float64(0.08841083413231063), 'link_prediction_map': np.float64(0.06416620059656972), 'link_prediction_top': np.float64(0.19546979865771813)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1130/1130 [55:39<00:00,  2.96s/it] \n",
      "Val: 100%|██████████| 5/5 [00:07<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06, Train loss: 13.730094240045654, Val metrics: {'link_prediction_precision': np.float64(0.05893456375838926), 'link_prediction_recall': np.float64(0.0952141259188239), 'link_prediction_map': np.float64(0.0637117822520507), 'link_prediction_top': np.float64(0.20553691275167785)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1130/1130 [55:46<00:00,  2.96s/it] \n",
      "Val: 100%|██████████| 5/5 [00:07<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07, Train loss: 13.672291335220528, Val metrics: {'link_prediction_precision': np.float64(0.05725671140939597), 'link_prediction_recall': np.float64(0.09168264621284755), 'link_prediction_map': np.float64(0.061964019388516034), 'link_prediction_top': np.float64(0.1988255033557047)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1130/1130 [56:02<00:00,  2.98s/it] \n",
      "Val: 100%|██████████| 5/5 [00:07<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08, Train loss: 13.573945471917492, Val metrics: {'link_prediction_precision': np.float64(0.057885906040268456), 'link_prediction_recall': np.float64(0.0920881271971876), 'link_prediction_map': np.float64(0.06313502050708426), 'link_prediction_top': np.float64(0.20134228187919462)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1130/1130 [55:56<00:00,  2.97s/it] \n",
      "Val: 100%|██████████| 5/5 [00:07<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09, Train loss: 13.533757880756776, Val metrics: {'link_prediction_precision': np.float64(0.05683724832214765), 'link_prediction_recall': np.float64(0.09063398849472674), 'link_prediction_map': np.float64(0.06106683445190156), 'link_prediction_top': np.float64(0.19714765100671142)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1130/1130 [56:42<00:00,  3.01s/it] \n",
      "Val: 100%|██████████| 5/5 [00:07<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train loss: 13.484285618741263, Val metrics: {'link_prediction_precision': np.float64(0.057466442953020135), 'link_prediction_recall': np.float64(0.09096356663470757), 'link_prediction_map': np.float64(0.06195819351230424), 'link_prediction_top': np.float64(0.1988255033557047)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best val: 100%|██████████| 5/5 [00:07<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val metrics: {'link_prediction_precision': np.float64(0.14345637583892618), 'link_prediction_recall': np.float64(0.21569590923617768), 'link_prediction_map': np.float64(0.15491004847129008), 'link_prediction_top': np.float64(0.47315436241610737)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 10/10 [00:10<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best test metrics: {'link_prediction_precision': np.float64(0.13122721749696234), 'link_prediction_recall': np.float64(0.18079086657458468), 'link_prediction_map': np.float64(0.14091681292471084), 'link_prediction_top': np.float64(0.41271769947347103)}\n"
     ]
    }
   ],
   "source": [
    "# Training and evaluation loop\n",
    "state_dict = None\n",
    "best_val_metric = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train()\n",
    "    \n",
    "    if epoch % eval_epochs_interval == 0:\n",
    "        val_pred = test(loader_dict[\"val\"], desc=\"Val\")\n",
    "        val_metrics = task.evaluate(val_pred, task.get_table(\"val\"))\n",
    "        print(f\"Epoch: {epoch:02d}, Train loss: {train_loss}, Val metrics: {val_metrics}\")\n",
    "\n",
    "        if val_metrics[tune_metric] > best_val_metric:\n",
    "            best_val_metric = val_metrics[tune_metric]\n",
    "            state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "# Load the best model and evaluate on validation and test sets\n",
    "assert state_dict is not None\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "val_pred = test(loader_dict[\"val\"], desc=\"Best val\")\n",
    "val_metrics = task.evaluate(val_pred, task.get_table(\"val\"))\n",
    "print(f\"Best val metrics: {val_metrics}\")\n",
    "\n",
    "test_pred = test(loader_dict[\"test\"], desc=\"Test\")\n",
    "test_metrics = task.evaluate(test_pred)\n",
    "print(f\"Best test metrics: {test_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"best_fullDorsa_model1.pth\"\n",
    "torch.save(state_dict, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_path = \"best_Dorsa_model.pth\"\n",
    "state_dict = torch.load(model_save_path)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 10/10 [00:06<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best test metrics: {'link_prediction_precision': np.float64(0.12829080599432968), 'link_prediction_recall': np.float64(0.17587147787633814), 'link_prediction_map': np.float64(0.1295818144997975), 'link_prediction_top': np.float64(0.40785743215876874)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_pred = test(loader_dict[\"test\"], desc=\"Test\")\n",
    "test_metrics = task.evaluate(test_pred)\n",
    "print(f\"Best test metrics: {test_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  72,  145,  105,  249],\n",
       "       [  72,  145,  105, 1503],\n",
       "       [  72,  145,  105, 1503],\n",
       "       ...,\n",
       "       [  72,  145,  105, 1073],\n",
       "       [  72,  145,  105, 1503],\n",
       "       [  72,  105,  145,  323]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_ids = test_table.df[\"customers_id\"].values  # Extract the customer_id column\n",
    "predicted_labels = test_pred  # This contains the model's predicted labels\n",
    "\n",
    "# Now zip customer_ids with the predictions\n",
    "results = list(zip(customer_ids, predicted_labels))\n",
    "\n",
    "# If you'd like to show or process the results, you can format them as a dataframe\n",
    "results_df = pd.DataFrame(results, columns=[\"customers_id\", \"predicted_articles\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      customers_id    predicted_articles\n",
      "0             9806   [72, 145, 105, 249]\n",
      "1           126374  [72, 145, 105, 1503]\n",
      "2            12345  [72, 145, 105, 1503]\n",
      "3           126357  [72, 145, 105, 1503]\n",
      "4           132536  [72, 145, 105, 1073]\n",
      "...            ...                   ...\n",
      "2464        102668   [105, 145, 72, 140]\n",
      "2465         98453  [72, 145, 105, 1678]\n",
      "2466         96365  [72, 145, 105, 1073]\n",
      "2467        140302  [72, 145, 105, 1503]\n",
      "2468          1489   [72, 105, 145, 323]\n",
      "\n",
      "[2469 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article': articles_id\n",
      "54821.0       0\n",
      "53612.0       1\n",
      "59459.0       2\n",
      "54126.0       3\n",
      "44301.0       4\n",
      "           ... \n",
      "17315.0    7435\n",
      "23028.0    7436\n",
      "46775.0    7437\n",
      "39866.0    7438\n",
      "42931.0    7439\n",
      "Name: index, Length: 7440, dtype: Int64, 'customer': customers_id\n",
      "10121860.0         0\n",
      "10447769.0         1\n",
      "10469286.0         2\n",
      "10075749.0         3\n",
      "10432889.0         4\n",
      "               ...  \n",
      "10461949.0    146774\n",
      "10477874.0    146775\n",
      "10411981.0    146776\n",
      "10440367.0    146777\n",
      "10455252.0    146778\n",
      "Name: index, Length: 146779, dtype: Int64, 'branches': BranchCode\n",
      "10284.0     0\n",
      "10124.0     1\n",
      "10706.0     2\n",
      "11008.0     3\n",
      "10002.0     4\n",
      "10100.0     5\n",
      "10131.0     6\n",
      "11086.0     7\n",
      "10562.0     8\n",
      "10593.0     9\n",
      "10168.0    10\n",
      "10267.0    11\n",
      "10887.0    12\n",
      "10008.0    13\n",
      "10006.0    14\n",
      "10003.0    15\n",
      "10099.0    16\n",
      "10656.0    17\n",
      "10672.0    18\n",
      "10536.0    19\n",
      "10594.0    20\n",
      "10113.0    21\n",
      "10595.0    22\n",
      "10732.0    23\n",
      "11155.0    24\n",
      "10642.0    25\n",
      "10803.0    26\n",
      "10001.0    27\n",
      "10643.0    28\n",
      "10709.0    29\n",
      "10748.0    30\n",
      "10010.0    31\n",
      "10633.0    32\n",
      "Name: index, dtype: Int64}\n"
     ]
    }
   ],
   "source": [
    "index_map_dict1 = db.reindex_pkeys_and_fkeys()\n",
    "print(index_map_dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64\n"
     ]
    }
   ],
   "source": [
    "# Access the 'article' Series from index_map_dict1\n",
    "article_series = index_map_dict1['article']\n",
    "\n",
    "# Get the data type of the 'article' Series\n",
    "article_dtype = article_series.dtype\n",
    "\n",
    "# Print the data type\n",
    "print(article_dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{np.int64(0): 54821.0,\n",
       " np.int64(1): 53612.0,\n",
       " np.int64(2): 59459.0,\n",
       " np.int64(3): 54126.0,\n",
       " np.int64(4): 44301.0,\n",
       " np.int64(5): 28378.0,\n",
       " np.int64(6): 44023.0,\n",
       " np.int64(7): 47122.0,\n",
       " np.int64(8): 54025.0,\n",
       " np.int64(9): 41372.0,\n",
       " np.int64(10): 42282.0,\n",
       " np.int64(11): 57538.0,\n",
       " np.int64(12): 44592.0,\n",
       " np.int64(13): 59476.0,\n",
       " np.int64(14): 34528.0,\n",
       " np.int64(15): 53435.0,\n",
       " np.int64(16): 54161.0,\n",
       " np.int64(17): 35142.0,\n",
       " np.int64(18): 47567.0,\n",
       " np.int64(19): 49489.0,\n",
       " np.int64(20): 31794.0,\n",
       " np.int64(21): 18213.0,\n",
       " np.int64(22): 49482.0,\n",
       " np.int64(23): 41415.0,\n",
       " np.int64(24): 24837.0,\n",
       " np.int64(25): 50084.0,\n",
       " np.int64(26): 47535.0,\n",
       " np.int64(27): 53614.0,\n",
       " np.int64(28): 44382.0,\n",
       " np.int64(29): 22012.0,\n",
       " np.int64(30): 24294.0,\n",
       " np.int64(31): 32147.0,\n",
       " np.int64(32): 59518.0,\n",
       " np.int64(33): 34656.0,\n",
       " np.int64(34): 41609.0,\n",
       " np.int64(35): 22439.0,\n",
       " np.int64(36): 38374.0,\n",
       " np.int64(37): 32658.0,\n",
       " np.int64(38): 54052.0,\n",
       " np.int64(39): 59457.0,\n",
       " np.int64(40): 59486.0,\n",
       " np.int64(41): 54293.0,\n",
       " np.int64(42): 29876.0,\n",
       " np.int64(43): 54327.0,\n",
       " np.int64(44): 54332.0,\n",
       " np.int64(45): 48229.0,\n",
       " np.int64(46): 54404.0,\n",
       " np.int64(47): 29793.0,\n",
       " np.int64(48): 28776.0,\n",
       " np.int64(49): 52193.0,\n",
       " np.int64(50): 59519.0,\n",
       " np.int64(51): 19775.0,\n",
       " np.int64(52): 19791.0,\n",
       " np.int64(53): 45254.0,\n",
       " np.int64(54): 29794.0,\n",
       " np.int64(55): 49702.0,\n",
       " np.int64(56): 32848.0,\n",
       " np.int64(57): 54142.0,\n",
       " np.int64(58): 29817.0,\n",
       " np.int64(59): 18187.0,\n",
       " np.int64(60): 1156.0,\n",
       " np.int64(61): 27510.0,\n",
       " np.int64(62): 43714.0,\n",
       " np.int64(63): 19793.0,\n",
       " np.int64(64): 17698.0,\n",
       " np.int64(65): 44540.0,\n",
       " np.int64(66): 34653.0,\n",
       " np.int64(67): 41513.0,\n",
       " np.int64(68): 34238.0,\n",
       " np.int64(69): 38612.0,\n",
       " np.int64(70): 30704.0,\n",
       " np.int64(71): 24224.0,\n",
       " np.int64(72): 42413.0,\n",
       " np.int64(73): 38415.0,\n",
       " np.int64(74): 24216.0,\n",
       " np.int64(75): 24973.0,\n",
       " np.int64(76): 22015.0,\n",
       " np.int64(77): 19635.0,\n",
       " np.int64(78): 38507.0,\n",
       " np.int64(79): 41639.0,\n",
       " np.int64(80): 18186.0,\n",
       " np.int64(81): 28574.0,\n",
       " np.int64(82): 27923.0,\n",
       " np.int64(83): 30552.0,\n",
       " np.int64(84): 31646.0,\n",
       " np.int64(85): 44556.0,\n",
       " np.int64(86): 25147.0,\n",
       " np.int64(87): 22486.0,\n",
       " np.int64(88): 42728.0,\n",
       " np.int64(89): 31690.0,\n",
       " np.int64(90): 24521.0,\n",
       " np.int64(91): 24823.0,\n",
       " np.int64(92): 26464.0,\n",
       " np.int64(93): 19752.0,\n",
       " np.int64(94): 41736.0,\n",
       " np.int64(95): 42447.0,\n",
       " np.int64(96): 17470.0,\n",
       " np.int64(97): 13712.0,\n",
       " np.int64(98): 40916.0,\n",
       " np.int64(99): 44379.0,\n",
       " np.int64(100): 34379.0,\n",
       " np.int64(101): 22166.0,\n",
       " np.int64(102): 34237.0,\n",
       " np.int64(103): 44167.0,\n",
       " np.int64(104): 53173.0,\n",
       " np.int64(105): 55102.0,\n",
       " np.int64(106): 2143.0,\n",
       " np.int64(107): 38294.0,\n",
       " np.int64(108): 43353.0,\n",
       " np.int64(109): 45248.0,\n",
       " np.int64(110): 47110.0,\n",
       " np.int64(111): 40968.0,\n",
       " np.int64(112): 28611.0,\n",
       " np.int64(113): 19300.0,\n",
       " np.int64(114): 42052.0,\n",
       " np.int64(115): 42053.0,\n",
       " np.int64(116): 27924.0,\n",
       " np.int64(117): 29739.0,\n",
       " np.int64(118): 33981.0,\n",
       " np.int64(119): 54328.0,\n",
       " np.int64(120): 42392.0,\n",
       " np.int64(121): 22939.0,\n",
       " np.int64(122): 41958.0,\n",
       " np.int64(123): 28759.0,\n",
       " np.int64(124): 28992.0,\n",
       " np.int64(125): 35135.0,\n",
       " np.int64(126): 13711.0,\n",
       " np.int64(127): 33527.0,\n",
       " np.int64(128): 30084.0,\n",
       " np.int64(129): 29738.0,\n",
       " np.int64(130): 37960.0,\n",
       " np.int64(131): 38313.0,\n",
       " np.int64(132): 43566.0,\n",
       " np.int64(133): 44537.0,\n",
       " np.int64(134): 35036.0,\n",
       " np.int64(135): 18164.0,\n",
       " np.int64(136): 22258.0,\n",
       " np.int64(137): 32634.0,\n",
       " np.int64(138): 37931.0,\n",
       " np.int64(139): 37961.0,\n",
       " np.int64(140): 38470.0,\n",
       " np.int64(141): 41610.0,\n",
       " np.int64(142): 28864.0,\n",
       " np.int64(143): 44411.0,\n",
       " np.int64(144): 22987.0,\n",
       " np.int64(145): 38471.0,\n",
       " np.int64(146): 44823.0,\n",
       " np.int64(147): 34644.0,\n",
       " np.int64(148): 44682.0,\n",
       " np.int64(149): 41942.0,\n",
       " np.int64(150): 18650.0,\n",
       " np.int64(151): 34970.0,\n",
       " np.int64(152): 45329.0,\n",
       " np.int64(153): 44555.0,\n",
       " np.int64(154): 49649.0,\n",
       " np.int64(155): 20101.0,\n",
       " np.int64(156): 13197.0,\n",
       " np.int64(157): 32576.0,\n",
       " np.int64(158): 45335.0,\n",
       " np.int64(159): 17523.0,\n",
       " np.int64(160): 22459.0,\n",
       " np.int64(161): 24549.0,\n",
       " np.int64(162): 34784.0,\n",
       " np.int64(163): 37738.0,\n",
       " np.int64(164): 38361.0,\n",
       " np.int64(165): 41916.0,\n",
       " np.int64(166): 29952.0,\n",
       " np.int64(167): 44068.0,\n",
       " np.int64(168): 32655.0,\n",
       " np.int64(169): 41339.0,\n",
       " np.int64(170): 41512.0,\n",
       " np.int64(171): 42029.0,\n",
       " np.int64(172): 48290.0,\n",
       " np.int64(173): 44558.0,\n",
       " np.int64(174): 54339.0,\n",
       " np.int64(175): 16560.0,\n",
       " np.int64(176): 19741.0,\n",
       " np.int64(177): 32421.0,\n",
       " np.int64(178): 37944.0,\n",
       " np.int64(179): 44453.0,\n",
       " np.int64(180): 44455.0,\n",
       " np.int64(181): 43548.0,\n",
       " np.int64(182): 54472.0,\n",
       " np.int64(183): 53160.0,\n",
       " np.int64(184): 59445.0,\n",
       " np.int64(185): 42285.0,\n",
       " np.int64(186): 42433.0,\n",
       " np.int64(187): 2187.0,\n",
       " np.int64(188): 27927.0,\n",
       " np.int64(189): 32401.0,\n",
       " np.int64(190): 34368.0,\n",
       " np.int64(191): 37955.0,\n",
       " np.int64(192): 44554.0,\n",
       " np.int64(193): 48181.0,\n",
       " np.int64(194): 37529.0,\n",
       " np.int64(195): 22206.0,\n",
       " np.int64(196): 41587.0,\n",
       " np.int64(197): 34806.0,\n",
       " np.int64(198): 42049.0,\n",
       " np.int64(199): 29873.0,\n",
       " np.int64(200): 41918.0,\n",
       " np.int64(201): 51336.0,\n",
       " np.int64(202): 42326.0,\n",
       " np.int64(203): 44599.0,\n",
       " np.int64(204): 55129.0,\n",
       " np.int64(205): 29782.0,\n",
       " np.int64(206): 12020.0,\n",
       " np.int64(207): 18166.0,\n",
       " np.int64(208): 19777.0,\n",
       " np.int64(209): 20072.0,\n",
       " np.int64(210): 35589.0,\n",
       " np.int64(211): 37690.0,\n",
       " np.int64(212): 42051.0,\n",
       " np.int64(213): 42333.0,\n",
       " np.int64(214): 42445.0,\n",
       " np.int64(215): 42891.0,\n",
       " np.int64(216): 44045.0,\n",
       " np.int64(217): 47146.0,\n",
       " np.int64(218): 41832.0,\n",
       " np.int64(219): 32886.0,\n",
       " np.int64(220): 44000.0,\n",
       " np.int64(221): 48799.0,\n",
       " np.int64(222): 44571.0,\n",
       " np.int64(223): 42300.0,\n",
       " np.int64(224): 44606.0,\n",
       " np.int64(225): 47127.0,\n",
       " np.int64(226): 41489.0,\n",
       " np.int64(227): 59449.0,\n",
       " np.int64(228): 45375.0,\n",
       " np.int64(229): 35037.0,\n",
       " np.int64(230): 41947.0,\n",
       " np.int64(231): 41586.0,\n",
       " np.int64(232): 44557.0,\n",
       " np.int64(233): 42291.0,\n",
       " np.int64(234): 38296.0,\n",
       " np.int64(235): 24219.0,\n",
       " np.int64(236): 54310.0,\n",
       " np.int64(237): 48224.0,\n",
       " np.int64(238): 44363.0,\n",
       " np.int64(239): 41653.0,\n",
       " np.int64(240): 37628.0,\n",
       " np.int64(241): 18334.0,\n",
       " np.int64(242): 19773.0,\n",
       " np.int64(243): 19776.0,\n",
       " np.int64(244): 22083.0,\n",
       " np.int64(245): 29924.0,\n",
       " np.int64(246): 31723.0,\n",
       " np.int64(247): 31728.0,\n",
       " np.int64(248): 32515.0,\n",
       " np.int64(249): 35592.0,\n",
       " np.int64(250): 37919.0,\n",
       " np.int64(251): 37962.0,\n",
       " np.int64(252): 38773.0,\n",
       " np.int64(253): 40741.0,\n",
       " np.int64(254): 44545.0,\n",
       " np.int64(255): 49001.0,\n",
       " np.int64(256): 49753.0,\n",
       " np.int64(257): 44481.0,\n",
       " np.int64(258): 34531.0,\n",
       " np.int64(259): 47573.0,\n",
       " np.int64(260): 59443.0,\n",
       " np.int64(261): 34962.0,\n",
       " np.int64(262): 54853.0,\n",
       " np.int64(263): 54854.0,\n",
       " np.int64(264): 22307.0,\n",
       " np.int64(265): 57067.0,\n",
       " np.int64(266): 30124.0,\n",
       " np.int64(267): 58441.0,\n",
       " np.int64(268): 47568.0,\n",
       " np.int64(269): 47401.0,\n",
       " np.int64(270): 37720.0,\n",
       " np.int64(271): 47695.0,\n",
       " np.int64(272): 42027.0,\n",
       " np.int64(273): 1420.0,\n",
       " np.int64(274): 59455.0,\n",
       " np.int64(275): 47481.0,\n",
       " np.int64(276): 57068.0,\n",
       " np.int64(277): 32329.0,\n",
       " np.int64(278): 54001.0,\n",
       " np.int64(279): 22443.0,\n",
       " np.int64(280): 53159.0,\n",
       " np.int64(281): 48297.0,\n",
       " np.int64(282): 47576.0,\n",
       " np.int64(283): 35591.0,\n",
       " np.int64(284): 54976.0,\n",
       " np.int64(285): 54005.0,\n",
       " np.int64(286): 49622.0,\n",
       " np.int64(287): 47125.0,\n",
       " np.int64(288): 37577.0,\n",
       " np.int64(289): 54820.0,\n",
       " np.int64(290): 54343.0,\n",
       " np.int64(291): 59882.0,\n",
       " np.int64(292): 54296.0,\n",
       " np.int64(293): 44568.0,\n",
       " np.int64(294): 34781.0,\n",
       " np.int64(295): 47197.0,\n",
       " np.int64(296): 54006.0,\n",
       " np.int64(297): 47628.0,\n",
       " np.int64(298): 30222.0,\n",
       " np.int64(299): 55111.0,\n",
       " np.int64(300): 30238.0,\n",
       " np.int64(301): 41583.0,\n",
       " np.int64(302): 57045.0,\n",
       " np.int64(303): 41576.0,\n",
       " np.int64(304): 19422.0,\n",
       " np.int64(305): 34224.0,\n",
       " np.int64(306): 24364.0,\n",
       " np.int64(307): 17089.0,\n",
       " np.int64(308): 26471.0,\n",
       " np.int64(309): 32330.0,\n",
       " np.int64(310): 35134.0,\n",
       " np.int64(311): 41805.0,\n",
       " np.int64(312): 22868.0,\n",
       " np.int64(313): 35590.0,\n",
       " np.int64(314): 35595.0,\n",
       " np.int64(315): 37696.0,\n",
       " np.int64(316): 38528.0,\n",
       " np.int64(317): 41343.0,\n",
       " np.int64(318): 42038.0,\n",
       " np.int64(319): 42881.0,\n",
       " np.int64(320): 44003.0,\n",
       " np.int64(321): 44032.0,\n",
       " np.int64(322): 44297.0,\n",
       " np.int64(323): 49625.0,\n",
       " np.int64(324): 59487.0,\n",
       " np.int64(325): 49722.0,\n",
       " np.int64(326): 40752.0,\n",
       " np.int64(327): 49480.0,\n",
       " np.int64(328): 32414.0,\n",
       " np.int64(329): 30217.0,\n",
       " np.int64(330): 19137.0,\n",
       " np.int64(331): 29912.0,\n",
       " np.int64(332): 22487.0,\n",
       " np.int64(333): 26466.0,\n",
       " np.int64(334): 35489.0,\n",
       " np.int64(335): 24199.0,\n",
       " np.int64(336): 7334.0,\n",
       " np.int64(337): 51294.0,\n",
       " np.int64(338): 59485.0,\n",
       " np.int64(339): 55076.0,\n",
       " np.int64(340): 52200.0,\n",
       " np.int64(341): 42398.0,\n",
       " np.int64(342): 54454.0,\n",
       " np.int64(343): 47124.0,\n",
       " np.int64(344): 47252.0,\n",
       " np.int64(345): 49184.0,\n",
       " np.int64(346): 54134.0,\n",
       " np.int64(347): 48176.0,\n",
       " np.int64(348): 33650.0,\n",
       " np.int64(349): 32840.0,\n",
       " np.int64(350): 29966.0,\n",
       " np.int64(351): 29855.0,\n",
       " np.int64(352): 29848.0,\n",
       " np.int64(353): 30235.0,\n",
       " np.int64(354): 18335.0,\n",
       " np.int64(355): 20036.0,\n",
       " np.int64(356): 42010.0,\n",
       " np.int64(357): 42286.0,\n",
       " np.int64(358): 54333.0,\n",
       " np.int64(359): 55138.0,\n",
       " np.int64(360): 47435.0,\n",
       " np.int64(361): 55091.0,\n",
       " np.int64(362): 18333.0,\n",
       " np.int64(363): 41457.0,\n",
       " np.int64(364): 40925.0,\n",
       " np.int64(365): 59470.0,\n",
       " np.int64(366): 22864.0,\n",
       " np.int64(367): 48802.0,\n",
       " np.int64(368): 43488.0,\n",
       " np.int64(369): 42405.0,\n",
       " np.int64(370): 43483.0,\n",
       " np.int64(371): 59517.0,\n",
       " np.int64(372): 53177.0,\n",
       " np.int64(373): 43788.0,\n",
       " np.int64(374): 44508.0,\n",
       " np.int64(375): 44506.0,\n",
       " np.int64(376): 28376.0,\n",
       " np.int64(377): 44505.0,\n",
       " np.int64(378): 37872.0,\n",
       " np.int64(379): 58824.0,\n",
       " np.int64(380): 38833.0,\n",
       " np.int64(381): 43568.0,\n",
       " np.int64(382): 24362.0,\n",
       " np.int64(383): 59442.0,\n",
       " np.int64(384): 54490.0,\n",
       " np.int64(385): 18274.0,\n",
       " np.int64(386): 54160.0,\n",
       " np.int64(387): 34527.0,\n",
       " np.int64(388): 47126.0,\n",
       " np.int64(389): 54066.0,\n",
       " np.int64(390): 24524.0,\n",
       " np.int64(391): 54482.0,\n",
       " np.int64(392): 59446.0,\n",
       " np.int64(393): 50076.0,\n",
       " np.int64(394): 54329.0,\n",
       " np.int64(395): 47398.0,\n",
       " np.int64(396): 44063.0,\n",
       " np.int64(397): 29891.0,\n",
       " np.int64(398): 45729.0,\n",
       " np.int64(399): 30233.0,\n",
       " np.int64(400): 29940.0,\n",
       " np.int64(401): 30246.0,\n",
       " np.int64(402): 54406.0,\n",
       " np.int64(403): 24907.0,\n",
       " np.int64(404): 42292.0,\n",
       " np.int64(405): 29565.0,\n",
       " np.int64(406): 41978.0,\n",
       " np.int64(407): 32504.0,\n",
       " np.int64(408): 30219.0,\n",
       " np.int64(409): 54403.0,\n",
       " np.int64(410): 54342.0,\n",
       " np.int64(411): 53290.0,\n",
       " np.int64(412): 16739.0,\n",
       " np.int64(413): 52199.0,\n",
       " np.int64(414): 55105.0,\n",
       " np.int64(415): 49967.0,\n",
       " np.int64(416): 42327.0,\n",
       " np.int64(417): 48235.0,\n",
       " np.int64(418): 48191.0,\n",
       " np.int64(419): 59477.0,\n",
       " np.int64(420): 48161.0,\n",
       " np.int64(421): 42284.0,\n",
       " np.int64(422): 54176.0,\n",
       " np.int64(423): 44479.0,\n",
       " np.int64(424): 52470.0,\n",
       " np.int64(425): 42306.0,\n",
       " np.int64(426): 38733.0,\n",
       " np.int64(427): 38774.0,\n",
       " np.int64(428): 54029.0,\n",
       " np.int64(429): 41949.0,\n",
       " np.int64(430): 38778.0,\n",
       " np.int64(431): 59463.0,\n",
       " np.int64(432): 44674.0,\n",
       " np.int64(433): 50083.0,\n",
       " np.int64(434): 42390.0,\n",
       " np.int64(435): 49966.0,\n",
       " np.int64(436): 41607.0,\n",
       " np.int64(437): 30226.0,\n",
       " np.int64(438): 41349.0,\n",
       " np.int64(439): 43351.0,\n",
       " np.int64(440): 50078.0,\n",
       " np.int64(441): 30232.0,\n",
       " np.int64(442): 48912.0,\n",
       " np.int64(443): 24797.0,\n",
       " np.int64(444): 30389.0,\n",
       " np.int64(445): 28986.0,\n",
       " np.int64(446): 49181.0,\n",
       " np.int64(447): 34830.0,\n",
       " np.int64(448): 59469.0,\n",
       " np.int64(449): 32512.0,\n",
       " np.int64(450): 28377.0,\n",
       " np.int64(451): 57518.0,\n",
       " np.int64(452): 42393.0,\n",
       " np.int64(453): 24787.0,\n",
       " np.int64(454): 16742.0,\n",
       " np.int64(455): 37575.0,\n",
       " np.int64(456): 50087.0,\n",
       " np.int64(457): 59426.0,\n",
       " np.int64(458): 32837.0,\n",
       " np.int64(459): 34529.0,\n",
       " np.int64(460): 42283.0,\n",
       " np.int64(461): 42439.0,\n",
       " np.int64(462): 42417.0,\n",
       " np.int64(463): 41564.0,\n",
       " np.int64(464): 47142.0,\n",
       " np.int64(465): 19816.0,\n",
       " np.int64(466): 20003.0,\n",
       " np.int64(467): 54855.0,\n",
       " np.int64(468): 54334.0,\n",
       " np.int64(469): 24226.0,\n",
       " np.int64(470): 56874.0,\n",
       " np.int64(471): 41886.0,\n",
       " np.int64(472): 48530.0,\n",
       " np.int64(473): 32919.0,\n",
       " np.int64(474): 41520.0,\n",
       " np.int64(475): 7332.0,\n",
       " np.int64(476): 54410.0,\n",
       " np.int64(477): 28426.0,\n",
       " np.int64(478): 48522.0,\n",
       " np.int64(479): 48190.0,\n",
       " np.int64(480): 22006.0,\n",
       " np.int64(481): 28757.0,\n",
       " np.int64(482): 44612.0,\n",
       " np.int64(483): 10485.0,\n",
       " np.int64(484): 42281.0,\n",
       " np.int64(485): 57539.0,\n",
       " np.int64(486): 47657.0,\n",
       " np.int64(487): 47636.0,\n",
       " np.int64(488): 17086.0,\n",
       " np.int64(489): 41577.0,\n",
       " np.int64(490): 47119.0,\n",
       " np.int64(491): 41935.0,\n",
       " np.int64(492): 41141.0,\n",
       " np.int64(493): 44591.0,\n",
       " np.int64(494): 49639.0,\n",
       " np.int64(495): 22225.0,\n",
       " np.int64(496): 22024.0,\n",
       " np.int64(497): 42410.0,\n",
       " np.int64(498): 43484.0,\n",
       " np.int64(499): 22082.0,\n",
       " np.int64(500): 41865.0,\n",
       " np.int64(501): 48177.0,\n",
       " np.int64(502): 47667.0,\n",
       " np.int64(503): 44576.0,\n",
       " np.int64(504): 44569.0,\n",
       " np.int64(505): 48529.0,\n",
       " np.int64(506): 12075.0,\n",
       " np.int64(507): 55107.0,\n",
       " np.int64(508): 59429.0,\n",
       " np.int64(509): 45403.0,\n",
       " np.int64(510): 42073.0,\n",
       " np.int64(511): 54344.0,\n",
       " np.int64(512): 41377.0,\n",
       " np.int64(513): 24897.0,\n",
       " np.int64(514): 50075.0,\n",
       " np.int64(515): 49641.0,\n",
       " np.int64(516): 54466.0,\n",
       " np.int64(517): 48226.0,\n",
       " np.int64(518): 22049.0,\n",
       " np.int64(519): 29892.0,\n",
       " np.int64(520): 54481.0,\n",
       " np.int64(521): 50362.0,\n",
       " np.int64(522): 54470.0,\n",
       " np.int64(523): 54491.0,\n",
       " np.int64(524): 24358.0,\n",
       " np.int64(525): 54467.0,\n",
       " np.int64(526): 41619.0,\n",
       " np.int64(527): 52154.0,\n",
       " np.int64(528): 34534.0,\n",
       " np.int64(529): 55106.0,\n",
       " np.int64(530): 57225.0,\n",
       " np.int64(531): 49180.0,\n",
       " np.int64(532): 58926.0,\n",
       " np.int64(533): 42046.0,\n",
       " np.int64(534): 49496.0,\n",
       " np.int64(535): 58357.0,\n",
       " np.int64(536): 30227.0,\n",
       " np.int64(537): 54271.0,\n",
       " np.int64(538): 34786.0,\n",
       " np.int64(539): 37993.0,\n",
       " np.int64(540): 40999.0,\n",
       " np.int64(541): 41373.0,\n",
       " np.int64(542): 52244.0,\n",
       " np.int64(543): 53186.0,\n",
       " np.int64(544): 37561.0,\n",
       " np.int64(545): 47147.0,\n",
       " np.int64(546): 54312.0,\n",
       " np.int64(547): 54311.0,\n",
       " np.int64(548): 49502.0,\n",
       " np.int64(549): 59483.0,\n",
       " np.int64(550): 54326.0,\n",
       " np.int64(551): 43552.0,\n",
       " np.int64(552): 41518.0,\n",
       " np.int64(553): 47673.0,\n",
       " np.int64(554): 47674.0,\n",
       " np.int64(555): 47478.0,\n",
       " np.int64(556): 19615.0,\n",
       " np.int64(557): 18397.0,\n",
       " np.int64(558): 41754.0,\n",
       " np.int64(559): 43787.0,\n",
       " np.int64(560): 49580.0,\n",
       " np.int64(561): 48520.0,\n",
       " np.int64(562): 28277.0,\n",
       " np.int64(563): 41342.0,\n",
       " np.int64(564): 35144.0,\n",
       " np.int64(565): 42411.0,\n",
       " np.int64(566): 38608.0,\n",
       " np.int64(567): 41515.0,\n",
       " np.int64(568): 38639.0,\n",
       " np.int64(569): 22255.0,\n",
       " np.int64(570): 54026.0,\n",
       " np.int64(571): 41514.0,\n",
       " np.int64(572): 32849.0,\n",
       " np.int64(573): 44171.0,\n",
       " np.int64(574): 42434.0,\n",
       " np.int64(575): 29566.0,\n",
       " np.int64(576): 48198.0,\n",
       " np.int64(577): 53806.0,\n",
       " np.int64(578): 32839.0,\n",
       " np.int64(579): 54488.0,\n",
       " np.int64(580): 41177.0,\n",
       " np.int64(581): 50074.0,\n",
       " np.int64(582): 52379.0,\n",
       " np.int64(583): 54409.0,\n",
       " np.int64(584): 57520.0,\n",
       " np.int64(585): 59438.0,\n",
       " np.int64(586): 59489.0,\n",
       " np.int64(587): 18171.0,\n",
       " np.int64(588): 18174.0,\n",
       " np.int64(589): 28441.0,\n",
       " np.int64(590): 29791.0,\n",
       " np.int64(591): 30081.0,\n",
       " np.int64(592): 32632.0,\n",
       " np.int64(593): 32641.0,\n",
       " np.int64(594): 34229.0,\n",
       " np.int64(595): 34788.0,\n",
       " np.int64(596): 34815.0,\n",
       " np.int64(597): 37925.0,\n",
       " np.int64(598): 41055.0,\n",
       " np.int64(599): 41065.0,\n",
       " np.int64(600): 41492.0,\n",
       " np.int64(601): 41527.0,\n",
       " np.int64(602): 41888.0,\n",
       " np.int64(603): 41936.0,\n",
       " np.int64(604): 41948.0,\n",
       " np.int64(605): 42391.0,\n",
       " np.int64(606): 42399.0,\n",
       " np.int64(607): 43200.0,\n",
       " np.int64(608): 44446.0,\n",
       " np.int64(609): 44475.0,\n",
       " np.int64(610): 47207.0,\n",
       " np.int64(611): 47618.0,\n",
       " np.int64(612): 47665.0,\n",
       " np.int64(613): 48800.0,\n",
       " np.int64(614): 49503.0,\n",
       " np.int64(615): 49504.0,\n",
       " np.int64(616): 49640.0,\n",
       " np.int64(617): 53997.0,\n",
       " np.int64(618): 57528.0,\n",
       " np.int64(619): 41575.0,\n",
       " np.int64(620): 41632.0,\n",
       " np.int64(621): 41991.0,\n",
       " np.int64(622): 42077.0,\n",
       " np.int64(623): 22257.0,\n",
       " np.int64(624): 18204.0,\n",
       " np.int64(625): 30223.0,\n",
       " np.int64(626): 34809.0,\n",
       " np.int64(627): 38228.0,\n",
       " np.int64(628): 44570.0,\n",
       " np.int64(629): 47324.0,\n",
       " np.int64(630): 47483.0,\n",
       " np.int64(631): 48122.0,\n",
       " np.int64(632): 49633.0,\n",
       " np.int64(633): 52192.0,\n",
       " np.int64(634): 44067.0,\n",
       " np.int64(635): 49492.0,\n",
       " np.int64(636): 51786.0,\n",
       " np.int64(637): 52469.0,\n",
       " np.int64(638): 59430.0,\n",
       " np.int64(639): 57519.0,\n",
       " np.int64(640): 49620.0,\n",
       " np.int64(641): 41747.0,\n",
       " np.int64(642): 29934.0,\n",
       " np.int64(643): 19027.0,\n",
       " np.int64(644): 41629.0,\n",
       " np.int64(645): 41445.0,\n",
       " np.int64(646): 21981.0,\n",
       " np.int64(647): 18275.0,\n",
       " np.int64(648): 38414.0,\n",
       " np.int64(649): 25015.0,\n",
       " np.int64(650): 33206.0,\n",
       " np.int64(651): 41582.0,\n",
       " np.int64(652): 43262.0,\n",
       " np.int64(653): 22931.0,\n",
       " np.int64(654): 30218.0,\n",
       " np.int64(655): 22264.0,\n",
       " np.int64(656): 22348.0,\n",
       " np.int64(657): 41050.0,\n",
       " np.int64(658): 34782.0,\n",
       " np.int64(659): 38398.0,\n",
       " np.int64(660): 41536.0,\n",
       " np.int64(661): 22440.0,\n",
       " np.int64(662): 43489.0,\n",
       " np.int64(663): 32952.0,\n",
       " np.int64(664): 47655.0,\n",
       " np.int64(665): 50085.0,\n",
       " np.int64(666): 56867.0,\n",
       " np.int64(667): 54094.0,\n",
       " np.int64(668): 44265.0,\n",
       " np.int64(669): 37627.0,\n",
       " np.int64(670): 53425.0,\n",
       " np.int64(671): 44533.0,\n",
       " np.int64(672): 40852.0,\n",
       " np.int64(673): 41367.0,\n",
       " np.int64(674): 29733.0,\n",
       " np.int64(675): 54114.0,\n",
       " np.int64(676): 53278.0,\n",
       " np.int64(677): 51726.0,\n",
       " np.int64(678): 38468.0,\n",
       " np.int64(679): 41758.0,\n",
       " np.int64(680): 6503.0,\n",
       " np.int64(681): 38460.0,\n",
       " np.int64(682): 47475.0,\n",
       " np.int64(683): 28561.0,\n",
       " np.int64(684): 47760.0,\n",
       " np.int64(685): 48126.0,\n",
       " np.int64(686): 54394.0,\n",
       " np.int64(687): 32866.0,\n",
       " np.int64(688): 43744.0,\n",
       " np.int64(689): 38298.0,\n",
       " np.int64(690): 40963.0,\n",
       " np.int64(691): 41652.0,\n",
       " np.int64(692): 20066.0,\n",
       " np.int64(693): 53320.0,\n",
       " np.int64(694): 58525.0,\n",
       " np.int64(695): 54008.0,\n",
       " np.int64(696): 51730.0,\n",
       " np.int64(697): 44474.0,\n",
       " np.int64(698): 34783.0,\n",
       " np.int64(699): 38861.0,\n",
       " np.int64(700): 47566.0,\n",
       " np.int64(701): 18168.0,\n",
       " np.int64(702): 37928.0,\n",
       " np.int64(703): 42412.0,\n",
       " np.int64(704): 47724.0,\n",
       " np.int64(705): 43555.0,\n",
       " np.int64(706): 32340.0,\n",
       " np.int64(707): 41521.0,\n",
       " np.int64(708): 47439.0,\n",
       " np.int64(709): 47616.0,\n",
       " np.int64(710): 54421.0,\n",
       " np.int64(711): 49894.0,\n",
       " np.int64(712): 42079.0,\n",
       " np.int64(713): 22023.0,\n",
       " np.int64(714): 32582.0,\n",
       " np.int64(715): 38085.0,\n",
       " np.int64(716): 38299.0,\n",
       " np.int64(717): 44478.0,\n",
       " np.int64(718): 54159.0,\n",
       " np.int64(719): 34831.0,\n",
       " np.int64(720): 21974.0,\n",
       " np.int64(721): 43545.0,\n",
       " np.int64(722): 55134.0,\n",
       " np.int64(723): 41987.0,\n",
       " np.int64(724): 47675.0,\n",
       " np.int64(725): 44672.0,\n",
       " np.int64(726): 54147.0,\n",
       " np.int64(727): 14315.0,\n",
       " np.int64(728): 37982.0,\n",
       " np.int64(729): 54230.0,\n",
       " np.int64(730): 34787.0,\n",
       " np.int64(731): 54069.0,\n",
       " np.int64(732): 47286.0,\n",
       " np.int64(733): 49199.0,\n",
       " np.int64(734): 32645.0,\n",
       " np.int64(735): 57164.0,\n",
       " np.int64(736): 44394.0,\n",
       " np.int64(737): 29937.0,\n",
       " np.int64(738): 37832.0,\n",
       " np.int64(739): 59472.0,\n",
       " np.int64(740): 41961.0,\n",
       " np.int64(741): 34530.0,\n",
       " np.int64(742): 37691.0,\n",
       " np.int64(743): 37562.0,\n",
       " np.int64(744): 19842.0,\n",
       " np.int64(745): 22867.0,\n",
       " np.int64(746): 19317.0,\n",
       " np.int64(747): 43481.0,\n",
       " np.int64(748): 35597.0,\n",
       " np.int64(749): 54088.0,\n",
       " np.int64(750): 54065.0,\n",
       " np.int64(751): 58611.0,\n",
       " np.int64(752): 54232.0,\n",
       " np.int64(753): 41673.0,\n",
       " np.int64(754): 19631.0,\n",
       " np.int64(755): 41148.0,\n",
       " np.int64(756): 41835.0,\n",
       " np.int64(757): 49934.0,\n",
       " np.int64(758): 54447.0,\n",
       " np.int64(759): 20122.0,\n",
       " np.int64(760): 45336.0,\n",
       " np.int64(761): 41062.0,\n",
       " np.int64(762): 54294.0,\n",
       " np.int64(763): 32507.0,\n",
       " np.int64(764): 7330.0,\n",
       " np.int64(765): 57224.0,\n",
       " np.int64(766): 53745.0,\n",
       " np.int64(767): 47288.0,\n",
       " np.int64(768): 47325.0,\n",
       " np.int64(769): 51719.0,\n",
       " np.int64(770): 44254.0,\n",
       " np.int64(771): 41590.0,\n",
       " np.int64(772): 60636.0,\n",
       " np.int64(773): 40994.0,\n",
       " np.int64(774): 41004.0,\n",
       " np.int64(775): 52434.0,\n",
       " np.int64(776): 54446.0,\n",
       " np.int64(777): 60723.0,\n",
       " np.int64(778): 54169.0,\n",
       " np.int64(779): 54492.0,\n",
       " np.int64(780): 41589.0,\n",
       " np.int64(781): 42416.0,\n",
       " np.int64(782): 54497.0,\n",
       " np.int64(783): 53188.0,\n",
       " np.int64(784): 49714.0,\n",
       " np.int64(785): 54993.0,\n",
       " np.int64(786): 55160.0,\n",
       " np.int64(787): 54440.0,\n",
       " np.int64(788): 54138.0,\n",
       " np.int64(789): 54463.0,\n",
       " np.int64(790): 59454.0,\n",
       " np.int64(791): 54502.0,\n",
       " np.int64(792): 41481.0,\n",
       " np.int64(793): 54503.0,\n",
       " np.int64(794): 34532.0,\n",
       " np.int64(795): 54407.0,\n",
       " np.int64(796): 54076.0,\n",
       " np.int64(797): 54453.0,\n",
       " np.int64(798): 54433.0,\n",
       " np.int64(799): 54398.0,\n",
       " np.int64(800): 42404.0,\n",
       " np.int64(801): 52439.0,\n",
       " np.int64(802): 58306.0,\n",
       " np.int64(803): 49898.0,\n",
       " np.int64(804): 49626.0,\n",
       " np.int64(805): 49703.0,\n",
       " np.int64(806): 38521.0,\n",
       " np.int64(807): 42062.0,\n",
       " np.int64(808): 18214.0,\n",
       " np.int64(809): 59063.0,\n",
       " np.int64(810): 44546.0,\n",
       " np.int64(811): 54418.0,\n",
       " np.int64(812): 58760.0,\n",
       " np.int64(813): 32671.0,\n",
       " np.int64(814): 61128.0,\n",
       " np.int64(815): 48236.0,\n",
       " np.int64(816): 49756.0,\n",
       " np.int64(817): 47626.0,\n",
       " np.int64(818): 58613.0,\n",
       " np.int64(819): 44195.0,\n",
       " np.int64(820): 47277.0,\n",
       " np.int64(821): 44169.0,\n",
       " np.int64(822): 47722.0,\n",
       " np.int64(823): 55085.0,\n",
       " np.int64(824): 41962.0,\n",
       " np.int64(825): 49172.0,\n",
       " np.int64(826): 49200.0,\n",
       " np.int64(827): 26470.0,\n",
       " np.int64(828): 41900.0,\n",
       " np.int64(829): 44454.0,\n",
       " np.int64(830): 44480.0,\n",
       " np.int64(831): 46142.0,\n",
       " np.int64(832): 42403.0,\n",
       " np.int64(833): 54411.0,\n",
       " np.int64(834): 49852.0,\n",
       " np.int64(835): 9970.0,\n",
       " np.int64(836): 41563.0,\n",
       " np.int64(837): 53744.0,\n",
       " np.int64(838): 41068.0,\n",
       " np.int64(839): 53398.0,\n",
       " np.int64(840): 52452.0,\n",
       " np.int64(841): 60645.0,\n",
       " np.int64(842): 54086.0,\n",
       " np.int64(843): 16320.0,\n",
       " np.int64(844): 22020.0,\n",
       " np.int64(845): 57417.0,\n",
       " np.int64(846): 51713.0,\n",
       " np.int64(847): 46143.0,\n",
       " np.int64(848): 41072.0,\n",
       " np.int64(849): 38117.0,\n",
       " np.int64(850): 57517.0,\n",
       " np.int64(851): 54081.0,\n",
       " np.int64(852): 52239.0,\n",
       " np.int64(853): 49853.0,\n",
       " np.int64(854): 54441.0,\n",
       " np.int64(855): 52293.0,\n",
       " np.int64(856): 25149.0,\n",
       " np.int64(857): 58491.0,\n",
       " np.int64(858): 60724.0,\n",
       " np.int64(859): 26457.0,\n",
       " np.int64(860): 32134.0,\n",
       " np.int64(861): 51703.0,\n",
       " np.int64(862): 6289.0,\n",
       " np.int64(863): 41640.0,\n",
       " np.int64(864): 43485.0,\n",
       " np.int64(865): 38522.0,\n",
       " np.int64(866): 51714.0,\n",
       " np.int64(867): 37839.0,\n",
       " np.int64(868): 48207.0,\n",
       " np.int64(869): 53251.0,\n",
       " np.int64(870): 47381.0,\n",
       " np.int64(871): 41039.0,\n",
       " np.int64(872): 48194.0,\n",
       " np.int64(873): 44295.0,\n",
       " np.int64(874): 17682.0,\n",
       " np.int64(875): 37915.0,\n",
       " np.int64(876): 53409.0,\n",
       " np.int64(877): 54024.0,\n",
       " np.int64(878): 49812.0,\n",
       " np.int64(879): 52390.0,\n",
       " np.int64(880): 57194.0,\n",
       " np.int64(881): 55334.0,\n",
       " np.int64(882): 55159.0,\n",
       " np.int64(883): 54465.0,\n",
       " np.int64(884): 48192.0,\n",
       " np.int64(885): 22030.0,\n",
       " np.int64(886): 22216.0,\n",
       " np.int64(887): 41591.0,\n",
       " np.int64(888): 50105.0,\n",
       " np.int64(889): 54476.0,\n",
       " np.int64(890): 54500.0,\n",
       " np.int64(891): 51721.0,\n",
       " np.int64(892): 52432.0,\n",
       " np.int64(893): 54487.0,\n",
       " np.int64(894): 41499.0,\n",
       " np.int64(895): 43562.0,\n",
       " np.int64(896): 34814.0,\n",
       " np.int64(897): 42278.0,\n",
       " np.int64(898): 47437.0,\n",
       " np.int64(899): 32642.0,\n",
       " np.int64(900): 19806.0,\n",
       " np.int64(901): 52117.0,\n",
       " np.int64(902): 59997.0,\n",
       " np.int64(903): 44246.0,\n",
       " np.int64(904): 47717.0,\n",
       " np.int64(905): 34710.0,\n",
       " np.int64(906): 41638.0,\n",
       " np.int64(907): 31671.0,\n",
       " np.int64(908): 54501.0,\n",
       " np.int64(909): 58538.0,\n",
       " np.int64(910): 41567.0,\n",
       " np.int64(911): 53336.0,\n",
       " np.int64(912): 41089.0,\n",
       " np.int64(913): 41834.0,\n",
       " np.int64(914): 49634.0,\n",
       " np.int64(915): 57165.0,\n",
       " np.int64(916): 52413.0,\n",
       " np.int64(917): 38137.0,\n",
       " np.int64(918): 47438.0,\n",
       " np.int64(919): 54127.0,\n",
       " np.int64(920): 42039.0,\n",
       " np.int64(921): 37930.0,\n",
       " np.int64(922): 52467.0,\n",
       " np.int64(923): 42057.0,\n",
       " np.int64(924): 44259.0,\n",
       " np.int64(925): 42004.0,\n",
       " np.int64(926): 44743.0,\n",
       " np.int64(927): 54509.0,\n",
       " np.int64(928): 52194.0,\n",
       " np.int64(929): 58382.0,\n",
       " np.int64(930): 54299.0,\n",
       " np.int64(931): 49190.0,\n",
       " np.int64(932): 54822.0,\n",
       " np.int64(933): 53249.0,\n",
       " np.int64(934): 44398.0,\n",
       " np.int64(935): 41409.0,\n",
       " np.int64(936): 42331.0,\n",
       " np.int64(937): 49490.0,\n",
       " np.int64(938): 30386.0,\n",
       " np.int64(939): 19026.0,\n",
       " np.int64(940): 47589.0,\n",
       " np.int64(941): 44267.0,\n",
       " np.int64(942): 41417.0,\n",
       " np.int64(943): 34810.0,\n",
       " np.int64(944): 48519.0,\n",
       " np.int64(945): 54420.0,\n",
       " np.int64(946): 53341.0,\n",
       " np.int64(947): 47504.0,\n",
       " np.int64(948): 43550.0,\n",
       " np.int64(949): 42389.0,\n",
       " np.int64(950): 52401.0,\n",
       " np.int64(951): 54397.0,\n",
       " np.int64(952): 21989.0,\n",
       " np.int64(953): 41578.0,\n",
       " np.int64(954): 49505.0,\n",
       " np.int64(955): 57016.0,\n",
       " np.int64(956): 49884.0,\n",
       " np.int64(957): 42419.0,\n",
       " np.int64(958): 59480.0,\n",
       " np.int64(959): 21988.0,\n",
       " np.int64(960): 58593.0,\n",
       " np.int64(961): 54135.0,\n",
       " np.int64(962): 53649.0,\n",
       " np.int64(963): 57173.0,\n",
       " np.int64(964): 59432.0,\n",
       " np.int64(965): 32860.0,\n",
       " np.int64(966): 50088.0,\n",
       " np.int64(967): 58331.0,\n",
       " np.int64(968): 54452.0,\n",
       " np.int64(969): 54474.0,\n",
       " np.int64(970): 55958.0,\n",
       " np.int64(971): 44542.0,\n",
       " np.int64(972): 54059.0,\n",
       " np.int64(973): 47135.0,\n",
       " np.int64(974): 12071.0,\n",
       " np.int64(975): 45374.0,\n",
       " np.int64(976): 28361.0,\n",
       " np.int64(977): 53860.0,\n",
       " np.int64(978): 41500.0,\n",
       " np.int64(979): 54442.0,\n",
       " np.int64(980): 52291.0,\n",
       " np.int64(981): 53407.0,\n",
       " np.int64(982): 32393.0,\n",
       " np.int64(983): 47708.0,\n",
       " np.int64(984): 53214.0,\n",
       " np.int64(985): 54087.0,\n",
       " np.int64(986): 54027.0,\n",
       " np.int64(987): 19783.0,\n",
       " np.int64(988): 38002.0,\n",
       " np.int64(989): 20068.0,\n",
       " np.int64(990): 43569.0,\n",
       " np.int64(991): 1176.0,\n",
       " np.int64(992): 24228.0,\n",
       " np.int64(993): 42032.0,\n",
       " np.int64(994): 32364.0,\n",
       " np.int64(995): 44609.0,\n",
       " np.int64(996): 22305.0,\n",
       " np.int64(997): 1079.0,\n",
       " np.int64(998): 44667.0,\n",
       " np.int64(999): 44543.0,\n",
       " ...}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_mapping_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming reverse_mapping_articles is already defined\n",
    "reverse_mapping_articles = {v: k for k, v in index_map_dict1['article'].items()}\n",
    "\n",
    "# Apply reverse mapping to each article in the list\n",
    "results_df['mapped_articles_id'] = results_df['predicted_articles'].apply(\n",
    "    lambda x: [reverse_mapping_articles.get(article, article) for article in x]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_mapping = {v: k for k, v in index_map_dict1['customer'].items()}\n",
    "\n",
    "# Map back to original customers_id\n",
    "results_df['mapped_customers_id'] = results_df['customers_id'].map(reverse_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "incompatible index of inserted column with frame index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\pandas\\core\\frame.py:12687\u001b[0m, in \u001b[0;36m_reindex_for_setitem\u001b[1;34m(value, index)\u001b[0m\n\u001b[0;32m  12686\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m> 12687\u001b[0m     reindexed_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m  12688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m  12689\u001b[0m     \u001b[38;5;66;03m# raised in MultiIndex.from_tuples, see test_insert_error_msmgs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\pandas\\core\\series.py:5153\u001b[0m, in \u001b[0;36mSeries.reindex\u001b[1;34m(self, index, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[0;32m   5136\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[0;32m   5137\u001b[0m     NDFrame\u001b[38;5;241m.\u001b[39mreindex,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m   5138\u001b[0m     klass\u001b[38;5;241m=\u001b[39m_shared_doc_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mklass\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5151\u001b[0m     tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5152\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[1;32m-> 5153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\pandas\\core\\generic.py:5610\u001b[0m, in \u001b[0;36mNDFrame.reindex\u001b[1;34m(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[0;32m   5609\u001b[0m \u001b[38;5;66;03m# perform the reindex on the axes\u001b[39;00m\n\u001b[1;32m-> 5610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_axes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5611\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m   5612\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\pandas\\core\\generic.py:5633\u001b[0m, in \u001b[0;36mNDFrame._reindex_axes\u001b[1;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[0;32m   5632\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(a)\n\u001b[1;32m-> 5633\u001b[0m new_index, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\n\u001b[0;32m   5635\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5637\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(a)\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:4433\u001b[0m, in \u001b[0;36mIndex.reindex\u001b[1;34m(self, target, method, level, limit, tolerance)\u001b[0m\n\u001b[0;32m   4431\u001b[0m             indexer, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_indexer_non_unique(target)\n\u001b[1;32m-> 4433\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap_reindex_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m target, indexer\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:2717\u001b[0m, in \u001b[0;36mMultiIndex._wrap_reindex_result\u001b[1;34m(self, target, indexer, preserve_names)\u001b[0m\n\u001b[0;32m   2716\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2717\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[43mMultiIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tuples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2718\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   2719\u001b[0m     \u001b[38;5;66;03m# not all tuples, see test_constructor_dict_multiindex_reindex_flat\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:222\u001b[0m, in \u001b[0;36mnames_compat.<locals>.new_meth\u001b[1;34m(self_or_cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_or_cls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:617\u001b[0m, in \u001b[0;36mMultiIndex.from_tuples\u001b[1;34m(cls, tuples, sortorder, names)\u001b[0m\n\u001b[0;32m    615\u001b[0m         tuples \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(tuples\u001b[38;5;241m.\u001b[39m_values)\n\u001b[1;32m--> 617\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtuples_to_object_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtuples\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tuples, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[1;32mlib.pyx:3029\u001b[0m, in \u001b[0;36mpandas._libs.lib.tuples_to_object_array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Buffer dtype mismatch, expected 'Python object' but got 'long long'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8524\\1061085734.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreverse_mapping_articles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marticle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marticle\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Convert the mapped_articles_id column to the desired format (float64)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mresults_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mapped_articles_id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mapped_articles_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float64'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Check the data type of the 'mapped_articles_id' column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mapped_articles_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4307\u001b[0m             \u001b[1;31m# Column to set is duplicated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4308\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4309\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4310\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4311\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4521\u001b[0m         \u001b[0mSeries\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mTimeSeries\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mconformed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mDataFrames\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4522\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4523\u001b[0m         \"\"\"\n\u001b[1;32m-> 4524\u001b[1;33m         \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrefs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4526\u001b[0m         if (\n\u001b[0;32m   4527\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   5259\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5260\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5261\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5262\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5263\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_reindex_for_setitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5266\u001b[0m             \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(value, index)\u001b[0m\n\u001b[0;32m  12690\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12691\u001b[0m             \u001b[1;31m# duplicate axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12692\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 12694\u001b[1;33m         raise TypeError(\n\u001b[0m\u001b[0;32m  12695\u001b[0m             \u001b[1;34m\"incompatible index of inserted column with frame index\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12696\u001b[0m         \u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12697\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mreindexed_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: incompatible index of inserted column with frame index"
     ]
    }
   ],
   "source": [
    "# Assuming reverse_mapping_articles is already defined\n",
    "reverse_mapping_articles = {v: k for k, v in index_map_dict1['article'].items()}\n",
    "\n",
    "# Apply reverse mapping to each article in the list and cast the result to float64\n",
    "results_df['mapped_articles_id'] = results_df['predicted_articles'].apply(\n",
    "    lambda x: [float(reverse_mapping_articles.get(article, article)) for article in x]\n",
    ")\n",
    "\n",
    "# Convert the mapped_articles_id column to the desired format (float64)\n",
    "results_df['mapped_articles_id'] = results_df['mapped_articles_id'].apply(pd.Series).stack().astype('float64')\n",
    "\n",
    "# Check the data type of the 'mapped_articles_id' column\n",
    "print(results_df['mapped_articles_id'].dtype)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2469 entries, 0 to 2468\n",
      "Data columns (total 4 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   customers_id         2469 non-null   int64  \n",
      " 1   predicted_articles   2469 non-null   object \n",
      " 2   mapped_articles_id   2469 non-null   object \n",
      " 3   mapped_customers_id  1436 non-null   float64\n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 77.3+ KB\n"
     ]
    }
   ],
   "source": [
    "results_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customers_id</th>\n",
       "      <th>predicted_articles</th>\n",
       "      <th>mapped_articles_id</th>\n",
       "      <th>mapped_customers_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9806</td>\n",
       "      <td>[72, 145, 105, 249]</td>\n",
       "      <td>[42413.0, 38471.0, 55102.0, 35592.0]</td>\n",
       "      <td>10421548.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126374</td>\n",
       "      <td>[72, 145, 105, 1503]</td>\n",
       "      <td>[42413.0, 38471.0, 55102.0, 41647.0]</td>\n",
       "      <td>10377320.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12345</td>\n",
       "      <td>[72, 145, 105, 1503]</td>\n",
       "      <td>[42413.0, 38471.0, 55102.0, 41647.0]</td>\n",
       "      <td>10292410.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>126357</td>\n",
       "      <td>[72, 145, 105, 1503]</td>\n",
       "      <td>[42413.0, 38471.0, 55102.0, 41647.0]</td>\n",
       "      <td>10450448.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132536</td>\n",
       "      <td>[72, 145, 105, 1073]</td>\n",
       "      <td>[42413.0, 38471.0, 55102.0, 53840.0]</td>\n",
       "      <td>10436446.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>102668</td>\n",
       "      <td>[105, 145, 72, 140]</td>\n",
       "      <td>[55102.0, 38471.0, 42413.0, 38470.0]</td>\n",
       "      <td>10213421.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>98453</td>\n",
       "      <td>[72, 145, 105, 1678]</td>\n",
       "      <td>[42413.0, 38471.0, 55102.0, 29925.0]</td>\n",
       "      <td>10444305.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>96365</td>\n",
       "      <td>[72, 145, 105, 1073]</td>\n",
       "      <td>[42413.0, 38471.0, 55102.0, 53840.0]</td>\n",
       "      <td>10125017.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>140302</td>\n",
       "      <td>[72, 145, 105, 1503]</td>\n",
       "      <td>[42413.0, 38471.0, 55102.0, 41647.0]</td>\n",
       "      <td>10465791.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>1489</td>\n",
       "      <td>[72, 105, 145, 323]</td>\n",
       "      <td>[42413.0, 55102.0, 38471.0, 49625.0]</td>\n",
       "      <td>10294345.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2469 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      customers_id    predicted_articles  \\\n",
       "0             9806   [72, 145, 105, 249]   \n",
       "1           126374  [72, 145, 105, 1503]   \n",
       "2            12345  [72, 145, 105, 1503]   \n",
       "3           126357  [72, 145, 105, 1503]   \n",
       "4           132536  [72, 145, 105, 1073]   \n",
       "...            ...                   ...   \n",
       "2464        102668   [105, 145, 72, 140]   \n",
       "2465         98453  [72, 145, 105, 1678]   \n",
       "2466         96365  [72, 145, 105, 1073]   \n",
       "2467        140302  [72, 145, 105, 1503]   \n",
       "2468          1489   [72, 105, 145, 323]   \n",
       "\n",
       "                        mapped_articles_id  mapped_customers_id  \n",
       "0     [42413.0, 38471.0, 55102.0, 35592.0]           10421548.0  \n",
       "1     [42413.0, 38471.0, 55102.0, 41647.0]           10377320.0  \n",
       "2     [42413.0, 38471.0, 55102.0, 41647.0]           10292410.0  \n",
       "3     [42413.0, 38471.0, 55102.0, 41647.0]           10450448.0  \n",
       "4     [42413.0, 38471.0, 55102.0, 53840.0]           10436446.0  \n",
       "...                                    ...                  ...  \n",
       "2464  [55102.0, 38471.0, 42413.0, 38470.0]           10213421.0  \n",
       "2465  [42413.0, 38471.0, 55102.0, 29925.0]           10444305.0  \n",
       "2466  [42413.0, 38471.0, 55102.0, 53840.0]           10125017.0  \n",
       "2467  [42413.0, 38471.0, 55102.0, 41647.0]           10465791.0  \n",
       "2468  [42413.0, 55102.0, 38471.0, 49625.0]           10294345.0  \n",
       "\n",
       "[2469 rows x 4 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values: 106\n"
     ]
    }
   ],
   "source": [
    "nan_count = results_df['mapped_customers_id'].isna().sum()\n",
    "print(f\"Number of NaN values: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.dropna(subset=['mapped_customers_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Table(df=\n",
       "      timestamp  customers_id                              articles_id\n",
       "0    2025-01-11          9806                     [981, 1491, 72, 105]\n",
       "1    2025-01-11        126374                 [2391, 3426, 6226, 1830]\n",
       "2    2025-01-11         12345  [6892, 1106, 840, 5991, 1780, 977, 145]\n",
       "3    2025-01-11        126357     [1949, 7190, 5265, 5849, 1290, 7344]\n",
       "4    2025-01-11        132536                                    [869]\n",
       "...         ...           ...                                      ...\n",
       "2464 2025-01-11        102668                                    [879]\n",
       "2465 2025-01-11         98453                                   [1235]\n",
       "2466 2025-01-11         96365                                   [1140]\n",
       "2467 2025-01-11        140302                                    [168]\n",
       "2468 2025-01-11          1489                                    [934]\n",
       "\n",
       "[2469 rows x 3 columns],\n",
       "  fkey_col_to_pkey_table={'customers_id': 'customer', 'articles_id': 'article'},\n",
       "  pkey_col=None,\n",
       "  time_col=timestamp)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data1 = task.get_table(\"test\", mask_input_cols=False)\n",
    "test_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_ids = test_data1.df[\"customers_id\"].values  # Extract the customer_id column\n",
    "predicted_labels = test_data1.df[\"articles_id\"].values  # This contains the model's predicted labels\n",
    "\n",
    "# Now zip customer_ids with the predictions\n",
    "results = list(zip(customer_ids, predicted_labels))\n",
    "\n",
    "# If you'd like to show or process the results, you can format them as a dataframe\n",
    "results_df1 = pd.DataFrame(results, columns=[\"customers_id\", \"groundtruth_articles\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming reverse_mapping_articles is already defined\n",
    "reverse_mapping_articles = {v: k for k, v in index_map_dict1['article'].items()}\n",
    "\n",
    "# Apply reverse mapping to each article in the list\n",
    "results_df1['mapped_articles_id'] = results_df1['groundtruth_articles'].apply(\n",
    "    lambda x: [reverse_mapping_articles.get(article, article) for article in x]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_mapping = {v: k for k, v in index_map_dict1['customer'].items()}\n",
    "\n",
    "# Map back to original customers_id\n",
    "results_df1['mapped_customers_id'] = results_df1['customers_id'].map(reverse_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customers_id</th>\n",
       "      <th>groundtruth_articles</th>\n",
       "      <th>mapped_articles_id</th>\n",
       "      <th>mapped_customers_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9806</td>\n",
       "      <td>[981, 1491, 72, 105]</td>\n",
       "      <td>[53407.0, 52402.0, 42413.0, 55102.0]</td>\n",
       "      <td>10421548.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126374</td>\n",
       "      <td>[2391, 3426, 6226, 1830]</td>\n",
       "      <td>[53183.0, 32590.0, 45394.0, 47141.0]</td>\n",
       "      <td>10377320.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12345</td>\n",
       "      <td>[6892, 1106, 840, 5991, 1780, 977, 145]</td>\n",
       "      <td>[16274.0, 59028.0, 52452.0, 30373.0, 11176.0, ...</td>\n",
       "      <td>10292410.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>126357</td>\n",
       "      <td>[1949, 7190, 5265, 5849, 1290, 7344]</td>\n",
       "      <td>[41781.0, 59161.0, 35600.0, 16144.0, 38860.0, ...</td>\n",
       "      <td>10450448.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132536</td>\n",
       "      <td>[869]</td>\n",
       "      <td>[53251.0]</td>\n",
       "      <td>10436446.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>102668</td>\n",
       "      <td>[879]</td>\n",
       "      <td>[52390.0]</td>\n",
       "      <td>10213421.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>98453</td>\n",
       "      <td>[1235]</td>\n",
       "      <td>[48204.0]</td>\n",
       "      <td>10444305.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>96365</td>\n",
       "      <td>[1140]</td>\n",
       "      <td>[55073.0]</td>\n",
       "      <td>10125017.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>140302</td>\n",
       "      <td>[168]</td>\n",
       "      <td>[32655.0]</td>\n",
       "      <td>10465791.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>1489</td>\n",
       "      <td>[934]</td>\n",
       "      <td>[44398.0]</td>\n",
       "      <td>10294345.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2469 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      customers_id                     groundtruth_articles  \\\n",
       "0             9806                     [981, 1491, 72, 105]   \n",
       "1           126374                 [2391, 3426, 6226, 1830]   \n",
       "2            12345  [6892, 1106, 840, 5991, 1780, 977, 145]   \n",
       "3           126357     [1949, 7190, 5265, 5849, 1290, 7344]   \n",
       "4           132536                                    [869]   \n",
       "...            ...                                      ...   \n",
       "2464        102668                                    [879]   \n",
       "2465         98453                                   [1235]   \n",
       "2466         96365                                   [1140]   \n",
       "2467        140302                                    [168]   \n",
       "2468          1489                                    [934]   \n",
       "\n",
       "                                     mapped_articles_id  mapped_customers_id  \n",
       "0                  [53407.0, 52402.0, 42413.0, 55102.0]           10421548.0  \n",
       "1                  [53183.0, 32590.0, 45394.0, 47141.0]           10377320.0  \n",
       "2     [16274.0, 59028.0, 52452.0, 30373.0, 11176.0, ...           10292410.0  \n",
       "3     [41781.0, 59161.0, 35600.0, 16144.0, 38860.0, ...           10450448.0  \n",
       "4                                             [53251.0]           10436446.0  \n",
       "...                                                 ...                  ...  \n",
       "2464                                          [52390.0]           10213421.0  \n",
       "2465                                          [48204.0]           10444305.0  \n",
       "2466                                          [55073.0]           10125017.0  \n",
       "2467                                          [32655.0]           10465791.0  \n",
       "2468                                          [44398.0]           10294345.0  \n",
       "\n",
       "[2469 rows x 4 columns]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df1 = results_df1.dropna(subset=['mapped_customers_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      customers_id_x    predicted_articles  \\\n",
      "0               9806   [72, 145, 105, 249]   \n",
      "1             126374  [72, 145, 105, 1503]   \n",
      "2              12345  [72, 145, 105, 1503]   \n",
      "3             126357  [72, 145, 105, 1503]   \n",
      "4             132536  [72, 145, 105, 1073]   \n",
      "...              ...                   ...   \n",
      "2358          102668   [105, 145, 72, 140]   \n",
      "2359           98453  [72, 145, 105, 1678]   \n",
      "2360           96365  [72, 145, 105, 1073]   \n",
      "2361          140302  [72, 145, 105, 1503]   \n",
      "2362            1489   [72, 105, 145, 323]   \n",
      "\n",
      "                      mapped_articles_id_x  mapped_customers_id  \\\n",
      "0     [42413.0, 38471.0, 55102.0, 35592.0]           10421548.0   \n",
      "1     [42413.0, 38471.0, 55102.0, 41647.0]           10377320.0   \n",
      "2     [42413.0, 38471.0, 55102.0, 41647.0]           10292410.0   \n",
      "3     [42413.0, 38471.0, 55102.0, 41647.0]           10450448.0   \n",
      "4     [42413.0, 38471.0, 55102.0, 53840.0]           10436446.0   \n",
      "...                                    ...                  ...   \n",
      "2358  [55102.0, 38471.0, 42413.0, 38470.0]           10213421.0   \n",
      "2359  [42413.0, 38471.0, 55102.0, 29925.0]           10444305.0   \n",
      "2360  [42413.0, 38471.0, 55102.0, 53840.0]           10125017.0   \n",
      "2361  [42413.0, 38471.0, 55102.0, 41647.0]           10465791.0   \n",
      "2362  [42413.0, 55102.0, 38471.0, 49625.0]           10294345.0   \n",
      "\n",
      "      customers_id_y                     groundtruth_articles  \\\n",
      "0               9806                     [981, 1491, 72, 105]   \n",
      "1             126374                 [2391, 3426, 6226, 1830]   \n",
      "2              12345  [6892, 1106, 840, 5991, 1780, 977, 145]   \n",
      "3             126357     [1949, 7190, 5265, 5849, 1290, 7344]   \n",
      "4             132536                                    [869]   \n",
      "...              ...                                      ...   \n",
      "2358          102668                                    [879]   \n",
      "2359           98453                                   [1235]   \n",
      "2360           96365                                   [1140]   \n",
      "2361          140302                                    [168]   \n",
      "2362            1489                                    [934]   \n",
      "\n",
      "                                   mapped_articles_id_y  \n",
      "0                  [53407.0, 52402.0, 42413.0, 55102.0]  \n",
      "1                  [53183.0, 32590.0, 45394.0, 47141.0]  \n",
      "2     [16274.0, 59028.0, 52452.0, 30373.0, 11176.0, ...  \n",
      "3     [41781.0, 59161.0, 35600.0, 16144.0, 38860.0, ...  \n",
      "4                                             [53251.0]  \n",
      "...                                                 ...  \n",
      "2358                                          [52390.0]  \n",
      "2359                                          [48204.0]  \n",
      "2360                                          [55073.0]  \n",
      "2361                                          [32655.0]  \n",
      "2362                                          [44398.0]  \n",
      "\n",
      "[2363 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "merged_df = pd.merge(results_df, results_df1, on='mapped_customers_id', how='inner')\n",
    "\n",
    "# Print the result\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customers_id_x</th>\n",
       "      <th>predicted_articles</th>\n",
       "      <th>mapped_articles_id_x</th>\n",
       "      <th>mapped_customers_id</th>\n",
       "      <th>customers_id_y</th>\n",
       "      <th>groundtruth_articles</th>\n",
       "      <th>mapped_articles_id_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9806</td>\n",
       "      <td>[72, 145, 105, 249]</td>\n",
       "      <td>[42413.0, 38471.0, 55102.0, 35592.0]</td>\n",
       "      <td>10421548.0</td>\n",
       "      <td>9806</td>\n",
       "      <td>[981, 1491, 72, 105]</td>\n",
       "      <td>[53407.0, 52402.0, 42413.0, 55102.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126374</td>\n",
       "      <td>[72, 145, 105, 1503]</td>\n",
       "      <td>[42413.0, 38471.0, 55102.0, 41647.0]</td>\n",
       "      <td>10377320.0</td>\n",
       "      <td>126374</td>\n",
       "      <td>[2391, 3426, 6226, 1830]</td>\n",
       "      <td>[53183.0, 32590.0, 45394.0, 47141.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12345</td>\n",
       "      <td>[72, 145, 105, 1503]</td>\n",
       "      <td>[42413.0, 38471.0, 55102.0, 41647.0]</td>\n",
       "      <td>10292410.0</td>\n",
       "      <td>12345</td>\n",
       "      <td>[6892, 1106, 840, 5991, 1780, 977, 145]</td>\n",
       "      <td>[16274.0, 59028.0, 52452.0, 30373.0, 11176.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>126357</td>\n",
       "      <td>[72, 145, 105, 1503]</td>\n",
       "      <td>[42413.0, 38471.0, 55102.0, 41647.0]</td>\n",
       "      <td>10450448.0</td>\n",
       "      <td>126357</td>\n",
       "      <td>[1949, 7190, 5265, 5849, 1290, 7344]</td>\n",
       "      <td>[41781.0, 59161.0, 35600.0, 16144.0, 38860.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132536</td>\n",
       "      <td>[72, 145, 105, 1073]</td>\n",
       "      <td>[42413.0, 38471.0, 55102.0, 53840.0]</td>\n",
       "      <td>10436446.0</td>\n",
       "      <td>132536</td>\n",
       "      <td>[869]</td>\n",
       "      <td>[53251.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358</th>\n",
       "      <td>102668</td>\n",
       "      <td>[105, 145, 72, 140]</td>\n",
       "      <td>[55102.0, 38471.0, 42413.0, 38470.0]</td>\n",
       "      <td>10213421.0</td>\n",
       "      <td>102668</td>\n",
       "      <td>[879]</td>\n",
       "      <td>[52390.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2359</th>\n",
       "      <td>98453</td>\n",
       "      <td>[72, 145, 105, 1678]</td>\n",
       "      <td>[42413.0, 38471.0, 55102.0, 29925.0]</td>\n",
       "      <td>10444305.0</td>\n",
       "      <td>98453</td>\n",
       "      <td>[1235]</td>\n",
       "      <td>[48204.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2360</th>\n",
       "      <td>96365</td>\n",
       "      <td>[72, 145, 105, 1073]</td>\n",
       "      <td>[42413.0, 38471.0, 55102.0, 53840.0]</td>\n",
       "      <td>10125017.0</td>\n",
       "      <td>96365</td>\n",
       "      <td>[1140]</td>\n",
       "      <td>[55073.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2361</th>\n",
       "      <td>140302</td>\n",
       "      <td>[72, 145, 105, 1503]</td>\n",
       "      <td>[42413.0, 38471.0, 55102.0, 41647.0]</td>\n",
       "      <td>10465791.0</td>\n",
       "      <td>140302</td>\n",
       "      <td>[168]</td>\n",
       "      <td>[32655.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2362</th>\n",
       "      <td>1489</td>\n",
       "      <td>[72, 105, 145, 323]</td>\n",
       "      <td>[42413.0, 55102.0, 38471.0, 49625.0]</td>\n",
       "      <td>10294345.0</td>\n",
       "      <td>1489</td>\n",
       "      <td>[934]</td>\n",
       "      <td>[44398.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2363 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      customers_id_x    predicted_articles  \\\n",
       "0               9806   [72, 145, 105, 249]   \n",
       "1             126374  [72, 145, 105, 1503]   \n",
       "2              12345  [72, 145, 105, 1503]   \n",
       "3             126357  [72, 145, 105, 1503]   \n",
       "4             132536  [72, 145, 105, 1073]   \n",
       "...              ...                   ...   \n",
       "2358          102668   [105, 145, 72, 140]   \n",
       "2359           98453  [72, 145, 105, 1678]   \n",
       "2360           96365  [72, 145, 105, 1073]   \n",
       "2361          140302  [72, 145, 105, 1503]   \n",
       "2362            1489   [72, 105, 145, 323]   \n",
       "\n",
       "                      mapped_articles_id_x  mapped_customers_id  \\\n",
       "0     [42413.0, 38471.0, 55102.0, 35592.0]           10421548.0   \n",
       "1     [42413.0, 38471.0, 55102.0, 41647.0]           10377320.0   \n",
       "2     [42413.0, 38471.0, 55102.0, 41647.0]           10292410.0   \n",
       "3     [42413.0, 38471.0, 55102.0, 41647.0]           10450448.0   \n",
       "4     [42413.0, 38471.0, 55102.0, 53840.0]           10436446.0   \n",
       "...                                    ...                  ...   \n",
       "2358  [55102.0, 38471.0, 42413.0, 38470.0]           10213421.0   \n",
       "2359  [42413.0, 38471.0, 55102.0, 29925.0]           10444305.0   \n",
       "2360  [42413.0, 38471.0, 55102.0, 53840.0]           10125017.0   \n",
       "2361  [42413.0, 38471.0, 55102.0, 41647.0]           10465791.0   \n",
       "2362  [42413.0, 55102.0, 38471.0, 49625.0]           10294345.0   \n",
       "\n",
       "      customers_id_y                     groundtruth_articles  \\\n",
       "0               9806                     [981, 1491, 72, 105]   \n",
       "1             126374                 [2391, 3426, 6226, 1830]   \n",
       "2              12345  [6892, 1106, 840, 5991, 1780, 977, 145]   \n",
       "3             126357     [1949, 7190, 5265, 5849, 1290, 7344]   \n",
       "4             132536                                    [869]   \n",
       "...              ...                                      ...   \n",
       "2358          102668                                    [879]   \n",
       "2359           98453                                   [1235]   \n",
       "2360           96365                                   [1140]   \n",
       "2361          140302                                    [168]   \n",
       "2362            1489                                    [934]   \n",
       "\n",
       "                                   mapped_articles_id_y  \n",
       "0                  [53407.0, 52402.0, 42413.0, 55102.0]  \n",
       "1                  [53183.0, 32590.0, 45394.0, 47141.0]  \n",
       "2     [16274.0, 59028.0, 52452.0, 30373.0, 11176.0, ...  \n",
       "3     [41781.0, 59161.0, 35600.0, 16144.0, 38860.0, ...  \n",
       "4                                             [53251.0]  \n",
       "...                                                 ...  \n",
       "2358                                          [52390.0]  \n",
       "2359                                          [48204.0]  \n",
       "2360                                          [55073.0]  \n",
       "2361                                          [32655.0]  \n",
       "2362                                          [44398.0]  \n",
       "\n",
       "[2363 rows x 7 columns]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\KN2C\\AppData\\Local\\Temp\\ipykernel_8524\\2395165695.py:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  merged_df.to_csv('C:\\\\Users\\\\KN2C\\Desktop\\\\Dani\\\\contextgnn\\\\result_recom2.csv', index=False)\n"
     ]
    }
   ],
   "source": [
    "merged_df.to_csv('C:\\\\Users\\\\KN2C\\Desktop\\\\Dani\\\\contextgnn\\\\result_recom2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 2000/2000 [4:26:32<00:00,  8.00s/it]  \n",
      "Val: 100%|██████████| 59/59 [10:40<00:00, 10.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train loss: 72.34259055215553, Val metrics: {'link_prediction_precision': np.float64(0.11947518388443443), 'link_prediction_recall': np.float64(0.08714295934837625), 'link_prediction_map': np.float64(0.10646246106951163), 'link_prediction_top': np.float64(0.3480219998674707)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 2000/2000 [4:21:52<00:00,  7.86s/it]  \n",
      "Val: 100%|██████████| 59/59 [10:39<00:00, 10.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02, Train loss: 69.44414806568167, Val metrics: {'link_prediction_precision': np.float64(0.0778444105758399), 'link_prediction_recall': np.float64(0.059326035200976276), 'link_prediction_map': np.float64(0.060867772550232284), 'link_prediction_top': np.float64(0.25134185938638925)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 2000/2000 [4:20:35<00:00,  7.82s/it]  \n",
      "Val: 100%|██████████| 59/59 [10:41<00:00, 10.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03, Train loss: 68.62711653797692, Val metrics: {'link_prediction_precision': np.float64(0.04976476045325028), 'link_prediction_recall': np.float64(0.03974686011645201), 'link_prediction_map': np.float64(0.03744229452433018), 'link_prediction_top': np.float64(0.167517063150222)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 2000/2000 [4:21:12<00:00,  7.84s/it]  \n",
      "Val: 100%|██████████| 59/59 [10:42<00:00, 10.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04, Train loss: 68.28201239375534, Val metrics: {'link_prediction_precision': np.float64(0.04057053873169439), 'link_prediction_recall': np.float64(0.033005813262233834), 'link_prediction_map': np.float64(0.030213464242852622), 'link_prediction_top': np.float64(0.13809555364124312)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 2000/2000 [4:20:12<00:00,  7.81s/it]  \n",
      "Val: 100%|██████████| 59/59 [10:39<00:00, 10.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Train loss: 67.99987211956321, Val metrics: {'link_prediction_precision': np.float64(0.03801934928102843), 'link_prediction_recall': np.float64(0.03166580717627368), 'link_prediction_map': np.float64(0.029117336307880336), 'link_prediction_top': np.float64(0.13074017626399842)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  35%|███▌      | 702/2000 [1:31:24<2:49:00,  7.81s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m best_val_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m eval_epochs_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      8\u001b[0m         val_pred \u001b[38;5;241m=\u001b[39m test(loader_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m], desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[35], line 39\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m     loss \u001b[38;5;241m=\u001b[39m sparse_cross_entropy(logits, edge_label_index)\n\u001b[0;32m     37\u001b[0m     numel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[task\u001b[38;5;241m.\u001b[39mdst_entity_table]\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[1;32m---> 39\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     42\u001b[0m loss_accum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(loss) \u001b[38;5;241m*\u001b[39m numel\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training and evaluation loop\n",
    "state_dict = None\n",
    "best_val_metric = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train()\n",
    "    \n",
    "    if epoch % eval_epochs_interval == 0:\n",
    "        val_pred = test(loader_dict[\"val\"], desc=\"Val\")\n",
    "        val_metrics = task.evaluate(val_pred, task.get_table(\"val\"))\n",
    "        print(f\"Epoch: {epoch:02d}, Train loss: {train_loss}, Val metrics: {val_metrics}\")\n",
    "\n",
    "        if val_metrics[tune_metric] > best_val_metric:\n",
    "            best_val_metric = val_metrics[tune_metric]\n",
    "            state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "# Load the best model and evaluate on validation and test sets\n",
    "assert state_dict is not None\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "val_pred = test(loader_dict[\"val\"], desc=\"Best val\")\n",
    "val_metrics = task.evaluate(val_pred, task.get_table(\"val\"))\n",
    "print(f\"Best val metrics: {val_metrics}\")\n",
    "\n",
    "test_pred = test(loader_dict[\"test\"], desc=\"Test\")\n",
    "test_metrics = task.evaluate(test_pred)\n",
    "print(f\"Best test metrics: {test_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from relbench.base import Dataset, RecommendationTask, TaskType\n",
    "from relbench.datasets import get_dataset\n",
    "from relbench.modeling.graph import (\n",
    "    get_link_train_table_input,\n",
    "    make_pkey_fkey_graph,\n",
    ")\n",
    "from relbench.modeling.loader import SparseTensor\n",
    "from relbench.modeling.utils import get_stype_proposal\n",
    "from relbench.tasks import get_task\n",
    "from torch_frame import stype\n",
    "from torch_frame.config.text_embedder import TextEmbedderConfig\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.seed import seed_everything\n",
    "from torch_geometric.typing import NodeType\n",
    "from torch_geometric.utils.cross_entropy import sparse_cross_entropy\n",
    "from tqdm import tqdm\n",
    "from contextgnn.nn.models import ContextGNN\n",
    "from contextgnn.utils import GloveTextEmbedding, RHSEmbeddingMode\n",
    "\n",
    "# Static Configuration\n",
    "# dataset_name = \"rel-hm\"\n",
    "# task_name = \"user-item-purchase\"\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "eval_epochs_interval = 1\n",
    "batch_size = 64\n",
    "channels = 128\n",
    "aggregation_method = \"sum\"\n",
    "num_layers = 4\n",
    "num_neighbors = 128\n",
    "rhs_sample_size = 1000  # Use -1 for sampling all RHS\n",
    "temporal_strategy = \"last\"\n",
    "max_steps_per_epoch = 200\n",
    "num_workers = 0\n",
    "seed = 42\n",
    "cache_dir = os.path.expanduser(\"D:/Dani/relbench/relbench/.cache/relbench_examples\")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_num_threads(1 if torch.cuda.is_available() else os.cpu_count())\n",
    "torch.manual_seed(seed)\n",
    "seed_everything(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load dataset and task\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataset: Dataset \u001b[38;5;241m=\u001b[39m get_dataset(\u001b[43mdataset_name\u001b[49m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m task: RecommendationTask \u001b[38;5;241m=\u001b[39m get_task(dataset_name, task_name, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Ensure task type is LINK_PREDICTION\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset_name' is not defined"
     ]
    }
   ],
   "source": [
    "# Load dataset and task\n",
    "dataset: Dataset = get_dataset(dataset_name, download=True)\n",
    "task: RecommendationTask = get_task(dataset_name, task_name, download=True)\n",
    "\n",
    "# Ensure task type is LINK_PREDICTION\n",
    "assert task.task_type == TaskType.LINK_PREDICTION\n",
    "\n",
    "# Tune metric\n",
    "tune_metric = \"link_prediction_map\"\n",
    "\n",
    "# Handle column type mappings\n",
    "stypes_cache_path = Path(f\"{cache_dir}/{dataset_name}/stypes.json\")\n",
    "try:\n",
    "    with open(stypes_cache_path, \"r\") as f:\n",
    "        col_to_stype_dict = json.load(f)\n",
    "    for table, col_to_stype in col_to_stype_dict.items():\n",
    "        for col, stype_str in col_to_stype.items():\n",
    "            col_to_stype[col] = stype(stype_str)\n",
    "except FileNotFoundError:\n",
    "    col_to_stype_dict = get_stype_proposal(dataset.get_db())\n",
    "    Path(stypes_cache_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(stypes_cache_path, \"w\") as f:\n",
    "        json.dump(col_to_stype_dict, f, indent=2, default=str)\n",
    "\n",
    "# Prepare graph data and column stats\n",
    "data, col_stats_dict = make_pkey_fkey_graph(\n",
    "    dataset.get_db(),\n",
    "    col_to_stype_dict=col_to_stype_dict,\n",
    "    text_embedder_cfg=TextEmbedderConfig(\n",
    "        text_embedder=GloveTextEmbedding(device=device), batch_size=256\n",
    "    ),\n",
    "    cache_dir=f\"{cache_dir}/{dataset_name}/materialized2\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of neighbors for NeighborLoader\n",
    "num_neighbors_list = [int(num_neighbors // 2**i) for i in range(num_layers)]\n",
    "\n",
    "# Loader dictionaries\n",
    "loader_dict: Dict[str, NeighborLoader] = {}\n",
    "dst_nodes_dict: Dict[str, Tuple[NodeType, Tensor]] = {}\n",
    "num_dst_nodes_dict: Dict[str, int] = {}\n",
    "\n",
    "# Initialize data loaders for train, val, and test splits\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    table = task.get_table(split)\n",
    "    table_input = get_link_train_table_input(table, task)\n",
    "    dst_nodes_dict[split] = table_input.dst_nodes\n",
    "    num_dst_nodes_dict[split] = table_input.num_dst_nodes\n",
    "    loader_dict[split] = NeighborLoader(\n",
    "        data,\n",
    "        num_neighbors=num_neighbors_list,\n",
    "        time_attr=\"time\",\n",
    "        input_nodes=table_input.src_nodes,\n",
    "        input_time=table_input.src_time,\n",
    "        subgraph_type=\"bidirectional\",\n",
    "        batch_size=batch_size,\n",
    "        temporal_strategy=temporal_strategy,\n",
    "        shuffle=split == \"train\",\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=num_workers > 0,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ContextGNN model\n",
    "model: ContextGNN = ContextGNN(\n",
    "    data=data,\n",
    "    col_stats_dict=col_stats_dict,\n",
    "    rhs_emb_mode=RHSEmbeddingMode.FUSION,\n",
    "    dst_entity_table=task.dst_entity_table,\n",
    "    num_nodes=num_dst_nodes_dict[\"train\"],\n",
    "    num_layers=num_layers,\n",
    "    channels=channels,\n",
    "    aggr=aggregation_method,\n",
    "    norm=\"layer_norm\",\n",
    "    embedding_dim=64,\n",
    "    torch_frame_model_kwargs={\"channels\": 128, \"num_layers\": 4},\n",
    "    rhs_sample_size=rhs_sample_size,\n",
    ").to(device)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train() -> float:\n",
    "    model.train()\n",
    "\n",
    "    loss_accum = count_accum = 0.0\n",
    "    steps = 0\n",
    "    total_steps = min(len(loader_dict[\"train\"]), max_steps_per_epoch)\n",
    "    sparse_tensor = SparseTensor(dst_nodes_dict[\"train\"][1], device=device)\n",
    "\n",
    "    for batch in tqdm(loader_dict[\"train\"], total=total_steps, desc=\"Train\"):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Get ground-truth\n",
    "        input_id = batch[task.src_entity_table].input_id\n",
    "        src_batch, dst_index = sparse_tensor[input_id]\n",
    "\n",
    "        # Optimization\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, lhs_y_batch, rhs_y_index = model.forward_sample_softmax(\n",
    "            batch, task.src_entity_table, task.dst_entity_table, src_batch, dst_index\n",
    "        )\n",
    "        edge_label_index = torch.stack([lhs_y_batch, rhs_y_index], dim=0)\n",
    "        loss = sparse_cross_entropy(logits, edge_label_index)\n",
    "\n",
    "        numel = len(batch[task.dst_entity_table].batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_accum += float(loss) * numel\n",
    "        count_accum += numel\n",
    "\n",
    "        steps += 1\n",
    "        if steps >= max_steps_per_epoch:\n",
    "            break\n",
    "\n",
    "    if count_accum == 0:\n",
    "        warnings.warn(f\"Did not sample a single '{task.dst_entity_table}' node in any mini-batch.\")\n",
    "\n",
    "    return loss_accum / count_accum if count_accum > 0 else float(\"nan\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(loader: NeighborLoader, desc: str) -> np.ndarray:\n",
    "    model.eval()\n",
    "\n",
    "    pred_list: List[Tensor] = []\n",
    "    for batch in tqdm(loader, desc=desc):\n",
    "        batch = batch.to(device)\n",
    "        out = model(batch, task.src_entity_table, task.dst_entity_table).detach()\n",
    "        scores = torch.sigmoid(out)\n",
    "        _, pred_mini = torch.topk(scores, k=task.eval_k, dim=1)\n",
    "        pred_list.append(pred_mini)\n",
    "    pred = torch.cat(pred_list, dim=0).cpu().numpy()\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|█████████▉| 199/200 [02:30<00:00,  1.32it/s]\n",
      "Val: 100%|██████████| 236/236 [06:25<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train loss: 72.70253252920064, Val metrics: {'link_prediction_precision': np.float64(0.09749188257902061), 'link_prediction_recall': np.float64(0.06706510539651768), 'link_prediction_map': np.float64(0.08634469404133442), 'link_prediction_top': np.float64(0.28182360347226826)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|█████████▉| 199/200 [04:43<00:01,  1.42s/it]\n",
      "Val: 100%|██████████| 236/236 [03:54<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02, Train loss: 59.54639587311882, Val metrics: {'link_prediction_precision': np.float64(0.00357829169703797), 'link_prediction_recall': np.float64(0.005774022671159914), 'link_prediction_map': np.float64(0.004895301835531112), 'link_prediction_top': np.float64(0.012524020939632894)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|█████████▉| 199/200 [06:20<00:01,  1.91s/it]\n",
      "Val: 100%|██████████| 236/236 [03:54<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03, Train loss: 57.7290588755016, Val metrics: {'link_prediction_precision': np.float64(0.016185143462991186), 'link_prediction_recall': np.float64(0.020551927445840258), 'link_prediction_map': np.float64(0.014822576370021868), 'link_prediction_top': np.float64(0.05552978596514479)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|█████████▉| 199/200 [05:53<00:01,  1.78s/it]\n",
      "Val: 100%|██████████| 236/236 [03:47<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04, Train loss: 57.93324835977943, Val metrics: {'link_prediction_precision': np.float64(0.02725134185938639), 'link_prediction_recall': np.float64(0.03162931530839053), 'link_prediction_map': np.float64(0.02378303109285151), 'link_prediction_top': np.float64(0.09310184878404347)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|█████████▉| 199/200 [06:00<00:01,  1.81s/it]\n",
      "Val: 100%|██████████| 236/236 [03:46<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Train loss: 57.70648078049331, Val metrics: {'link_prediction_precision': np.float64(0.012126432973295341), 'link_prediction_recall': np.float64(0.017434615671957702), 'link_prediction_map': np.float64(0.012312342161258732), 'link_prediction_top': np.float64(0.04247564773706183)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|█████████▉| 199/200 [05:34<00:01,  1.68s/it]\n",
      "Val: 100%|██████████| 236/236 [03:46<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06, Train loss: 58.250239113912855, Val metrics: {'link_prediction_precision': np.float64(0.010933669074282684), 'link_prediction_recall': np.float64(0.0175141116371128), 'link_prediction_map': np.float64(0.011881621864393052), 'link_prediction_top': np.float64(0.03889735604002385)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|█████████▉| 199/200 [05:41<00:01,  1.72s/it]\n",
      "Val: 100%|██████████| 236/236 [03:44<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07, Train loss: 57.74961981015315, Val metrics: {'link_prediction_precision': np.float64(0.009840302166854416), 'link_prediction_recall': np.float64(0.01664239200228313), 'link_prediction_map': np.float64(0.013073465420891038), 'link_prediction_top': np.float64(0.03618050493671725)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|█████████▉| 199/200 [05:01<00:01,  1.51s/it]\n",
      "Val: 100%|██████████| 236/236 [03:46<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08, Train loss: 57.77018339028004, Val metrics: {'link_prediction_precision': np.float64(0.07794380756742429), 'link_prediction_recall': np.float64(0.06456611555428791), 'link_prediction_map': np.float64(0.06902660894278413), 'link_prediction_top': np.float64(0.2522032999801206)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|█████████▉| 199/200 [05:12<00:01,  1.57s/it]\n",
      "Val: 100%|██████████| 236/236 [03:46<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09, Train loss: 57.02791413712222, Val metrics: {'link_prediction_precision': np.float64(0.08634285335630508), 'link_prediction_recall': np.float64(0.068243259848566), 'link_prediction_map': np.float64(0.08089902738203049), 'link_prediction_top': np.float64(0.26254058710489697)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|█████████▉| 199/200 [04:59<00:01,  1.51s/it]\n",
      "Val: 100%|██████████| 236/236 [03:44<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train loss: 57.83727987871257, Val metrics: {'link_prediction_precision': np.float64(0.08056126167914651), 'link_prediction_recall': np.float64(0.06190801027918744), 'link_prediction_map': np.float64(0.07615604223267732), 'link_prediction_top': np.float64(0.24299251209330064)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best val: 100%|██████████| 236/236 [03:44<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val metrics: {'link_prediction_precision': np.float64(0.08634285335630508), 'link_prediction_recall': np.float64(0.068243259848566), 'link_prediction_map': np.float64(0.08089902738203049), 'link_prediction_top': np.float64(0.26254058710489697)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 243/243 [03:27<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best test metrics: {'link_prediction_precision': np.float64(0.09036803500193025), 'link_prediction_recall': np.float64(0.07055906015060619), 'link_prediction_map': np.float64(0.0829043881096384), 'link_prediction_top': np.float64(0.27705571998455797)}\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables for tracking the best model and validation metrics\n",
    "state_dict = None\n",
    "best_val_metric = 0\n",
    "\n",
    "# Training and evaluation loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train()\n",
    "\n",
    "    if epoch % eval_epochs_interval == 0:\n",
    "        # Run validation\n",
    "        val_pred = test(loader_dict[\"val\"], desc=\"Val\")\n",
    "        val_metrics = task.evaluate(val_pred, task.get_table(\"val\"))\n",
    "        print(f\"Epoch: {epoch:02d}, Train loss: {train_loss}, Val metrics: {val_metrics}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_metrics[tune_metric] > best_val_metric:\n",
    "            best_val_metric = val_metrics[tune_metric]\n",
    "            state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "# Load the best model weights\n",
    "assert state_dict is not None\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Evaluate on validation and test sets\n",
    "val_pred = test(loader_dict[\"val\"], desc=\"Best val\")\n",
    "val_metrics = task.evaluate(val_pred, task.get_table(\"val\"))\n",
    "print(f\"Best val metrics: {val_metrics}\")\n",
    "\n",
    "test_pred = test(loader_dict[\"test\"], desc=\"Test\")\n",
    "test_metrics = task.evaluate(test_pred)\n",
    "print(f\"Best test metrics: {test_metrics}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/200 [00:19<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.43 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Training and evaluation loop\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 7\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m eval_epochs_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;66;03m# Run validation\u001b[39;00m\n\u001b[0;32m     11\u001b[0m         val_pred \u001b[38;5;241m=\u001b[39m test(loader_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m], desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[43], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Optimization\u001b[39;00m\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 19\u001b[0m logits, lhs_y_batch, rhs_y_index \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_sample_softmax\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msrc_entity_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdst_entity_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_index\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m edge_label_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([lhs_y_batch, rhs_y_index], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m sparse_cross_entropy(logits, edge_label_index)\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\Desktop\\Dani\\ContextGNN\\contextgnn\\nn\\models\\contextgnn.py:211\u001b[0m, in \u001b[0;36mContextGNN.forward_sample_softmax\u001b[1;34m(self, batch, entity_table, dst_table, src_batch, dst_index)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Forward function with RHS sample softmax.\"\"\"\u001b[39;00m\n\u001b[0;32m    210\u001b[0m seed_time \u001b[38;5;241m=\u001b[39m batch[entity_table]\u001b[38;5;241m.\u001b[39mseed_time\n\u001b[1;32m--> 211\u001b[0m x_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_gnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m seed_time\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    214\u001b[0m lhs_embedding \u001b[38;5;241m=\u001b[39m x_dict[entity_table][:\n\u001b[0;32m    215\u001b[0m                                      batch_size]  \u001b[38;5;66;03m# batch_size, channel\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\Desktop\\Dani\\ContextGNN\\contextgnn\\nn\\models\\contextgnn.py:160\u001b[0m, in \u001b[0;36mContextGNN.forward_gnn\u001b[1;34m(self, batch, entity_table)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_gnn\u001b[39m(\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    156\u001b[0m     batch: HeteroData,\n\u001b[0;32m    157\u001b[0m     entity_table: NodeType,\n\u001b[0;32m    158\u001b[0m ):\n\u001b[0;32m    159\u001b[0m     seed_time \u001b[38;5;241m=\u001b[39m batch[entity_table]\u001b[38;5;241m.\u001b[39mseed_time\n\u001b[1;32m--> 160\u001b[0m     x_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtf_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# Add ID-awareness to the root node\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     x_dict[entity_table][:seed_time\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    164\u001b[0m                                          )] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid_awareness_emb\u001b[38;5;241m.\u001b[39mweight\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\Desktop\\Dani\\ContextGNN\\contextgnn\\nn\\encoder.py:88\u001b[0m, in \u001b[0;36mHeteroEncoder.forward\u001b[1;34m(self, tf_dict)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     85\u001b[0m     tf_dict: Dict[NodeType, torch_frame\u001b[38;5;241m.\u001b[39mTensorFrame],\n\u001b[0;32m     86\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[NodeType, Tensor]:\n\u001b[0;32m     87\u001b[0m     x_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 88\u001b[0m         node_type: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m node_type, tf \u001b[38;5;129;01min\u001b[39;00m tf_dict\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     90\u001b[0m     }\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x_dict\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch_frame\\nn\\models\\resnet.py:197\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, tf)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tf: TensorFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    189\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Transforming :class:`TensorFrame` object into output prediction.\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m        torch.Tensor: Output of shape [batch_size, out_channels].\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m     x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;66;03m# Flattening the encoder output\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), math\u001b[38;5;241m.\u001b[39mprod(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]))\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch_frame\\nn\\encoder\\stypewise_encoder.py:84\u001b[0m, in \u001b[0;36mStypeWiseFeatureEncoder.forward\u001b[1;34m(self, tf)\u001b[0m\n\u001b[0;32m     82\u001b[0m feat \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfeat_dict[stype]\n\u001b[0;32m     83\u001b[0m col_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol_names_dict[stype]\n\u001b[1;32m---> 84\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m xs\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[0;32m     86\u001b[0m all_col_names\u001b[38;5;241m.\u001b[39mextend(col_names)\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch_frame\\nn\\base.py:83\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate()\n\u001b[1;32m---> 83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch_frame\\nn\\encoder\\stype_encoder.py:140\u001b[0m, in \u001b[0;36mStypeEncoder.forward\u001b[1;34m(self, feat, col_names)\u001b[0m\n\u001b[0;32m    138\u001b[0m feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mna_forward(feat)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# Main encoding into column embeddings\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Handle NaN in case na_strategy is None\u001b[39;00m\n\u001b[0;32m    142\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnan_to_num(x, nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch_frame\\nn\\encoder\\stype_encoder.py:446\u001b[0m, in \u001b[0;36mLinearEncoder.encode_forward\u001b[1;34m(self, feat, col_names)\u001b[0m\n\u001b[0;32m    443\u001b[0m x_lin \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mij,jk->ijk\u001b[39m\u001b[38;5;124m\"\u001b[39m, feat, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m    444\u001b[0m \u001b[38;5;66;03m# [batch_size, num_cols, channels] + [num_cols, channels]\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# -> [batch_size, num_cols, channels]\u001b[39;00m\n\u001b[1;32m--> 446\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx_lin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.43 GiB. GPU "
     ]
    }
   ],
   "source": [
    "# Initialize variables for tracking the best model and validation metrics\n",
    "state_dict = None\n",
    "best_val_metric = 0\n",
    "\n",
    "# Training and evaluation loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train()\n",
    "\n",
    "    if epoch % eval_epochs_interval == 0:\n",
    "        # Run validation\n",
    "        val_pred = test(loader_dict[\"val\"], desc=\"Val\")\n",
    "        val_metrics = task.evaluate(val_pred, task.get_table(\"val\"))\n",
    "        print(f\"Epoch: {epoch:02d}, Train loss: {train_loss}, Val metrics: {val_metrics}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_metrics[tune_metric] > best_val_metric:\n",
    "            best_val_metric = val_metrics[tune_metric]\n",
    "            state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "# Load the best model weights\n",
    "assert state_dict is not None\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Evaluate on validation and test sets\n",
    "val_pred = test(loader_dict[\"val\"], desc=\"Best val\")\n",
    "val_metrics = task.evaluate(val_pred, task.get_table(\"val\"))\n",
    "print(f\"Best val metrics: {val_metrics}\")\n",
    "\n",
    "test_pred = test(loader_dict[\"test\"], desc=\"Test\")\n",
    "test_metrics = task.evaluate(test_pred)\n",
    "print(f\"Best test metrics: {test_metrics}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kamal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
