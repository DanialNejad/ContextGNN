{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relbench.datasets import get_dataset, get_dataset_names, register_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from relbench.base import Database, Dataset, Table\n",
    "\n",
    "class TransactionalDataset(Dataset):\n",
    "    # Set timestamps or other relevant information if needed\n",
    "    val_timestamp = pd.Timestamp(\"2022-02-15\")\n",
    "    test_timestamp = pd.Timestamp(\"2022-02-22\")\n",
    "\n",
    "    def make_db(self) -> Database:\n",
    "        # Path to your CSVs folder\n",
    "        path = os.path.join(\"C:/Users/KN2C/Desktop/Dani/relbench/relbench/\", \"hyper_data\")\n",
    "        customers = os.path.join(path, \"Customers.csv\")\n",
    "        articles = os.path.join(path, \"Articles.csv\")\n",
    "        branches = os.path.join(path, \"Branches.csv\")\n",
    "        transactions = os.path.join(path, \"Transactions.csv\")\n",
    "\n",
    "        # Ensure that CSV files exist in the specified path\n",
    "        if not os.path.exists(customers):\n",
    "            raise RuntimeError(f\"Dataset not found at '{path}'. Please make sure the CSV files are in the correct folder.\")\n",
    "\n",
    "        # Read the CSV data into pandas DataFrames\n",
    "        customers_df = pd.read_csv(customers)\n",
    "        articles_df = pd.read_csv(articles)\n",
    "        branches_df = pd.read_csv(branches)\n",
    "        transactions_df = pd.read_csv(transactions)\n",
    "\n",
    "        ################################################################################\n",
    "        # Check for and handle duplicate primary keys in articles, customers, and branches tables\n",
    "        ################################################################################\n",
    "\n",
    "        # Handle duplicates in the articles table\n",
    "        if articles_df.duplicated(subset=['articles_id']).any():\n",
    "            print(\"Duplicates found in the 'articles_id' column. Removing duplicates...\")\n",
    "            articles_df = articles_df.drop_duplicates(subset=['articles_id'], keep='first')\n",
    "\n",
    "        # Handle duplicates in the customers table\n",
    "        if customers_df.duplicated(subset=['customers_id']).any():\n",
    "            print(\"Duplicates found in the 'customers_id' column. Removing duplicates...\")\n",
    "            customers_df = customers_df.drop_duplicates(subset=['customers_id'], keep='first')\n",
    "\n",
    "        # Handle duplicates in the branches table\n",
    "        if branches_df.duplicated(subset=['BranchCode']).any():\n",
    "            print(\"Duplicates found in the 'BranchCode' column. Removing duplicates...\")\n",
    "            branches_df = branches_df.drop_duplicates(subset=['BranchCode'], keep='first')\n",
    "\n",
    "        ################################################################################\n",
    "        # Clean and process the data (drop unnecessary columns, handle missing data)\n",
    "        ################################################################################\n",
    "        # Drop unnecessary columns\n",
    "        transactions_df.drop(columns=[\"Return Amount\"], inplace=True)\n",
    "        articles_df.drop(columns=[\"Item Barcode\", \"External Item Number\"], inplace=True)\n",
    "\n",
    "        # Replace any missing or invalid values\n",
    "        transactions_df[\"salesTime\"] = transactions_df[\"salesTime\"].replace(r\"^\\\\N$\", \"00:00:00\", regex=True)\n",
    "        transactions_df = transactions_df.replace(r\"^\\\\N$\", np.nan, regex=True)\n",
    "\n",
    "        # Combine date and time into a single 'datetime' column\n",
    "        # transactions_df['datetime'] = pd.to_datetime(transactions_df['d_dat'] + ' ' + transactions_df['salesTime'])\n",
    "        # transactions_df.drop(columns=[\"d_dat\"], inplace=True)        \n",
    "        # Convert date column to pd.Timestamp\n",
    "        # transactions_df[\"datetime\"] = pd.to_datetime(transactions_df[\"datetime\"])\n",
    "\n",
    "        transactions_df[\"datetime\"] = pd.to_datetime(\n",
    "        transactions_df[\"d_dat\"], format=\"%Y-%m-%d\"\n",
    "        )\n",
    "        transactions_df.drop(columns=[\"d_dat\"], inplace=True)          \n",
    "        # Convert other fields if necessary\n",
    "        transactions_df['price_purchase'] = pd.to_numeric(transactions_df['price_purchase'], errors='coerce')\n",
    "        transactions_df['Discount_ratio'] = pd.to_numeric(transactions_df['Discount_ratio'], errors='coerce')\n",
    "        transactions_df['Quantity'] = pd.to_numeric(transactions_df['Quantity'], errors='coerce')\n",
    "\n",
    "        ################################################################################\n",
    "        # Now we define the table structure and relationships.\n",
    "        ################################################################################\n",
    "\n",
    "        tables = {}\n",
    "\n",
    "        # Articles table\n",
    "        tables[\"article\"] = Table(\n",
    "            df=pd.DataFrame(articles_df),\n",
    "            fkey_col_to_pkey_table={},\n",
    "            pkey_col=\"articles_id\",\n",
    "            time_col=None,\n",
    "        )\n",
    "\n",
    "        # Customers table\n",
    "        tables[\"customer\"] = Table(\n",
    "            df=pd.DataFrame(customers_df),\n",
    "            fkey_col_to_pkey_table={},\n",
    "            pkey_col=\"customers_id\",\n",
    "            time_col=None,\n",
    "        )\n",
    "\n",
    "        # Branches table (renamed from \"branche\" to \"branches\")\n",
    "        tables[\"branches\"] = Table(\n",
    "            df=pd.DataFrame(branches_df),\n",
    "            fkey_col_to_pkey_table={},\n",
    "            pkey_col=\"BranchCode\",\n",
    "            time_col=None,\n",
    "        )\n",
    "\n",
    "        # Transactions table\n",
    "        tables[\"transactions\"] = Table(\n",
    "            df=pd.DataFrame(transactions_df),\n",
    "            fkey_col_to_pkey_table={\n",
    "                \"articles_id\": \"article\",    # Foreign key to articles\n",
    "                \"customers_id\": \"customer\",  # Foreign key to customers\n",
    "                \"BranchCode\": \"branches\",    # Foreign key to branches\n",
    "            },\n",
    "            pkey_col=None,\n",
    "            time_col=\"datetime\",  # Use the combined datetime column for time-based operations\n",
    "        )\n",
    "\n",
    "        return Database(tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates found in the 'articles_id' column. Removing duplicates...\n",
      "Duplicates found in the 'customers_id' column. Removing duplicates...\n"
     ]
    }
   ],
   "source": [
    "transactional_dataset = TransactionalDataset()\n",
    "db = transactional_dataset.make_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = db.table_dict[\"transactions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Table(df=\n",
       "         factor_id   articles_id  customers_id  BranchCode salesTime  \\\n",
       "0                0  3.515792e+07    9150651948           9  14:35:06   \n",
       "1                0  2.172070e+08    9150651948           9  14:35:06   \n",
       "2                0  9.892272e+08    9150651948           9  14:35:06   \n",
       "3                0  2.011346e+06    9150651948           9  14:35:06   \n",
       "4                0  2.006610e+06    9150651948           9  14:35:06   \n",
       "...            ...           ...           ...         ...       ...   \n",
       "6950117    1273663  3.214409e+07    9157962188         356  11:56:45   \n",
       "6950118    1273664  1.234744e+09    9153858979         356  18:53:39   \n",
       "6950119    1273665  2.013925e+06    9123311767         275  10:29:11   \n",
       "6950120    1273666  1.682033e+08    9393251711           4  09:16:39   \n",
       "6950121    1273667  1.234744e+09    9151875866         356  19:16:49   \n",
       "\n",
       "         price_purchase  Discount_ratio  Quantity   datetime  \n",
       "0               19610.1        0.266504     1.000 2021-03-22  \n",
       "1               13500.0        0.100000     1.000 2021-03-22  \n",
       "2               15820.8        0.000000     0.865 2021-03-22  \n",
       "3                8356.0        0.086576     1.000 2021-03-22  \n",
       "4                9200.0        0.086577     1.000 2021-03-22  \n",
       "...                 ...             ...       ...        ...  \n",
       "6950117        123600.0        0.000000     6.180 2022-03-12  \n",
       "6950118         15000.0        0.000000     3.000 2022-03-12  \n",
       "6950119         64822.0        0.043444    10.000 2022-03-12  \n",
       "6950120        223080.0        0.000000     1.950 2022-03-12  \n",
       "6950121          8500.0        0.000000     5.000 2022-03-12  \n",
       "\n",
       "[6950122 rows x 9 columns],\n",
       "  fkey_col_to_pkey_table={'articles_id': 'article', 'customers_id': 'customer', 'BranchCode': 'branches'},\n",
       "  pkey_col=None,\n",
       "  time_col=datetime)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.table_dict[\"transactions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Table(df=\n",
       "   BranchCode             BranchName\n",
       "0           9   فروشگاه شاندیز(مشهد)\n",
       "1         275   فروشگاه سمنان(ققنوس)\n",
       "2         509  فروشگاه یزد(امام علی)\n",
       "3           4     فروشگاه گنبد کاووس\n",
       "4         356         فروشگاه بجنورد,\n",
       "  fkey_col_to_pkey_table={},\n",
       "  pkey_col=BranchCode,\n",
       "  time_col=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.table_dict[\"branches\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Table(df=\n",
       "        articles_id                                           art_name  \\\n",
       "0      3.515792e+07                              اصالت آبلیمو 900 گرمی   \n",
       "1      2.172070e+08                  رامک پنیر سفید پروبیوتیک 400 گرمی   \n",
       "2      9.892272e+08                                   تخم مرغ طلقی فله   \n",
       "3      2.011346e+06                      سس گلوریا 88 گرم فلفل زرد تند   \n",
       "4      2.006610e+06                   فانتا نوشابه لیمویی1500 سی سی پت   \n",
       "...             ...                                                ...   \n",
       "44876  1.073731e+09              سحرکمپوت زردالو قوطی ایزی اپن430 گرمی   \n",
       "44877  1.083732e+09                        رزگلد سینی تخت گلدار سایز 1   \n",
       "44894  1.063731e+09                                فامیلی کایل اصلی A5   \n",
       "44899  1.091735e+09  سام آرشیت مانابلوز بچه گانه مد کد 950 بغل مشکی...   \n",
       "44900  1.061730e+09                            فامیلی کیف دسته قهوه ای   \n",
       "\n",
       "                           Department Name                   group  \\\n",
       "0                   FMCG - کالای بسته بندی           سرکه و آبلیمو   \n",
       "1                   FMCG - کالای بسته بندی              انواع پنیر   \n",
       "2                       Fresh - تازه فروشی          تخم ماکیان فله   \n",
       "3                   FMCG - کالای بسته بندی                انواع سس   \n",
       "4                   FMCG - کالای بسته بندی            انواع نوشابه   \n",
       "...                                    ...                     ...   \n",
       "44876               FMCG - کالای بسته بندی          محصولات صبحانه   \n",
       "44877                      LHH - لوازم سبک  لوازم آشپزخانه و مصرفی   \n",
       "44894                      LHH - لوازم سبک                  موبایل   \n",
       "44899  Textile - پوشاک، منسوجات، کیف و کفش  پوشاک بچه گانه و نوزاد   \n",
       "44900                      LHH - لوازم سبک          اکسسوری مردانه   \n",
       "\n",
       "                                 Subgroup Name  group_id  \\\n",
       "0                                       آبلیمو        86   \n",
       "1                              انواع پنیر سفید        18   \n",
       "2                            انواع تخم مرغ فله        33   \n",
       "3                                      سس فلفل        10   \n",
       "4             انواع نوشابه پت بزرگ سایر طعم ها        11   \n",
       "...                                        ...       ...   \n",
       "44876  انواع مربای سایر طعم ها کمتر از 500 گرم        48   \n",
       "44877                                 پلاستیکی        54   \n",
       "44894                                کابل رابط       115   \n",
       "44899                  پیراهن بچه گانه و نوزاد       137   \n",
       "44900                            کمربند مردانه       142   \n",
       "\n",
       "                     whole_Branch_Name  \n",
       "0      Dry Food - مواد غذایی بسته بندی  \n",
       "1              OPSS - یخچالی و انجمادی  \n",
       "2                         پروتئینی فله  \n",
       "3      Dry Food - مواد غذایی بسته بندی  \n",
       "4                Beverage - نوشیدنی ها  \n",
       "...                                ...  \n",
       "44876  Dry Food - مواد غذایی بسته بندی  \n",
       "44877                 خانگی و آشپزخانه  \n",
       "44894                    کالای دیچیتال  \n",
       "44899                      انواع پوشاک  \n",
       "44900                          اکسسوری  \n",
       "\n",
       "[20298 rows x 7 columns],\n",
       "  fkey_col_to_pkey_table={},\n",
       "  pkey_col=articles_id,\n",
       "  time_col=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.table_dict[\"article\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Table(df=\n",
       "        customers_id  customers_no\n",
       "0         9150651948     508206249\n",
       "1         9155143265      36124744\n",
       "2         9144518134      42737591\n",
       "3         9151739848      24102811\n",
       "4         9138537082      72155679\n",
       "...              ...           ...\n",
       "300316    9152264499    1394869309\n",
       "300317    9166036927      72736526\n",
       "300318    9131583129      72736530\n",
       "300319    9132515574      72836534\n",
       "300320    9308737056    1193953900\n",
       "\n",
       "[299536 rows x 2 columns],\n",
       "  fkey_col_to_pkey_table={},\n",
       "  pkey_col=customers_id,\n",
       "  time_col=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.table_dict[\"customer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "factor_id                     1269935\n",
       "articles_id                 2000568.0\n",
       "customers_id               9151861977\n",
       "BranchCode                        356\n",
       "salesTime                    18:16:41\n",
       "price_purchase                30737.6\n",
       "Discount_ratio               0.043444\n",
       "Quantity                          1.0\n",
       "datetime          2022-03-12 00:00:00\n",
       "Name: 6930864, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.df.iloc[table.df[\"datetime\"].idxmax()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "factor_id                           0\n",
       "articles_id                35157919.0\n",
       "customers_id               9150651948\n",
       "BranchCode                          9\n",
       "salesTime                    14:35:06\n",
       "price_purchase                19610.1\n",
       "Discount_ratio               0.266504\n",
       "Quantity                          1.0\n",
       "datetime          2021-03-22 00:00:00\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.df.iloc[table.df[\"datetime\"].idxmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rel-amazon',\n",
       " 'rel-avito',\n",
       " 'rel-event',\n",
       " 'rel-f1',\n",
       " 'rel-hm',\n",
       " 'rel-stack',\n",
       " 'rel-trial',\n",
       " 'hyperr-aras']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "register_dataset(\"hyperr-aras\", TransactionalDataset)\n",
    "get_dataset_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransactionalDataset()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_dataset = get_dataset(\"hyperr-aras\")\n",
    "hyper_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2022-02-15 00:00:00'), Timestamp('2022-02-22 00:00:00'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_dataset.val_timestamp, hyper_dataset.test_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import relbench\n",
    "\n",
    "relbench.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "from relbench.tasks import get_task, get_task_names, register_task\n",
    "from relbench.base import Database, EntityTask, RecommendationTask, Table, TaskType\n",
    "from relbench.metrics import (\n",
    "    accuracy,\n",
    "    average_precision,\n",
    "    f1,\n",
    "    link_prediction_map,\n",
    "    link_prediction_precision,\n",
    "    link_prediction_recall,\n",
    "    mae,\n",
    "    r2,\n",
    "    rmse,\n",
    "    roc_auc,\n",
    ")\n",
    "from metrics import link_prediction_top\n",
    "class UserItemPurchaseTask(RecommendationTask):\n",
    "    r\"\"\"Predict the list of articles each customer will purchase in the next seven\n",
    "    days.\"\"\"\n",
    "\n",
    "    task_type = TaskType.LINK_PREDICTION\n",
    "    src_entity_col = \"customer_id\"\n",
    "    src_entity_table = \"customer\"\n",
    "    dst_entity_col = \"article_id\"\n",
    "    dst_entity_table = \"article\"\n",
    "    time_col = \"timestamp\"\n",
    "    timedelta = pd.Timedelta(days=7)\n",
    "    metrics = [link_prediction_precision, link_prediction_recall, link_prediction_map, link_prediction_top]\n",
    "    eval_k = 12\n",
    "\n",
    "    def make_table(self, db: Database, timestamps: \"pd.Series[pd.Timestamp]\") -> Table:\n",
    "        customer = db.table_dict[\"customer\"].df\n",
    "        transactions = db.table_dict[\"transactions\"].df\n",
    "        timestamp_df = pd.DataFrame({\"timestamp\": timestamps})\n",
    "\n",
    "        df = duckdb.sql(\n",
    "            f\"\"\"\n",
    "            SELECT\n",
    "                t.timestamp,\n",
    "                transactions.customer_id,\n",
    "                LIST(DISTINCT transactions.article_id) AS article_id\n",
    "            FROM\n",
    "                timestamp_df t\n",
    "            LEFT JOIN\n",
    "                transactions\n",
    "            ON\n",
    "                transactions.t_dat > t.timestamp AND\n",
    "                transactions.t_dat <= t.timestamp + INTERVAL '{self.timedelta} days'\n",
    "            GROUP BY\n",
    "                t.timestamp,\n",
    "                transactions.customer_id\n",
    "            \"\"\"\n",
    "        ).df()\n",
    "\n",
    "        return Table(\n",
    "            df=df,\n",
    "            fkey_col_to_pkey_table={\n",
    "                self.src_entity_col: self.src_entity_table,\n",
    "                self.dst_entity_col: self.dst_entity_table,\n",
    "            },\n",
    "            pkey_col=None,\n",
    "            time_col=self.time_col,\n",
    "        )\n",
    "\n",
    "# Task 1: Predict articles each customer will purchase in the next 7 days\n",
    "class CustomerArticlePurchaseTask(RecommendationTask):\n",
    "    r\"\"\"Predict the list of articles each customer will purchase in the next seven days.\"\"\"\n",
    "    \n",
    "    task_type = TaskType.LINK_PREDICTION\n",
    "    src_entity_col = \"customers_id\"\n",
    "    src_entity_table = \"customer\"\n",
    "    dst_entity_col = \"articles_id\"\n",
    "    dst_entity_table = \"article\"\n",
    "    time_col = \"timestamp\"\n",
    "    timedelta = pd.Timedelta(days=7)\n",
    "    metrics = [link_prediction_precision, link_prediction_recall, link_prediction_map, link_prediction_top]\n",
    "    eval_k = 4\n",
    "\n",
    "    def make_table(self, db: Database, timestamps: \"pd.Series[pd.Timestamp]\") -> Table:\n",
    "        transactions = db.table_dict[\"transactions\"].df\n",
    "        timestamp_df = pd.DataFrame({\"timestamp\": timestamps})\n",
    "\n",
    "        df = duckdb.sql(\n",
    "            f\"\"\"\n",
    "            SELECT\n",
    "                t.timestamp,\n",
    "                transactions.customers_id,\n",
    "                LIST(DISTINCT transactions.articles_id) AS articles_id\n",
    "            FROM\n",
    "                timestamp_df t\n",
    "            LEFT JOIN\n",
    "                transactions\n",
    "            ON\n",
    "                transactions.datetime > t.timestamp AND\n",
    "                transactions.datetime <= t.timestamp + INTERVAL '{self.timedelta.days} days'\n",
    "            GROUP BY\n",
    "                t.timestamp,\n",
    "                transactions.customers_id\n",
    "            \"\"\"\n",
    "        ).df()\n",
    "\n",
    "        return Table(\n",
    "            df=df,\n",
    "            fkey_col_to_pkey_table={\n",
    "                self.src_entity_col: self.src_entity_table,\n",
    "                self.dst_entity_col: self.dst_entity_table,\n",
    "            },\n",
    "            pkey_col=None,\n",
    "            time_col=self.time_col,\n",
    "        )\n",
    "\n",
    "\n",
    "# Task 2: Predict customer churn (no purchases in the next week)\n",
    "class CustomerChurnTask(EntityTask):\n",
    "    r\"\"\"Predict the churn for a customer (no transactions) in the next 6 days.\"\"\"\n",
    "\n",
    "    task_type = TaskType.BINARY_CLASSIFICATION\n",
    "    entity_col = \"customers_id\"\n",
    "    entity_table = \"customer\"\n",
    "    time_col = \"timestamp\"\n",
    "    target_col = \"churn\"\n",
    "    timedelta = pd.Timedelta(days=7)\n",
    "    metrics = [average_precision, accuracy, f1, roc_auc]\n",
    "\n",
    "    def make_table(self, db: Database, timestamps: \"pd.Series[pd.Timestamp]\") -> Table:\n",
    "        customer = db.table_dict[\"customer\"].df\n",
    "        transactions = db.table_dict[\"transactions\"].df\n",
    "        timestamp_df = pd.DataFrame({\"timestamp\": timestamps})\n",
    "\n",
    "        df = duckdb.sql(\n",
    "            f\"\"\"\n",
    "            SELECT\n",
    "                timestamp,\n",
    "                customers_id,\n",
    "                CAST(\n",
    "                    NOT EXISTS (\n",
    "                        SELECT 1\n",
    "                        FROM transactions\n",
    "                        WHERE\n",
    "                            transactions.customers_id = customer.customers_id AND\n",
    "                            transactions.datetime > timestamp AND\n",
    "                            transactions.datetime <= timestamp + INTERVAL '{self.timedelta}'\n",
    "                    ) AS INTEGER\n",
    "                ) AS churn\n",
    "            FROM\n",
    "                timestamp_df,\n",
    "                customer\n",
    "            WHERE\n",
    "                EXISTS (\n",
    "                    SELECT 1\n",
    "                    FROM transactions\n",
    "                    WHERE\n",
    "                        transactions.customers_id = customer.customers_id AND\n",
    "                        transactions.datetime > timestamp - INTERVAL '{self.timedelta}' AND\n",
    "                        transactions.datetime <= timestamp\n",
    "                )\n",
    "            \"\"\"\n",
    "        ).df()\n",
    "\n",
    "        return Table(\n",
    "            df=df,\n",
    "            fkey_col_to_pkey_table={self.entity_col: self.entity_table},\n",
    "            pkey_col=None,\n",
    "            time_col=self.time_col,\n",
    "        ) \n",
    "    # def make_table(self, db: Database, timestamps: \"pd.Series[pd.Timestamp]\") -> Table:\n",
    "    #     transactions = db.table_dict[\"transactions\"].df\n",
    "    #     customer = db.table_dict[\"customer\"].df\n",
    "    #     timestamp_df = pd.DataFrame({\"timestamp\": timestamps})\n",
    "\n",
    "    #     df = duckdb.sql(\n",
    "    #         f\"\"\"\n",
    "    #         SELECT\n",
    "    #             t.timestamp,\n",
    "    #             c.customers_id,\n",
    "    #             CAST(\n",
    "    #                 NOT EXISTS (\n",
    "    #                     SELECT 1\n",
    "    #                     FROM transactions\n",
    "    #                     WHERE\n",
    "    #                         transactions.customers_id = c.customers_id AND\n",
    "    #                         transactions.datetime > t.timestamp AND\n",
    "    #                         transactions.datetime <= t.timestamp + INTERVAL '{self.timedelta.days} days'\n",
    "    #                 ) AS INTEGER\n",
    "    #             ) AS churn\n",
    "    #         FROM\n",
    "    #             timestamp_df t,\n",
    "    #             customer c\n",
    "    #         WHERE\n",
    "    #             EXISTS (\n",
    "    #                 SELECT 1\n",
    "    #                 FROM transactions\n",
    "    #                 WHERE\n",
    "    #                     transactions.customers_id = c.customers_id AND\n",
    "    #                     transactions.datetime > t.timestamp - INTERVAL '{self.timedelta.days} days' AND\n",
    "    #                     transactions.datetime <= t.timestamp\n",
    "    #             )\n",
    "    #         \"\"\"\n",
    "    #     ).df()\n",
    "\n",
    "    #     return Table(\n",
    "    #         df=df,\n",
    "    #         fkey_col_to_pkey_table={self.entity_col: self.entity_table},\n",
    "    #         pkey_col=None,\n",
    "    #         time_col=self.time_col,\n",
    "    #     )\n",
    "\n",
    "\n",
    "# Task 3: Predict article sales in the next 7 days\n",
    "class ArticleSalesTask(EntityTask):\n",
    "    r\"\"\"Predict the total sales for an article (sum of `price_purchase`) in the next 7 days.\"\"\"\n",
    "    \n",
    "    task_type = TaskType.REGRESSION\n",
    "    entity_col = \"articles_id\"\n",
    "    entity_table = \"article\"\n",
    "    time_col = \"datetime\"\n",
    "    target_col = \"sales\"\n",
    "    timedelta = pd.Timedelta(days=7)\n",
    "    metrics = [r2, mae, rmse]\n",
    "\n",
    "    def make_table(self, db: Database, timestamps: \"pd.Series[pd.Timestamp]\") -> Table:\n",
    "        transactions = db.table_dict[\"transactions\"].df\n",
    "        articles = db.table_dict[\"article\"].df\n",
    "        timestamp_df = pd.DataFrame({\"timestamp\": timestamps})\n",
    "\n",
    "        df = duckdb.sql(\n",
    "            f\"\"\"\n",
    "            SELECT\n",
    "                t.timestamp,\n",
    "                a.articles_id,\n",
    "                COALESCE(SUM(transactions.price_purchase), 0) AS sales\n",
    "            FROM\n",
    "                timestamp_df t,\n",
    "                article a\n",
    "            LEFT JOIN\n",
    "                transactions\n",
    "            ON\n",
    "                transactions.articles_id = a.articles_id AND\n",
    "                transactions.datetime > t.timestamp AND\n",
    "                transactions.datetime <= t.timestamp + INTERVAL '{self.timedelta.days} days'\n",
    "            GROUP BY\n",
    "                t.timestamp,\n",
    "                a.articles_id\n",
    "            \"\"\"\n",
    "        ).df()\n",
    "\n",
    "        return Table(\n",
    "            df=df,\n",
    "            fkey_col_to_pkey_table={self.entity_col: self.entity_table},\n",
    "            pkey_col=None,\n",
    "            time_col=self.time_col,\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomerArticlePurchaseTask(dataset=TransactionalDataset())"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aras_recom_task = CustomerArticlePurchaseTask(hyper_dataset, cache_dir=\"./cache/hyper_aras390111d195\")\n",
    "aras_recom_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aras_recom_task', 'aras_recom_task1', 'aras_recom_task2']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "register_task(\"hyperr-aras\", \"aras_recom_task2\", CustomerArticlePurchaseTask)\n",
    "get_task_names(\"hyperr-aras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aras_recom_task', 'aras_recom_task1', 'aras_recom_task2']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_task_names(\"hyperr-aras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making task table for train split from scratch...\n",
      "(You can also use `get_task(..., download=True)` for tasks prepared by the RelBench team.)\n",
      "Done in 18.05 seconds.\n",
      "Making task table for val split from scratch...\n",
      "(You can also use `get_task(..., download=True)` for tasks prepared by the RelBench team.)\n",
      "Done in 0.21 seconds.\n",
      "Making task table for test split from scratch...\n",
      "(You can also use `get_task(..., download=True)` for tasks prepared by the RelBench team.)\n",
      "Done in 0.37 seconds.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torch.nn import BCEWithLogitsLoss, L1Loss\n",
    "from relbench.datasets import get_dataset\n",
    "from relbench.tasks import get_task\n",
    "\n",
    "dataset = get_dataset(\"hyperr-aras\")\n",
    "task = get_task(\"hyperr-aras\", \"aras_recom_task2\")\n",
    "\n",
    "\n",
    "train_table = task.get_table(\"train\")\n",
    "val_table = task.get_table(\"val\")\n",
    "test_table = task.get_table(\"test\")\n",
    "\n",
    "out_channels = 1\n",
    "loss_fn = BCEWithLogitsLoss()\n",
    "tune_metric = \"link_prediction_map\"\n",
    "higher_is_better = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Table(df=\n",
       "        timestamp  customers_id  \\\n",
       "0      2021-04-13         30946   \n",
       "1      2021-04-13          9379   \n",
       "2      2021-04-13         53821   \n",
       "3      2021-04-13         53823   \n",
       "4      2021-04-13         53825   \n",
       "...           ...           ...   \n",
       "720863 2022-02-08         14613   \n",
       "720864 2022-02-08         96407   \n",
       "720865 2022-02-08        122007   \n",
       "720866 2022-02-08        284085   \n",
       "720867 2022-02-08        271058   \n",
       "\n",
       "                                              articles_id  \n",
       "0       [4186, 7751, 1230, 1565, 7, 11495, 1793, 5503,...  \n",
       "1       [5856, 1868, 925, 2192, 1450, 7804, 314, 8471,...  \n",
       "2       [204, 1895, 323, 288, 1710, 1326, 4309, 1485, ...  \n",
       "3       [10486, 2589, 230, 38, 3388, 126, 182, 56, 505...  \n",
       "4       [478, 930, 2445, 1641, 1280, 1241, 818, 1017, ...  \n",
       "...                                                   ...  \n",
       "720863                                             [1257]  \n",
       "720864                                            [19049]  \n",
       "720865                                             [4273]  \n",
       "720866                                            [17438]  \n",
       "720867                                             [5076]  \n",
       "\n",
       "[720868 rows x 3 columns],\n",
       "  fkey_col_to_pkey_table={'customers_id': 'customer', 'articles_id': 'article'},\n",
       "  pkey_col=None,\n",
       "  time_col=timestamp)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Table(df=\n",
       "       timestamp  customers_id  \\\n",
       "0     2022-02-15        179203   \n",
       "1     2022-02-15          6980   \n",
       "2     2022-02-15        192267   \n",
       "3     2022-02-15         16392   \n",
       "4     2022-02-15        126384   \n",
       "...          ...           ...   \n",
       "15086 2022-02-15        287773   \n",
       "15087 2022-02-15          4373   \n",
       "15088 2022-02-15         12133   \n",
       "15089 2022-02-15        228877   \n",
       "15090 2022-02-15         21118   \n",
       "\n",
       "                                             articles_id  \n",
       "0      [170, 2783, 139, 3912, 5395, 180, 1450, 3100, ...  \n",
       "1      [695, 16568, 18163, 1788, 5409, 1913, 3523, 83...  \n",
       "2      [722, 2200, 19518, 15992, 7067, 2922, 700, 618...  \n",
       "3      [3799, 1938, 3186, 11819, 4041, 5709, 19379, 1...  \n",
       "4      [1025, 16657, 1369, 2702, 256, 10591, 12741, 8...  \n",
       "...                                                  ...  \n",
       "15086                                             [4000]  \n",
       "15087                                             [2748]  \n",
       "15088                                              [426]  \n",
       "15089                                             [1242]  \n",
       "15090                                             [1937]  \n",
       "\n",
       "[15091 rows x 3 columns],\n",
       "  fkey_col_to_pkey_table={'customers_id': 'customer', 'articles_id': 'article'},\n",
       "  pkey_col=None,\n",
       "  time_col=timestamp)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Table(df=\n",
       "       timestamp  customers_id  \\\n",
       "0     2022-02-22           810   \n",
       "1     2022-02-22         55934   \n",
       "2     2022-02-22         31213   \n",
       "3     2022-02-22        288753   \n",
       "4     2022-02-22        288754   \n",
       "...          ...           ...   \n",
       "15537 2022-02-22        292744   \n",
       "15538 2022-02-22        292791   \n",
       "15539 2022-02-22        292845   \n",
       "15540 2022-02-22        292852   \n",
       "15541 2022-02-22        292882   \n",
       "\n",
       "                                             articles_id  \n",
       "0      [430, 4862, 18183, 1079, 1990, 1443, 2959, 245...  \n",
       "1                                     [1661, 526, 19210]  \n",
       "2      [1272, 3047, 7450, 4812, 7138, 4835, 10475, 13...  \n",
       "3                             [16081, 3012, 16117, 1661]  \n",
       "4      [19374, 6629, 897, 3635, 2336, 19254, 2267, 11...  \n",
       "...                                                  ...  \n",
       "15537                                             [3616]  \n",
       "15538                                             [1286]  \n",
       "15539                                            [15992]  \n",
       "15540                                             [2388]  \n",
       "15541                                            [16447]  \n",
       "\n",
       "[15542 rows x 3 columns],\n",
       "  fkey_col_to_pkey_table={'customers_id': 'customer', 'articles_id': 'article'},\n",
       "  pkey_col=None,\n",
       "  time_col=timestamp)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "import torch_frame\n",
    "\n",
    "# Some book keeping\n",
    "from torch_geometric.seed import seed_everything\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)  # check that it's cuda if you want it to run in reasonable time!\n",
    "root_dir = \"./data_ARAS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article': {'articles_id': <stype.numerical: 'numerical'>,\n",
       "  'art_name': <stype.text_embedded: 'text_embedded'>,\n",
       "  'Department Name': <stype.categorical: 'categorical'>,\n",
       "  'group': <stype.text_embedded: 'text_embedded'>,\n",
       "  'Subgroup Name': <stype.text_embedded: 'text_embedded'>,\n",
       "  'group_id': <stype.numerical: 'numerical'>,\n",
       "  'whole_Branch_Name': <stype.text_embedded: 'text_embedded'>},\n",
       " 'branches': {'BranchCode': <stype.numerical: 'numerical'>,\n",
       "  'BranchName': <stype.text_embedded: 'text_embedded'>},\n",
       " 'customer': {'customers_id': <stype.numerical: 'numerical'>,\n",
       "  'customers_no': <stype.numerical: 'numerical'>},\n",
       " 'transactions': {'factor_id': <stype.numerical: 'numerical'>,\n",
       "  'articles_id': <stype.numerical: 'numerical'>,\n",
       "  'customers_id': <stype.numerical: 'numerical'>,\n",
       "  'BranchCode': <stype.categorical: 'categorical'>,\n",
       "  'salesTime': <stype.timestamp: 'timestamp'>,\n",
       "  'price_purchase': <stype.numerical: 'numerical'>,\n",
       "  'Discount_ratio': <stype.numerical: 'numerical'>,\n",
       "  'Quantity': <stype.numerical: 'numerical'>,\n",
       "  'datetime': <stype.timestamp: 'timestamp'>}}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from relbench.modeling.utils import get_stype_proposal\n",
    "\n",
    "db = dataset.get_db()\n",
    "col_to_stype_dict = get_stype_proposal(db)\n",
    "col_to_stype_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch import Tensor\n",
    "import torch\n",
    "\n",
    "class BertPersianTextEmbedding:\n",
    "    def __init__(self, device: Optional[torch.device] = None):\n",
    "        # Replace the model with a Persian BERT model\n",
    "        self.model = SentenceTransformer(\"HooshvareLab/bert-fa-zwnj-base\",  # Example Persian BERT model\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    def __call__(self, sentences: List[str]) -> Tensor:\n",
    "        # Encode the sentences using the Persian BERT model and return as a tensor\n",
    "        return torch.from_numpy(self.model.encode(sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name HooshvareLab/bert-fa-zwnj-base. Creating a new one with mean pooling.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at HooshvareLab/bert-fa-zwnj-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Embedding raw data in mini-batch: 100%|██████████| 318/318 [01:03<00:00,  5.03it/s]\n",
      "Embedding raw data in mini-batch: 100%|██████████| 318/318 [02:08<00:00,  2.47it/s]\n",
      "Embedding raw data in mini-batch: 100%|██████████| 318/318 [00:41<00:00,  7.60it/s]\n",
      "Embedding raw data in mini-batch: 100%|██████████| 318/318 [00:43<00:00,  7.29it/s]\n",
      "Embedding raw data in mini-batch: 100%|██████████| 1/1 [00:00<00:00, 76.96it/s]\n",
      "c:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch_frame\\data\\stats.py:177: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  ser = pd.to_datetime(ser, format=time_format)\n",
      "c:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch_frame\\data\\mapper.py:290: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  ser = pd.to_datetime(ser, format=self.format, errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "from torch_frame.config.text_embedder import TextEmbedderConfig\n",
    "from relbench.modeling.graph import make_pkey_fkey_graph\n",
    "\n",
    "text_embedder_cfg = TextEmbedderConfig(\n",
    "    text_embedder=BertPersianTextEmbedding(device=device), batch_size=64\n",
    ")\n",
    "\n",
    "data, col_stats_dict = make_pkey_fkey_graph(\n",
    "    db,\n",
    "    col_to_stype_dict=col_to_stype_dict,  # speficied column types\n",
    "    text_embedder_cfg=text_embedder_cfg,  # our chosen text encoder\n",
    "    cache_dir=os.path.join(\n",
    "        root_dir, f\"rel-aras_recom_materialized_cache\"\n",
    "    ),  # store materialized graph for convenience\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  article={ tf=TensorFrame([20298, 6]) },\n",
       "  branches={ tf=TensorFrame([5, 1]) },\n",
       "  customer={ tf=TensorFrame([299536, 1]) },\n",
       "  transactions={\n",
       "    tf=TensorFrame([6581467, 6]),\n",
       "    time=[6581467],\n",
       "  },\n",
       "  (transactions, f2p_articles_id, article)={ edge_index=[2, 6581467] },\n",
       "  (article, rev_f2p_articles_id, transactions)={ edge_index=[2, 6581467] },\n",
       "  (transactions, f2p_customers_id, customer)={ edge_index=[2, 6581467] },\n",
       "  (customer, rev_f2p_customers_id, transactions)={ edge_index=[2, 6581467] },\n",
       "  (transactions, f2p_BranchCode, branches)={ edge_index=[2, 6581467] },\n",
       "  (branches, rev_f2p_BranchCode, transactions)={ edge_index=[2, 6581467] }\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RelBench Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# from model import Model\n",
    "# from text_embedder import GloveTextEmbedding\n",
    "from torch import Tensor\n",
    "from torch_frame import stype\n",
    "from torch_frame.config.text_embedder import TextEmbedderConfig\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.seed import seed_everything\n",
    "from tqdm import tqdm\n",
    "\n",
    "from relbench.base import Dataset, RecommendationTask, TaskType\n",
    "from relbench.datasets import get_dataset\n",
    "from relbench.modeling.graph import get_link_train_table_input, make_pkey_fkey_graph\n",
    "from relbench.modeling.loader import SparseTensor\n",
    "from relbench.modeling.utils import get_stype_proposal\n",
    "from relbench.tasks import get_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loader dictionary\n",
    "loader_dict: Dict[str, NeighborLoader] = {}\n",
    "dst_nodes_dict: Dict[str, Tuple[NodeType, Tensor]] = {}\n",
    "\n",
    "# Loop over the train, val, and test splits\n",
    "for split, table in [\n",
    "    (\"train\", train_table),\n",
    "    (\"val\", val_table),\n",
    "    (\"test\", test_table),\n",
    "]:\n",
    "    # Get link train table input for link prediction task\n",
    "    table_input = get_link_train_table_input(\n",
    "        table=table,\n",
    "        task=task,\n",
    "    )\n",
    "    \n",
    "    # Save destination nodes for later use\n",
    "    dst_nodes_dict[split] = table_input.dst_nodes\n",
    "\n",
    "    # Create NeighborLoader for link prediction\n",
    "    loader_dict[split] = NeighborLoader(\n",
    "        data,\n",
    "        num_neighbors=[128 for _ in range(2)],  # Sample subgraphs of depth 2, 128 neighbors per node\n",
    "        time_attr=\"time\",  # Use time attribute if available\n",
    "        input_nodes=table_input.src_nodes,  # Source nodes for link prediction\n",
    "        input_time=table_input.src_time,  # Use src_time if time data is available\n",
    "        subgraph_type=\"bidirectional\",\n",
    "        batch_size=512,\n",
    "        temporal_strategy=\"last\",  # Uniform sampling strategy for time\n",
    "        shuffle=split == \"train\",  # Shuffle only during training\n",
    "        num_workers=0,\n",
    "        persistent_workers=False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model for link prediction task\n",
    "model = Model(\n",
    "    data=data,  # Heterogeneous data object\n",
    "    col_stats_dict=col_stats_dict,  # Column statistics dictionary\n",
    "    num_layers=2,  # Adjust this to match your desired architecture (depth of GNN)\n",
    "    channels=128,  # Number of hidden channels in GNN layers\n",
    "    out_channels=1,  # Output size (for link prediction, usually a scalar per edge)\n",
    "    aggr=\"sum\",  # Aggregation method (can be \"sum\", \"mean\", etc.)\n",
    "    norm=\"layer_norm\",  # Normalization method\n",
    "    id_awareness=True,  # Whether the model is aware of node IDs\n",
    ").to(device)  # Move model to the appropriate device (e.g., GPU)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Use the desired learning rate\n",
    "\n",
    "# Handling sparse destination nodes for training\n",
    "# dst_nodes_dict stores the destination nodes for the \"train\" split (in sparse format)\n",
    "train_sparse_tensor = SparseTensor(dst_nodes_dict[\"train\"][1], device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train() -> float:\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    loss_accum = count_accum = 0\n",
    "    steps = 0\n",
    "    total_steps = min(len(loader_dict[\"train\"]), 2000)  # Change the max_steps_per_epoch to 2000 or your preferred value\n",
    "\n",
    "    for batch in tqdm(loader_dict[\"train\"], total=total_steps):\n",
    "        batch = batch.to(device)  # Move batch data to device (GPU or CPU)\n",
    "\n",
    "        # Forward pass through the model for link prediction (source and destination tables)\n",
    "        out = model.forward_dst_readout(\n",
    "            batch, task.src_entity_table, task.dst_entity_table\n",
    "        ).flatten()  # Flatten the output\n",
    "\n",
    "        batch_size = batch[task.src_entity_table].batch_size  # Get batch size for the source entity table\n",
    "\n",
    "        # Get ground-truth labels\n",
    "        input_id = batch[task.src_entity_table].input_id  # Input IDs for the batch\n",
    "        src_batch, dst_index = train_sparse_tensor[input_id]  # Get the source and destination indices\n",
    "\n",
    "        # Get the target labels by checking if source-destination pairs exist\n",
    "        target = torch.isin(\n",
    "            batch[task.dst_entity_table].batch\n",
    "            + batch_size * batch[task.dst_entity_table].n_id,\n",
    "            src_batch + batch_size * dst_index,\n",
    "        ).float()  # Convert the result to float for loss computation\n",
    "\n",
    "        # Optimization\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        loss = F.binary_cross_entropy_with_logits(out, target)  # Compute binary cross-entropy loss\n",
    "        loss.backward()  # Backpropagation to compute gradients\n",
    "\n",
    "        optimizer.step()  # Update model parameters\n",
    "\n",
    "        # Accumulate the total loss and count for averaging later\n",
    "        loss_accum += float(loss) * out.numel()\n",
    "        count_accum += out.numel()\n",
    "\n",
    "        steps += 1\n",
    "        if steps >= total_steps:\n",
    "            break  # Break the loop if max steps per epoch is reached\n",
    "\n",
    "    # Handle the case where no data was sampled\n",
    "    if count_accum == 0:\n",
    "        warnings.warn(\n",
    "            f\"Did not sample a single '{task.dst_entity_table}' node in any mini-batch. \"\n",
    "            \"Try increasing the number of layers/hops or reducing the batch size.\"\n",
    "        )\n",
    "\n",
    "    # Return average loss for the epoch\n",
    "    return loss_accum / count_accum if count_accum > 0 else float(\"nan\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # No gradient computation for evaluation\n",
    "def test(loader: NeighborLoader) -> np.ndarray:\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    pred_list: list[Tensor] = []  # Store predictions\n",
    "    for batch in tqdm(loader):  # Iterate over batches in the test loader\n",
    "        batch = batch.to(device)  # Move the batch data to the device (GPU or CPU)\n",
    "\n",
    "        # Forward pass through the model for link prediction\n",
    "        out = (\n",
    "            model.forward_dst_readout(\n",
    "                batch, task.src_entity_table, task.dst_entity_table\n",
    "            )\n",
    "            .detach()\n",
    "            .flatten()  # Detach the output from the computational graph\n",
    "        )\n",
    "\n",
    "        batch_size = batch[task.src_entity_table].batch_size  # Get the batch size for source nodes\n",
    "\n",
    "        # Prepare a tensor to hold the scores for the source-destination pairs\n",
    "        scores = torch.zeros(batch_size, task.num_dst_nodes, device=out.device)\n",
    "\n",
    "        # Fill the scores with sigmoid activations for the destination nodes in the current batch\n",
    "        scores[\n",
    "            batch[task.dst_entity_table].batch, batch[task.dst_entity_table].n_id\n",
    "        ] = torch.sigmoid(out)  # Apply sigmoid activation to get probabilities\n",
    "\n",
    "        # Use top-k (e.g., top recommended items) based on the scores\n",
    "        _, pred_mini = torch.topk(scores, k=task.eval_k, dim=1)  # Get top-k predictions\n",
    "        pred_list.append(pred_mini)  # Append predictions to the list\n",
    "\n",
    "    # Concatenate all predictions and move to CPU for further processing\n",
    "    pred = torch.cat(pred_list, dim=0).cpu().numpy()\n",
    "\n",
    "    return pred  # Return the final predictions as a NumPy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# Initialize variables for tracking the best model and best validation metrics\n",
    "state_dict = None  # This will hold the best model state\n",
    "best_val_metric = 0  # This will store the best validation metric\n",
    "epochs = 10  # Set the number of epochs (you can adjust as needed)\n",
    "eval_epochs_interval = 1  # Evaluate every 'n' epochs (change this based on your needs)\n",
    "tune_metric = \"link_prediction_map\"  # Define the metric you are tuning\n",
    "\n",
    "# Training and evaluation loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # Run the training function\n",
    "    train_loss = train()\n",
    "    \n",
    "    # Perform evaluation every 'eval_epochs_interval' epochs\n",
    "    if epoch % eval_epochs_interval == 0:\n",
    "        # Run the validation on the validation dataset\n",
    "        val_pred = test(loader_dict[\"val\"])  # Get the predictions from the model\n",
    "        val_metrics = task.evaluate(val_pred, task.get_table(\"val\"))  # Evaluate predictions\n",
    "        \n",
    "        # Print the training loss and validation metrics\n",
    "        print(\n",
    "            f\"Epoch: {epoch:02d}, Train loss: {train_loss}, \"\n",
    "            f\"Val metrics: {val_metrics}\"\n",
    "        )\n",
    "\n",
    "        # Check if the current validation metric is the best\n",
    "        if val_metrics[tune_metric] > best_val_metric:\n",
    "            best_val_metric = val_metrics[tune_metric]  # Update best metric\n",
    "            state_dict = copy.deepcopy(model.state_dict())  # Save the best model state\n",
    "\n",
    "# After training, load the best model weights\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Evaluate the model on the validation set with the best weights\n",
    "val_pred = test(loader_dict[\"val\"])\n",
    "val_metrics = task.evaluate(val_pred, task.get_table(\"val\"))\n",
    "print(f\"Best Val metrics: {val_metrics}\")\n",
    "\n",
    "# Evaluate the model on the test set with the best weights\n",
    "test_pred = test(loader_dict[\"test\"])\n",
    "test_metrics = task.evaluate(test_pred)\n",
    "print(f\"Best test metrics: {test_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2df166d5b70>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from relbench.base import Dataset, RecommendationTask, TaskType\n",
    "from relbench.datasets import get_dataset\n",
    "from relbench.modeling.graph import (\n",
    "    get_link_train_table_input,\n",
    "    make_pkey_fkey_graph,\n",
    ")\n",
    "from relbench.modeling.loader import SparseTensor\n",
    "from relbench.modeling.utils import get_stype_proposal\n",
    "from relbench.tasks import get_task\n",
    "from torch import Tensor\n",
    "from torch_frame import stype\n",
    "from torch_frame.config.text_embedder import TextEmbedderConfig\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.seed import seed_everything\n",
    "from torch_geometric.typing import NodeType\n",
    "from torch_geometric.utils.cross_entropy import sparse_cross_entropy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from contextgnn.nn.models import IDGNN, ContextGNN, ShallowRHSGNN\n",
    "from contextgnn.utils import GloveTextEmbedding, RHSEmbeddingMode\n",
    "# Static configuration parameters\n",
    "learning_rate = 0.001\n",
    "epochs = 20\n",
    "eval_epochs_interval = 1\n",
    "batch_size = 512\n",
    "channels = 128\n",
    "aggregation_method = \"sum\"\n",
    "num_layers = 4\n",
    "num_neighbors = 128\n",
    "temporal_strategy = \"last\"\n",
    "share_same_time = True\n",
    "max_steps_per_epoch = 2000\n",
    "num_workers = 0\n",
    "seed = 42\n",
    "model_name = \"contextgnn\"  # For example, can be 'idgnn', 'contextgnn', or 'shallowrhsgnn'\n",
    "tune_metric = \"link_prediction_map\"  # Metric used to tune the model\n",
    "cache_dir = os.path.expanduser(\"~/.cache/relbench_examples\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\relbench\\modeling\\graph.py:217: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\SparseCsrTensorImpl.cpp:55.)\n",
      "  dst_node_indices = sparse_coo.to_sparse_csr()\n"
     ]
    }
   ],
   "source": [
    "# Define static num_neighbors for NeighborLoader\n",
    "num_neighbors = [num_neighbors // 2**i for i in range(num_layers)]\n",
    "\n",
    "# Loader dictionary for train, validation, and test sets\n",
    "loader_dict: Dict[str, NeighborLoader] = {}\n",
    "dst_nodes_dict: Dict[str, Tuple[NodeType, Tensor]] = {}\n",
    "num_dst_nodes_dict: Dict[str, int] = {}\n",
    "\n",
    "# Assuming `task` is already defined and provides the dataset information\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    table = task.get_table(split)\n",
    "    table_input = get_link_train_table_input(table, task)\n",
    "    dst_nodes_dict[split] = table_input.dst_nodes\n",
    "    num_dst_nodes_dict[split] = table_input.num_dst_nodes\n",
    "    loader_dict[split] = NeighborLoader(\n",
    "        data,\n",
    "        num_neighbors=num_neighbors,\n",
    "        time_attr=\"time\",\n",
    "        input_nodes=table_input.src_nodes,\n",
    "        input_time=table_input.src_time,\n",
    "        subgraph_type=\"bidirectional\",\n",
    "        batch_size=batch_size,\n",
    "        temporal_strategy=temporal_strategy,\n",
    "        shuffle=split == \"train\",\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=num_workers > 0,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"idgnn\":\n",
    "    model = IDGNN(\n",
    "        data=data,\n",
    "        col_stats_dict=col_stats_dict,\n",
    "        num_layers=num_layers,\n",
    "        channels=channels,\n",
    "        out_channels=1,\n",
    "        aggr=aggregation_method,\n",
    "        norm=\"layer_norm\",\n",
    "        torch_frame_model_kwargs={\n",
    "            \"channels\": 128,\n",
    "            \"num_layers\": 4,\n",
    "        },\n",
    "    ).to(device)\n",
    "elif model_name == \"contextgnn\":\n",
    "    model = ContextGNN(\n",
    "        data=data,\n",
    "        col_stats_dict=col_stats_dict,\n",
    "        rhs_emb_mode=RHSEmbeddingMode.FUSION,\n",
    "        dst_entity_table=task.dst_entity_table,\n",
    "        num_nodes=num_dst_nodes_dict[\"train\"],\n",
    "        num_layers=num_layers,\n",
    "        channels=channels,\n",
    "        aggr=\"sum\",\n",
    "        norm=\"layer_norm\",\n",
    "        embedding_dim=64,\n",
    "        torch_frame_model_kwargs={\n",
    "            \"channels\": 128,\n",
    "            \"num_layers\": 4,\n",
    "        },\n",
    "    ).to(device)\n",
    "elif model_name == 'shallowrhsgnn':\n",
    "    model = ShallowRHSGNN(\n",
    "        data=data,\n",
    "        col_stats_dict=col_stats_dict,\n",
    "        rhs_emb_mode=RHSEmbeddingMode.FUSION,\n",
    "        dst_entity_table=task.dst_entity_table,\n",
    "        num_nodes=num_dst_nodes_dict[\"train\"],\n",
    "        num_layers=num_layers,\n",
    "        channels=channels,\n",
    "        aggr=\"sum\",\n",
    "        norm=\"layer_norm\",\n",
    "        embedding_dim=64,\n",
    "        torch_frame_model_kwargs={\n",
    "            \"channels\": 128,\n",
    "            \"num_layers\": 4,\n",
    "        },\n",
    "    ).to(device)\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported model type {model_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train() -> float:\n",
    "    model.train()\n",
    "\n",
    "    loss_accum = count_accum = 0\n",
    "    steps = 0\n",
    "    total_steps = min(len(loader_dict[\"train\"]), max_steps_per_epoch)\n",
    "    sparse_tensor = SparseTensor(dst_nodes_dict[\"train\"][1], device=device)\n",
    "    \n",
    "    for batch in tqdm(loader_dict[\"train\"], total=total_steps, desc=\"Train\"):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Get ground-truth\n",
    "        input_id = batch[task.src_entity_table].input_id\n",
    "        src_batch, dst_index = sparse_tensor[input_id]\n",
    "\n",
    "        # Optimization\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if model_name == 'idgnn':\n",
    "            out = model(batch, task.src_entity_table, task.dst_entity_table).flatten()\n",
    "            batch_size = batch[task.src_entity_table].batch_size\n",
    "\n",
    "            # Get target label\n",
    "            target = torch.isin(\n",
    "                batch[task.dst_entity_table].batch +\n",
    "                batch_size * batch[task.dst_entity_table].n_id,\n",
    "                src_batch + batch_size * dst_index,\n",
    "            ).float()\n",
    "\n",
    "            loss = F.binary_cross_entropy_with_logits(out, target)\n",
    "            numel = out.numel()\n",
    "        elif model_name in ['contextgnn', 'shallowrhsgnn']:\n",
    "            logits = model(batch, task.src_entity_table, task.dst_entity_table)\n",
    "            edge_label_index = torch.stack([src_batch, dst_index], dim=0)\n",
    "            loss = sparse_cross_entropy(logits, edge_label_index)\n",
    "            numel = len(batch[task.dst_entity_table].batch)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_accum += float(loss) * numel\n",
    "        count_accum += numel\n",
    "\n",
    "        steps += 1\n",
    "        if steps > max_steps_per_epoch:\n",
    "            break\n",
    "\n",
    "    if count_accum == 0:\n",
    "        warnings.warn(f\"Did not sample a single '{task.dst_entity_table}' node in any mini-batch.\")\n",
    "\n",
    "    return loss_accum / count_accum if count_accum > 0 else float(\"nan\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "@torch.no_grad()\n",
    "def test(loader: NeighborLoader, desc: str) -> np.ndarray:\n",
    "    model.eval()\n",
    "\n",
    "    pred_list: List[Tensor] = []\n",
    "    for batch in tqdm(loader, desc=desc):\n",
    "        batch = batch.to(device)\n",
    "        batch_size = batch[task.src_entity_table].batch_size\n",
    "\n",
    "        if model_name == \"idgnn\":\n",
    "            out = (model.forward(batch, task.src_entity_table, task.dst_entity_table).detach().flatten())\n",
    "            scores = torch.zeros(batch_size, task.num_dst_nodes, device=out.device)\n",
    "            scores[batch[task.dst_entity_table].batch, batch[task.dst_entity_table].n_id] = torch.sigmoid(out)\n",
    "        elif model_name in ['contextgnn', 'shallowrhsgnn']:\n",
    "            out = model(batch, task.src_entity_table, task.dst_entity_table).detach()\n",
    "            scores = torch.sigmoid(out)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {model_name}.\")\n",
    "\n",
    "        _, pred_mini = torch.topk(scores, k=task.eval_k, dim=1)\n",
    "        pred_list.append(pred_mini)\n",
    "    \n",
    "    pred = torch.cat(pred_list, dim=0).cpu().numpy()\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 1/1408 [00:45<17:56:41, 45.91s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 192.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m best_val_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m eval_epochs_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      8\u001b[0m         val_pred \u001b[38;5;241m=\u001b[39m test(loader_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m], desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[42], line 34\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m     numel \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mnumel()\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontextgnn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshallowrhsgnn\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m---> 34\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msrc_entity_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdst_entity_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     edge_label_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([src_batch, dst_index], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     36\u001b[0m     loss \u001b[38;5;241m=\u001b[39m sparse_cross_entropy(logits, edge_label_index)\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\Desktop\\Dani\\ContextGNN\\contextgnn\\nn\\models\\contextgnn.py:184\u001b[0m, in \u001b[0;36mContextGNN.forward\u001b[1;34m(self, batch, entity_table, dst_table)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    179\u001b[0m     batch: HeteroData,\n\u001b[0;32m    180\u001b[0m     entity_table: NodeType,\n\u001b[0;32m    181\u001b[0m     dst_table: NodeType,\n\u001b[0;32m    182\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    183\u001b[0m     seed_time \u001b[38;5;241m=\u001b[39m batch[entity_table]\u001b[38;5;241m.\u001b[39mseed_time\n\u001b[1;32m--> 184\u001b[0m     x_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_gnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m seed_time\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    187\u001b[0m     lhs_embedding \u001b[38;5;241m=\u001b[39m x_dict[entity_table][:\n\u001b[0;32m    188\u001b[0m                                          batch_size]  \u001b[38;5;66;03m# batch_size, channel\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\Desktop\\Dani\\ContextGNN\\contextgnn\\nn\\models\\contextgnn.py:171\u001b[0m, in \u001b[0;36mContextGNN.forward_gnn\u001b[1;34m(self, batch, entity_table)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node_type, rel_time \u001b[38;5;129;01min\u001b[39;00m rel_time_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    169\u001b[0m     x_dict[node_type] \u001b[38;5;241m=\u001b[39m x_dict[node_type] \u001b[38;5;241m+\u001b[39m rel_time\n\u001b[1;32m--> 171\u001b[0m x_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgnn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x_dict\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\Desktop\\Dani\\ContextGNN\\contextgnn\\nn\\models\\graphsage.py:55\u001b[0m, in \u001b[0;36mHeteroGraphSAGE.forward\u001b[1;34m(self, x_dict, edge_index_dict)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     51\u001b[0m     x_dict: Dict[NodeType, Tensor],\n\u001b[0;32m     52\u001b[0m     edge_index_dict: Dict[NodeType, Tensor],\n\u001b[0;32m     53\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[NodeType, Tensor]:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (conv, norm_dict) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorms)):\n\u001b[1;32m---> 55\u001b[0m         x_dict \u001b[38;5;241m=\u001b[39m \u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m         x_dict \u001b[38;5;241m=\u001b[39m {key: norm_dict[key](x) \u001b[38;5;28;01mfor\u001b[39;00m key, x \u001b[38;5;129;01min\u001b[39;00m x_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     57\u001b[0m         x_dict \u001b[38;5;241m=\u001b[39m {key: x\u001b[38;5;241m.\u001b[39mrelu() \u001b[38;5;28;01mfor\u001b[39;00m key, x \u001b[38;5;129;01min\u001b[39;00m x_dict\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch_geometric\\nn\\conv\\hetero_conv.py:158\u001b[0m, in \u001b[0;36mHeteroConv.forward\u001b[1;34m(self, *args_dict, **kwargs_dict)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_edge_level_arg:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 158\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dst \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m out_dict:\n\u001b[0;32m    161\u001b[0m     out_dict[dst] \u001b[38;5;241m=\u001b[39m [out]\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch_geometric\\nn\\conv\\sage_conv.py:139\u001b[0m, in \u001b[0;36mSAGEConv.forward\u001b[1;34m(self, x, edge_index, size)\u001b[0m\n\u001b[0;32m    137\u001b[0m x_r \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_weight \u001b[38;5;129;01mand\u001b[39;00m x_r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 139\u001b[0m     out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin_r\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_r\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize:\n\u001b[0;32m    142\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(out, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch_geometric\\nn\\dense\\linear.py:147\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    142\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \n\u001b[0;32m    144\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m        x (torch.Tensor): The input features.\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 192.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "# Training and evaluation loop\n",
    "state_dict = None\n",
    "best_val_metric = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train()\n",
    "    \n",
    "    if epoch % eval_epochs_interval == 0:\n",
    "        val_pred = test(loader_dict[\"val\"], desc=\"Val\")\n",
    "        val_metrics = task.evaluate(val_pred, task.get_table(\"val\"))\n",
    "        print(f\"Epoch: {epoch:02d}, Train loss: {train_loss}, Val metrics: {val_metrics}\")\n",
    "\n",
    "        if val_metrics[tune_metric] > best_val_metric:\n",
    "            best_val_metric = val_metrics[tune_metric]\n",
    "            state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "# Load the best model and evaluate on validation and test sets\n",
    "assert state_dict is not None\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "val_pred = test(loader_dict[\"val\"], desc=\"Best val\")\n",
    "val_metrics = task.evaluate(val_pred, task.get_table(\"val\"))\n",
    "print(f\"Best val metrics: {val_metrics}\")\n",
    "\n",
    "test_pred = test(loader_dict[\"test\"], desc=\"Test\")\n",
    "test_metrics = task.evaluate(test_pred)\n",
    "print(f\"Best test metrics: {test_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from relbench.base import Dataset, RecommendationTask, TaskType\n",
    "from relbench.datasets import get_dataset\n",
    "from relbench.modeling.graph import (\n",
    "    get_link_train_table_input,\n",
    "    make_pkey_fkey_graph,\n",
    ")\n",
    "from relbench.modeling.loader import SparseTensor\n",
    "from relbench.modeling.utils import get_stype_proposal\n",
    "from relbench.tasks import get_task\n",
    "from torch_frame import stype\n",
    "from torch_frame.config.text_embedder import TextEmbedderConfig\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.seed import seed_everything\n",
    "from torch_geometric.typing import NodeType\n",
    "from torch_geometric.utils.cross_entropy import sparse_cross_entropy\n",
    "from tqdm import tqdm\n",
    "from contextgnn.nn.models import ContextGNN\n",
    "from contextgnn.utils import GloveTextEmbedding, RHSEmbeddingMode\n",
    "\n",
    "# Static Configuration\n",
    "dataset_name = \"rel-trial\"\n",
    "task_name = \"site-sponsor-run\"\n",
    "learning_rate = 0.001\n",
    "epochs = 20\n",
    "eval_epochs_interval = 1\n",
    "batch_size = 128\n",
    "channels = 128\n",
    "aggregation_method = \"sum\"\n",
    "num_layers = 6\n",
    "num_neighbors = 64\n",
    "rhs_sample_size = 1000  # Use -1 for sampling all RHS\n",
    "temporal_strategy = \"last\"\n",
    "max_steps_per_epoch = 200\n",
    "num_workers = 0\n",
    "seed = 42\n",
    "cache_dir = os.path.expanduser(\"~/.cache/relbench_examples\")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_num_threads(1 if torch.cuda.is_available() else os.cpu_count())\n",
    "torch.manual_seed(seed)\n",
    "seed_everything(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "PytorchStreamReader failed reading zip archive: failed finding central directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(col_to_stype_dict, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Prepare graph data and column stats\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m data, col_stats_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmake_pkey_fkey_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcol_to_stype_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcol_to_stype_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_embedder_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTextEmbedderConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_embedder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGloveTextEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcache_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/materialized\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\relbench\\modeling\\graph.py:75\u001b[0m, in \u001b[0;36mmake_pkey_fkey_graph\u001b[1;34m(db, col_to_stype_dict, text_embedder_cfg, cache_dir)\u001b[0m\n\u001b[0;32m     65\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__const__\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mlen\u001b[39m(table\u001b[38;5;241m.\u001b[39mdf)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfkey_dict})\n\u001b[0;32m     67\u001b[0m path \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m cache_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cache_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     69\u001b[0m )\n\u001b[0;32m     71\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcol_to_stype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcol_to_stype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcol_to_text_embedder_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_embedder_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m---> 75\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaterialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m data[table_name]\u001b[38;5;241m.\u001b[39mtf \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mtensor_frame\n\u001b[0;32m     78\u001b[0m col_stats_dict[table_name] \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mcol_stats\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch_frame\\data\\dataset.py:583\u001b[0m, in \u001b[0;36mDataset.materialize\u001b[1;34m(self, device, path)\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m osp\u001b[38;5;241m.\u001b[39misfile(path):\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;66;03m# Load tensor_frame and col_stats\u001b[39;00m\n\u001b[1;32m--> 583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensor_frame, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_col_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_frame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;66;03m# Instantiate the converter\u001b[39;00m\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_tensor_frame_converter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tensorframe_converter()\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch_frame\\utils\\io.py:98\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, device)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     83\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m, device: torch\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     84\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[TensorFrame, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m[StatType, Any]] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[0;32m     85\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Load saved :class:`TensorFrame` object and optional :obj:`col_stats`\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03m    from a specified path.\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m            optional :obj:`col_stats`.\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m     tf_dict, col_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m     tf_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeat_dict\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m deserialize_feat_dict(\n\u001b[0;32m    100\u001b[0m         tf_dict\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeat_serialized_dict\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    101\u001b[0m     tensor_frame \u001b[38;5;241m=\u001b[39m TensorFrame(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtf_dict)\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch\\serialization.py:1004\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1002\u001b[0m orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n\u001b[0;32m   1003\u001b[0m overall_storage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1004\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m   1005\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_torchscript_zip(opened_zipfile):\n\u001b[0;32m   1006\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m received a zip file that looks like a TorchScript archive\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1007\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dispatching to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directly to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1008\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m silence this warning)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mUserWarning\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch\\serialization.py:456\u001b[0m, in \u001b[0;36m_open_zipfile_reader.__init__\u001b[1;34m(self, name_or_buffer)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name_or_buffer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory"
     ]
    }
   ],
   "source": [
    "# Load dataset and task\n",
    "dataset: Dataset = get_dataset(dataset_name, download=True)\n",
    "task: RecommendationTask = get_task(dataset_name, task_name, download=True)\n",
    "\n",
    "# Ensure task type is LINK_PREDICTION\n",
    "assert task.task_type == TaskType.LINK_PREDICTION\n",
    "\n",
    "# Tune metric\n",
    "tune_metric = \"link_prediction_map\"\n",
    "\n",
    "# Handle column type mappings\n",
    "stypes_cache_path = Path(f\"{cache_dir}/{dataset_name}/stypes.json\")\n",
    "try:\n",
    "    with open(stypes_cache_path, \"r\") as f:\n",
    "        col_to_stype_dict = json.load(f)\n",
    "    for table, col_to_stype in col_to_stype_dict.items():\n",
    "        for col, stype_str in col_to_stype.items():\n",
    "            col_to_stype[col] = stype(stype_str)\n",
    "except FileNotFoundError:\n",
    "    col_to_stype_dict = get_stype_proposal(dataset.get_db())\n",
    "    Path(stypes_cache_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(stypes_cache_path, \"w\") as f:\n",
    "        json.dump(col_to_stype_dict, f, indent=2, default=str)\n",
    "\n",
    "# Prepare graph data and column stats\n",
    "data, col_stats_dict = make_pkey_fkey_graph(\n",
    "    dataset.get_db(),\n",
    "    col_to_stype_dict=col_to_stype_dict,\n",
    "    text_embedder_cfg=TextEmbedderConfig(\n",
    "        text_embedder=GloveTextEmbedding(device=device), batch_size=256\n",
    "    ),\n",
    "    cache_dir=f\"{cache_dir}/{dataset_name}/materialized\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of neighbors for NeighborLoader\n",
    "num_neighbors_list = [int(num_neighbors // 2**i) for i in range(num_layers)]\n",
    "\n",
    "# Loader dictionaries\n",
    "loader_dict: Dict[str, NeighborLoader] = {}\n",
    "dst_nodes_dict: Dict[str, Tuple[NodeType, Tensor]] = {}\n",
    "num_dst_nodes_dict: Dict[str, int] = {}\n",
    "\n",
    "# Initialize data loaders for train, val, and test splits\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    table = task.get_table(split)\n",
    "    table_input = get_link_train_table_input(table, task)\n",
    "    dst_nodes_dict[split] = table_input.dst_nodes\n",
    "    num_dst_nodes_dict[split] = table_input.num_dst_nodes\n",
    "    loader_dict[split] = NeighborLoader(\n",
    "        data,\n",
    "        num_neighbors=num_neighbors_list,\n",
    "        time_attr=\"time\",\n",
    "        input_nodes=table_input.src_nodes,\n",
    "        input_time=table_input.src_time,\n",
    "        subgraph_type=\"bidirectional\",\n",
    "        batch_size=batch_size,\n",
    "        temporal_strategy=temporal_strategy,\n",
    "        shuffle=split == \"train\",\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=num_workers > 0,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ContextGNN model\n",
    "model: ContextGNN = ContextGNN(\n",
    "    data=data,\n",
    "    col_stats_dict=col_stats_dict,\n",
    "    rhs_emb_mode=RHSEmbeddingMode.FUSION,\n",
    "    dst_entity_table=task.dst_entity_table,\n",
    "    num_nodes=num_dst_nodes_dict[\"train\"],\n",
    "    num_layers=num_layers,\n",
    "    channels=channels,\n",
    "    aggr=aggregation_method,\n",
    "    norm=\"layer_norm\",\n",
    "    embedding_dim=64,\n",
    "    torch_frame_model_kwargs={\"channels\": 128, \"num_layers\": 4},\n",
    "    rhs_sample_size=rhs_sample_size,\n",
    ").to(device)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train() -> float:\n",
    "    model.train()\n",
    "\n",
    "    loss_accum = count_accum = 0.0\n",
    "    steps = 0\n",
    "    total_steps = min(len(loader_dict[\"train\"]), max_steps_per_epoch)\n",
    "    sparse_tensor = SparseTensor(dst_nodes_dict[\"train\"][1], device=device)\n",
    "\n",
    "    for batch in tqdm(loader_dict[\"train\"], total=total_steps, desc=\"Train\"):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Get ground-truth\n",
    "        input_id = batch[task.src_entity_table].input_id\n",
    "        src_batch, dst_index = sparse_tensor[input_id]\n",
    "\n",
    "        # Optimization\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, lhs_y_batch, rhs_y_index = model.forward_sample_softmax(\n",
    "            batch, task.src_entity_table, task.dst_entity_table, src_batch, dst_index\n",
    "        )\n",
    "        edge_label_index = torch.stack([lhs_y_batch, rhs_y_index], dim=0)\n",
    "        loss = sparse_cross_entropy(logits, edge_label_index)\n",
    "\n",
    "        numel = len(batch[task.dst_entity_table].batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_accum += float(loss) * numel\n",
    "        count_accum += numel\n",
    "\n",
    "        steps += 1\n",
    "        if steps >= max_steps_per_epoch:\n",
    "            break\n",
    "\n",
    "    if count_accum == 0:\n",
    "        warnings.warn(f\"Did not sample a single '{task.dst_entity_table}' node in any mini-batch.\")\n",
    "\n",
    "    return loss_accum / count_accum if count_accum > 0 else float(\"nan\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(loader: NeighborLoader, desc: str) -> np.ndarray:\n",
    "    model.eval()\n",
    "\n",
    "    pred_list: List[Tensor] = []\n",
    "    for batch in tqdm(loader, desc=desc):\n",
    "        batch = batch.to(device)\n",
    "        out = model(batch, task.src_entity_table, task.dst_entity_table).detach()\n",
    "        scores = torch.sigmoid(out)\n",
    "        _, pred_mini = torch.topk(scores, k=task.eval_k, dim=1)\n",
    "        pred_list.append(pred_mini)\n",
    "    pred = torch.cat(pred_list, dim=0).cpu().numpy()\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|█████████▉| 199/200 [52:32<00:15, 15.84s/it]\n",
      "Val: 100%|██████████| 118/118 [17:12<00:00,  8.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train loss: 63.64355062029784, Val metrics: {'link_prediction_precision': np.float64(0.08001457822543237), 'link_prediction_recall': np.float64(0.06289739234376616), 'link_prediction_map': np.float64(0.06324225623808157), 'link_prediction_top': np.float64(0.25995626532370286)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|█████████▉| 199/200 [49:45<00:15, 15.00s/it]\n",
      "Val: 100%|██████████| 118/118 [27:39<00:00, 14.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02, Train loss: 58.35857021517497, Val metrics: {'link_prediction_precision': np.float64(0.08079318799284342), 'link_prediction_recall': np.float64(0.06300215009859066), 'link_prediction_map': np.float64(0.06365502985591116), 'link_prediction_top': np.float64(0.2613478232058843)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|█████████▉| 199/200 [47:15<00:14, 14.25s/it]\n",
      "Val: 100%|██████████| 118/118 [28:21<00:00, 14.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03, Train loss: 57.68884758799559, Val metrics: {'link_prediction_precision': np.float64(0.08203565038764826), 'link_prediction_recall': np.float64(0.06379913579833485), 'link_prediction_map': np.float64(0.06478797149147027), 'link_prediction_top': np.float64(0.2639321449870784)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|█████████▉| 199/200 [47:34<00:14, 14.35s/it]\n",
      "Val:   8%|▊         | 9/118 [02:25<29:24, 16.19s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m eval_epochs_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Run validation\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     val_pred \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     val_metrics \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mevaluate(val_pred, task\u001b[38;5;241m.\u001b[39mget_table(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val metrics: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_metrics\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[71], line 7\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(loader, desc)\u001b[0m\n\u001b[0;32m      5\u001b[0m pred_list: List[Tensor] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(loader, desc\u001b[38;5;241m=\u001b[39mdesc):\n\u001b[1;32m----> 7\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     out \u001b[38;5;241m=\u001b[39m model(batch, task\u001b[38;5;241m.\u001b[39msrc_entity_table, task\u001b[38;5;241m.\u001b[39mdst_entity_table)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m      9\u001b[0m     scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(out)\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch_geometric\\data\\data.py:362\u001b[0m, in \u001b[0;36mBaseData.to\u001b[1;34m(self, device, non_blocking, *args)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, device: Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m], \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    358\u001b[0m        non_blocking: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Performs tensor device conversion, either for all attributes or\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;124;03m    only the ones given in :obj:`*args`.\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch_geometric\\data\\data.py:342\u001b[0m, in \u001b[0;36mBaseData.apply\u001b[1;34m(self, func, *args)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies the function :obj:`func`, either to all attributes or only\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03mthe ones given in :obj:`*args`.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m store \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstores:\n\u001b[1;32m--> 342\u001b[0m     \u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch_geometric\\data\\storage.py:201\u001b[0m, in \u001b[0;36mBaseStorage.apply\u001b[1;34m(self, func, *args)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies the function :obj:`func`, either to all attributes or only\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;124;03mthe ones given in :obj:`*args`.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems(\u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28mself\u001b[39m[key] \u001b[38;5;241m=\u001b[39m \u001b[43mrecursive_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch_geometric\\data\\storage.py:908\u001b[0m, in \u001b[0;36mrecursive_apply\u001b[1;34m(data, func)\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 908\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch_geometric\\data\\data.py:363\u001b[0m, in \u001b[0;36mBaseData.to.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, device: Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m], \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    358\u001b[0m        non_blocking: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Performs tensor device conversion, either for all attributes or\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;124;03m    only the ones given in :obj:`*args`.\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m--> 363\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch_frame\\data\\tensor_frame.py:319\u001b[0m, in \u001b[0;36mTensorFrame.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    316\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m--> 319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch_frame\\data\\tensor_frame.py:347\u001b[0m, in \u001b[0;36mTensorFrame._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn: Callable[[TensorData], TensorData]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TensorFrame:\n\u001b[0;32m    346\u001b[0m     out \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 347\u001b[0m     out\u001b[38;5;241m.\u001b[39mfeat_dict \u001b[38;5;241m=\u001b[39m {stype: \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m stype, x \u001b[38;5;129;01min\u001b[39;00m out\u001b[38;5;241m.\u001b[39mfeat_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39my \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    349\u001b[0m         y \u001b[38;5;241m=\u001b[39m fn(out\u001b[38;5;241m.\u001b[39my)\n",
      "File \u001b[1;32mc:\\Users\\KN2C\\miniconda3\\envs\\kamal\\Lib\\site-packages\\torch_frame\\data\\tensor_frame.py:316\u001b[0m, in \u001b[0;36mTensorFrame.to.<locals>.fn\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    314\u001b[0m         x[key] \u001b[38;5;241m=\u001b[39m x[key]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 316\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize variables for tracking the best model and validation metrics\n",
    "state_dict = None\n",
    "best_val_metric = 0\n",
    "\n",
    "# Training and evaluation loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train()\n",
    "\n",
    "    if epoch % eval_epochs_interval == 0:\n",
    "        # Run validation\n",
    "        val_pred = test(loader_dict[\"val\"], desc=\"Val\")\n",
    "        val_metrics = task.evaluate(val_pred, task.get_table(\"val\"))\n",
    "        print(f\"Epoch: {epoch:02d}, Train loss: {train_loss}, Val metrics: {val_metrics}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_metrics[tune_metric] > best_val_metric:\n",
    "            best_val_metric = val_metrics[tune_metric]\n",
    "            state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "# Load the best model weights\n",
    "assert state_dict is not None\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Evaluate on validation and test sets\n",
    "val_pred = test(loader_dict[\"val\"], desc=\"Best val\")\n",
    "val_metrics = task.evaluate(val_pred, task.get_table(\"val\"))\n",
    "print(f\"Best val metrics: {val_metrics}\")\n",
    "\n",
    "test_pred = test(loader_dict[\"test\"], desc=\"Test\")\n",
    "test_metrics = task.evaluate(test_pred)\n",
    "print(f\"Best test metrics: {test_metrics}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
